{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Local Development Setup\n",
    "\n",
    "## üéØ Interactive Tutorial: Professional Spark Development\n",
    "\n",
    "Welcome to Lesson 3! In this notebook, we'll explore professional development practices for Apache Spark applications. You'll learn how to structure projects, manage configurations, implement testing, and integrate development tools.\n",
    "\n",
    "### üìã What You'll Learn\n",
    "1. **Project Structure Best Practices** - Modular, maintainable Spark applications\n",
    "2. **Development Workflow** - Debugging, testing, and quality assurance\n",
    "3. **Configuration Management** - Environment-specific settings and secrets\n",
    "4. **Development Tools Integration** - Git, Docker, CI/CD foundations\n",
    "\n",
    "### üîß Setup\n",
    "Make sure you've completed the environment setup:\n",
    "```bash\n",
    "make setup\n",
    "make install-dev\n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PySpark version: 3.5.7\n",
      "üìÅ Project root: /Users/liamnguyen/Documents/0.Coding/spark-bootcamp/lessons/lesson-03-local-development\n",
      "üêç Python path: /Users/liamnguyen/Documents/0.Coding/spark-bootcamp/lessons/lesson-03-local-development/.venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "# Initial setup and imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().absolute()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Essential imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, avg, count\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ PySpark version: {pyspark.__version__}\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üêç Python path: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Module 1: Project Structure Best Practices\n",
    "\n",
    "Let's start by understanding how to structure a professional Spark project. We'll demonstrate the key principles through practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ 1.1 Demonstrating Project Structure\n",
    "\n",
    "Let's examine what a well-structured Spark project looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Recommended Project Structure:\n",
      "\n",
      "    my-spark-project/\n",
      "    ‚îú‚îÄ‚îÄ README.md                   # Project documentation\n",
      "    ‚îú‚îÄ‚îÄ pyproject.toml             # Dependencies and configuration\n",
      "    ‚îú‚îÄ‚îÄ Makefile                   # Development commands\n",
      "    ‚îú‚îÄ‚îÄ .env.example               # Environment variables template\n",
      "    ‚îú‚îÄ‚îÄ .gitignore                 # Git ignore patterns\n",
      "    ‚îÇ\n",
      "    ‚îú‚îÄ‚îÄ src/                       # Source code (production)\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ config/                # Configuration management\n",
      "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py        # Application settings\n",
      "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ environments/      # Environment-specific configs\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ jobs/                  # Spark job definitions\n",
      "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_job.py        # Abstract base job class\n",
      "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl_job.py         # ETL job implementation\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ transformations/       # Data transformation functions\n",
      "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py        # Data cleaning functions\n",
      "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregations.py    # Aggregation functions\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utility functions\n",
      "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark_utils.py     # Spark session and utilities\n",
      "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ io_utils.py        # Input/output helpers\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ schemas/               # Data schemas\n",
      "    ‚îÇ       ‚îî‚îÄ‚îÄ input_schemas.py   # Input data schemas\n",
      "    ‚îÇ\n",
      "    ‚îú‚îÄ‚îÄ tests/                     # Test suite\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py           # Pytest configuration\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ unit/                 # Unit tests\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ integration/          # Integration tests\n",
      "    ‚îÇ\n",
      "    ‚îú‚îÄ‚îÄ data/                     # Local data directory\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Raw input data\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ processed/            # Processed data\n",
      "    ‚îÇ\n",
      "    ‚îî‚îÄ‚îÄ scripts/                  # Utility scripts\n",
      "        ‚îú‚îÄ‚îÄ setup.sh             # Environment setup\n",
      "        ‚îî‚îÄ‚îÄ run_job.py           # Job runner script\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Let's create a sample project structure to demonstrate\n",
    "def show_project_structure():\n",
    "    \"\"\"Display recommended project structure\"\"\"\n",
    "    structure = \"\"\"\n",
    "    my-spark-project/\n",
    "    ‚îú‚îÄ‚îÄ README.md                   # Project documentation\n",
    "    ‚îú‚îÄ‚îÄ pyproject.toml             # Dependencies and configuration\n",
    "    ‚îú‚îÄ‚îÄ Makefile                   # Development commands\n",
    "    ‚îú‚îÄ‚îÄ .env.example               # Environment variables template\n",
    "    ‚îú‚îÄ‚îÄ .gitignore                 # Git ignore patterns\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ src/                       # Source code (production)\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ config/                # Configuration management\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py        # Application settings\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ environments/      # Environment-specific configs\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ jobs/                  # Spark job definitions\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_job.py        # Abstract base job class\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl_job.py         # ETL job implementation\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ transformations/       # Data transformation functions\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py        # Data cleaning functions\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregations.py    # Aggregation functions\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utility functions\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark_utils.py     # Spark session and utilities\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ io_utils.py        # Input/output helpers\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ schemas/               # Data schemas\n",
    "    ‚îÇ       ‚îî‚îÄ‚îÄ input_schemas.py   # Input data schemas\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ tests/                     # Test suite\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py           # Pytest configuration\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ unit/                 # Unit tests\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ integration/          # Integration tests\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ data/                     # Local data directory\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Raw input data\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ processed/            # Processed data\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ scripts/                  # Utility scripts\n",
    "        ‚îú‚îÄ‚îÄ setup.sh             # Environment setup\n",
    "        ‚îî‚îÄ‚îÄ run_job.py           # Job runner script\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Recommended Project Structure:\")\n",
    "    print(structure)\n",
    "\n",
    "\n",
    "show_project_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß 1.2 Separation of Concerns Example\n",
    "\n",
    "Let's see how to properly separate different concerns in a Spark application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Monolithic approach - everything mixed together\n"
     ]
    }
   ],
   "source": [
    "# Example: Proper separation of concerns\n",
    "\n",
    "# ‚ùå BAD: Everything in one function\n",
    "def monolithic_data_processing():\n",
    "    # Spark session creation\n",
    "    spark = SparkSession.builder.appName(\"MonolithicApp\").getOrCreate()\n",
    "\n",
    "    # Data loading\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"data/raw/customers.csv\")\n",
    "\n",
    "    # Business logic mixed with technical concerns\n",
    "    processed_df = (\n",
    "        df.filter(col(\"age\") > 18)\n",
    "        .withColumn(\n",
    "            \"age_group\",\n",
    "            when(col(\"age\") < 30, \"Young\")\n",
    "            .when(col(\"age\") < 50, \"Middle\")\n",
    "            .otherwise(\"Senior\"),\n",
    "        )\n",
    "        .groupBy(\"age_group\")\n",
    "        .agg(count(\"*\").alias(\"count\"), avg(\"income\").alias(\"avg_income\"))\n",
    "    )\n",
    "\n",
    "    # Output writing\n",
    "    processed_df.write.mode(\"overwrite\").parquet(\"data/processed/customer_analysis\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "monolithic_data_processing()\n",
    "print(\"‚ùå Monolithic approach - everything mixed together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modular approach - separated concerns with clear responsibilities\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ GOOD: Separated concerns\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# 1. Configuration Management\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    app_name: str = \"CustomerAnalysis\"\n",
    "    input_path: str = \"data/raw/customers.csv\"\n",
    "    output_path: str = \"data/processed/customer_analysis\"\n",
    "    min_age: int = 18\n",
    "\n",
    "\n",
    "# 2. Spark Utilities\n",
    "class SparkUtils:\n",
    "    @staticmethod\n",
    "    def get_spark_session(app_name: str) -> SparkSession:\n",
    "        \"\"\"Create optimized Spark session\"\"\"\n",
    "        return (\n",
    "            SparkSession.builder.appName(app_name)\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "\n",
    "# 3. Data Schema Definition\n",
    "class CustomerSchema:\n",
    "    SCHEMA = StructType(\n",
    "        [\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"income\", DoubleType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# 4. Business Logic (Transformations)\n",
    "class CustomerTransformations:\n",
    "    @staticmethod\n",
    "    def filter_valid_customers(df, min_age: int = 18):\n",
    "        \"\"\"Filter customers with valid age\"\"\"\n",
    "        return df.filter(col(\"age\") >= min_age)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_age_group(df):\n",
    "        \"\"\"Add age group categorization\"\"\"\n",
    "        return df.withColumn(\n",
    "            \"age_group\",\n",
    "            when(col(\"age\") < 30, \"Young\")\n",
    "            .when(col(\"age\") < 50, \"Middle\")\n",
    "            .otherwise(\"Senior\"),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_age_group_stats(df):\n",
    "        \"\"\"Calculate statistics by age group\"\"\"\n",
    "        return (\n",
    "            df.groupBy(\"age_group\")\n",
    "            .agg(count(\"*\").alias(\"count\"), avg(\"income\").alias(\"avg_income\"))\n",
    "            .orderBy(\"age_group\")\n",
    "        )\n",
    "\n",
    "\n",
    "# 5. I/O Operations\n",
    "class DataIO:\n",
    "    @staticmethod\n",
    "    def read_customer_data(spark: SparkSession, path: str):\n",
    "        \"\"\"Read customer data with schema\"\"\"\n",
    "        return (\n",
    "            spark.read.schema(CustomerSchema.SCHEMA).option(\"header\", \"true\").csv(path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def write_analysis_results(df, path: str):\n",
    "        \"\"\"Write analysis results\"\"\"\n",
    "        (df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(path))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Modular approach - separated concerns with clear responsibilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è 1.3 Putting It All Together\n",
    "\n",
    "Now let's see how these separated components work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete modular job structure defined\n"
     ]
    }
   ],
   "source": [
    "# 6. Main Job Class\n",
    "class CustomerAnalysisJob:\n",
    "    def __init__(self, config: AppConfig):\n",
    "        self.config = config\n",
    "        self.spark = SparkUtils.get_spark_session(config.app_name)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete analysis pipeline\"\"\"\n",
    "        try:\n",
    "            print(f\"üöÄ Starting {self.config.app_name}\")\n",
    "\n",
    "            # Extract\n",
    "            print(\"üì• Loading customer data...\")\n",
    "            raw_data = DataIO.read_customer_data(self.spark, self.config.input_path)\n",
    "            print(f\"üìä Loaded {raw_data.count():,} customer records\")\n",
    "\n",
    "            # Transform\n",
    "            print(\"üîÑ Applying transformations...\")\n",
    "            valid_customers = CustomerTransformations.filter_valid_customers(\n",
    "                raw_data, self.config.min_age\n",
    "            )\n",
    "            customers_with_groups = CustomerTransformations.add_age_group(\n",
    "                valid_customers\n",
    "            )\n",
    "            analysis_results = CustomerTransformations.calculate_age_group_stats(\n",
    "                customers_with_groups\n",
    "            )\n",
    "\n",
    "            print(\"üìà Analysis results:\")\n",
    "            analysis_results.show()\n",
    "\n",
    "            # Load\n",
    "            print(f\"üíæ Saving results to {self.config.output_path}\")\n",
    "            DataIO.write_analysis_results(analysis_results, self.config.output_path)\n",
    "\n",
    "            print(\"‚úÖ Job completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Job failed: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.spark.stop()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Complete modular job structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ 1.4 Testing the Modular Structure\n",
    "\n",
    "Let's create some sample data and test our modular structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created test data at: /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/input/customers.csv\n",
      "üìä Sample data:\n",
      "  customer_id           name  age  income          city\n",
      "0        C001  Alice Johnson   25   50000      New York\n",
      "1        C002      Bob Smith   35   75000       Chicago\n",
      "2        C003  Charlie Brown   45   90000   Los Angeles\n",
      "3        C004   Diana Wilson   55  120000       Houston\n",
      "4        C005      Eve Davis   17   25000       Phoenix\n",
      "5        C006   Frank Miller   30   60000  Philadelphia\n",
      "\n",
      "üîß Configuration:\n",
      "  Input: /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/input/customers.csv\n",
      "  Output: /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/output/customer_analysis\n",
      "  Min age: 18\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for demonstration\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample customer data\n",
    "sample_data = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C004\", \"C005\", \"C006\"],\n",
    "        \"name\": [\n",
    "            \"Alice Johnson\",\n",
    "            \"Bob Smith\",\n",
    "            \"Charlie Brown\",\n",
    "            \"Diana Wilson\",\n",
    "            \"Eve Davis\",\n",
    "            \"Frank Miller\",\n",
    "        ],\n",
    "        \"age\": [25, 35, 45, 55, 17, 30],  # Note: one customer under 18\n",
    "        \"income\": [50000, 75000, 90000, 120000, 25000, 60000],\n",
    "        \"city\": [\n",
    "            \"New York\",\n",
    "            \"Chicago\",\n",
    "            \"Los Angeles\",\n",
    "            \"Houston\",\n",
    "            \"Phoenix\",\n",
    "            \"Philadelphia\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create temporary directories\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "input_dir = temp_dir / \"input\"\n",
    "output_dir = temp_dir / \"output\"\n",
    "input_dir.mkdir(exist_ok=True)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save sample data\n",
    "input_file = input_dir / \"customers.csv\"\n",
    "sample_data.to_csv(input_file, index=False)\n",
    "\n",
    "print(f\"üìÅ Created test data at: {input_file}\")\n",
    "print(\"üìä Sample data:\")\n",
    "print(sample_data)\n",
    "\n",
    "# Configure and run the job\n",
    "config = AppConfig(\n",
    "    input_path=str(input_file), output_path=str(output_dir / \"customer_analysis\")\n",
    ")\n",
    "\n",
    "print(\"\\nüîß Configuration:\")\n",
    "print(f\"  Input: {config.input_path}\")\n",
    "print(f\"  Output: {config.output_path}\")\n",
    "print(f\"  Min age: {config.min_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting CustomerAnalysis\n",
      "üì• Loading customer data...\n",
      "üìä Loaded 6 customer records\n",
      "üîÑ Applying transformations...\n",
      "üìà Analysis results:\n",
      "+---------+-----+----------+\n",
      "|age_group|count|avg_income|\n",
      "+---------+-----+----------+\n",
      "|   Middle|    3|   75000.0|\n",
      "|   Senior|    1|  120000.0|\n",
      "|    Young|    1|   50000.0|\n",
      "+---------+-----+----------+\n",
      "\n",
      "üíæ Saving results to /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/output/customer_analysis\n",
      "‚úÖ Job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run the modular job\n",
    "job = CustomerAnalysisJob(config)\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Module 2: Development Workflow\n",
    "\n",
    "Now let's explore professional development workflows, including debugging techniques and testing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõ 2.1 DataFrame Debugging Utilities\n",
    "\n",
    "Debugging DataFrames can be challenging. Let's create utilities to make it easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß DataFrame debugging utilities created\n"
     ]
    }
   ],
   "source": [
    "# Advanced debugging utilities for DataFrames\n",
    "from pyspark.sql import DataFrame\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "class DataFrameDebugger:\n",
    "    \"\"\"Comprehensive DataFrame debugging utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_dataframe(\n",
    "        df: DataFrame,\n",
    "        name: str = \"DataFrame\",\n",
    "        show_rows: int = 10,\n",
    "        show_schema: bool = True,\n",
    "        show_count: bool = True,\n",
    "        show_sample: bool = True,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Comprehensive DataFrame debugging\"\"\"\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç DEBUG: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        if show_schema:\n",
    "            print(\"\\nüìã Schema:\")\n",
    "            df.printSchema()\n",
    "\n",
    "        if show_count:\n",
    "            count = df.count()\n",
    "            print(f\"\\nüìä Row Count: {count:,} rows\")\n",
    "\n",
    "        if show_sample and df.count() > 0:\n",
    "            print(f\"\\nüîç Sample Data (first {show_rows} rows):\")\n",
    "            df.show(show_rows, truncate=False)\n",
    "\n",
    "            # Show data types and null counts\n",
    "            print(\"\\nüìà Column Statistics:\")\n",
    "            for column in df.columns:\n",
    "                null_count = df.filter(col(column).isNull()).count()\n",
    "                dtype = dict(df.dtypes)[column]\n",
    "                print(f\"  {column:20} | Type: {dtype:15} | Nulls: {null_count:,}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def profile_operation(operation_name: str):\n",
    "        \"\"\"Decorator to profile Spark operations\"\"\"\n",
    "\n",
    "        def decorator(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start_time = time.time()\n",
    "\n",
    "                print(f\"\\nüöÄ Starting: {operation_name}\")\n",
    "                result = func(*args, **kwargs)\n",
    "\n",
    "                # Force action if result is DataFrame\n",
    "                if hasattr(result, \"count\"):\n",
    "                    count = result.count()\n",
    "                    execution_time = time.time() - start_time\n",
    "                    print(f\"‚úÖ Completed: {operation_name}\")\n",
    "                    print(f\"‚è±Ô∏è  Execution time: {execution_time:.2f} seconds\")\n",
    "                    print(f\"üìä Result count: {count:,} rows\")\n",
    "                else:\n",
    "                    execution_time = time.time() - start_time\n",
    "                    print(f\"‚úÖ Completed: {operation_name}\")\n",
    "                    print(f\"‚è±Ô∏è  Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "                return result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        return decorator\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_dataframes(\n",
    "        df1: DataFrame,\n",
    "        df2: DataFrame,\n",
    "        name1: str = \"DataFrame 1\",\n",
    "        name2: str = \"DataFrame 2\",\n",
    "    ):\n",
    "        \"\"\"Compare two DataFrames\"\"\"\n",
    "        print(f\"\\nüîç Comparing {name1} vs {name2}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Compare counts\n",
    "        count1, count2 = df1.count(), df2.count()\n",
    "        print(f\"üìä Row counts: {name1}: {count1:,}, {name2}: {count2:,}\")\n",
    "\n",
    "        # Compare schemas\n",
    "        cols1, cols2 = set(df1.columns), set(df2.columns)\n",
    "        print(f\"üìã Column counts: {name1}: {len(cols1)}, {name2}: {len(cols2)}\")\n",
    "\n",
    "        if cols1 != cols2:\n",
    "            print(\"‚ö†Ô∏è  Schema differences:\")\n",
    "            print(f\"  Only in {name1}: {cols1 - cols2}\")\n",
    "            print(f\"  Only in {name2}: {cols2 - cols1}\")\n",
    "        else:\n",
    "            print(\"‚úÖ Schemas match\")\n",
    "\n",
    "\n",
    "print(\"üîß DataFrame debugging utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ 2.2 Testing Our Debug Utilities\n",
    "\n",
    "Let's test our debugging utilities with the customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç DEBUG: Raw Customer Data\n",
      "============================================================\n",
      "\n",
      "üìã Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "\n",
      "üìä Row Count: 6 rows\n",
      "\n",
      "üîç Sample Data (first 10 rows):\n",
      "+-----------+-------------+---+--------+------------+\n",
      "|customer_id|name         |age|income  |city        |\n",
      "+-----------+-------------+---+--------+------------+\n",
      "|C001       |Alice Johnson|25 |50000.0 |New York    |\n",
      "|C002       |Bob Smith    |35 |75000.0 |Chicago     |\n",
      "|C003       |Charlie Brown|45 |90000.0 |Los Angeles |\n",
      "|C004       |Diana Wilson |55 |120000.0|Houston     |\n",
      "|C005       |Eve Davis    |17 |25000.0 |Phoenix     |\n",
      "|C006       |Frank Miller |30 |60000.0 |Philadelphia|\n",
      "+-----------+-------------+---+--------+------------+\n",
      "\n",
      "\n",
      "üìà Column Statistics:\n",
      "  customer_id          | Type: string          | Nulls: 0\n",
      "  name                 | Type: string          | Nulls: 0\n",
      "  age                  | Type: int             | Nulls: 0\n",
      "  income               | Type: double          | Nulls: 0\n",
      "  city                 | Type: string          | Nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a new Spark session for debugging demonstration\n",
    "spark = SparkUtils.get_spark_session(\"DebuggingDemo\")\n",
    "\n",
    "# Load and debug the customer data\n",
    "customer_df = DataIO.read_customer_data(spark, str(input_file))\n",
    "customer_df = DataFrameDebugger.debug_dataframe(customer_df, \"Raw Customer Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting: Filter Valid Customers\n",
      "‚úÖ Completed: Filter Valid Customers\n",
      "‚è±Ô∏è  Execution time: 0.05 seconds\n",
      "üìä Result count: 5 rows\n",
      "\n",
      "üöÄ Starting: Add Age Groups\n",
      "‚úÖ Completed: Add Age Groups\n",
      "‚è±Ô∏è  Execution time: 0.05 seconds\n",
      "üìä Result count: 5 rows\n",
      "\n",
      "============================================================\n",
      "üîç DEBUG: Customers with Age Groups\n",
      "============================================================\n",
      "\n",
      "üìã Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age_group: string (nullable = false)\n",
      "\n",
      "\n",
      "üìä Row Count: 5 rows\n",
      "\n",
      "üîç Sample Data (first 10 rows):\n",
      "+-----------+-------------+---+--------+------------+---------+\n",
      "|customer_id|name         |age|income  |city        |age_group|\n",
      "+-----------+-------------+---+--------+------------+---------+\n",
      "|C001       |Alice Johnson|25 |50000.0 |New York    |Young    |\n",
      "|C002       |Bob Smith    |35 |75000.0 |Chicago     |Middle   |\n",
      "|C003       |Charlie Brown|45 |90000.0 |Los Angeles |Middle   |\n",
      "|C004       |Diana Wilson |55 |120000.0|Houston     |Senior   |\n",
      "|C006       |Frank Miller |30 |60000.0 |Philadelphia|Middle   |\n",
      "+-----------+-------------+---+--------+------------+---------+\n",
      "\n",
      "\n",
      "üìà Column Statistics:\n",
      "  customer_id          | Type: string          | Nulls: 0\n",
      "  name                 | Type: string          | Nulls: 0\n",
      "  age                  | Type: int             | Nulls: 0\n",
      "  income               | Type: double          | Nulls: 0\n",
      "  city                 | Type: string          | Nulls: 0\n",
      "  age_group            | Type: string          | Nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations with profiling\n",
    "@DataFrameDebugger.profile_operation(\"Filter Valid Customers\")\n",
    "def filter_customers_with_profiling(df):\n",
    "    return CustomerTransformations.filter_valid_customers(df)\n",
    "\n",
    "\n",
    "@DataFrameDebugger.profile_operation(\"Add Age Groups\")\n",
    "def add_age_groups_with_profiling(df):\n",
    "    return CustomerTransformations.add_age_group(df)\n",
    "\n",
    "\n",
    "# Apply transformations\n",
    "filtered_df = filter_customers_with_profiling(customer_df)\n",
    "grouped_df = add_age_groups_with_profiling(filtered_df)\n",
    "\n",
    "# Debug the final result\n",
    "final_df = DataFrameDebugger.debug_dataframe(grouped_df, \"Customers with Age Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Comparing Original Data vs Filtered Data (Age >= 18)\n",
      "==================================================\n",
      "üìä Row counts: Original Data: 6, Filtered Data (Age >= 18): 5\n",
      "üìã Column counts: Original Data: 5, Filtered Data (Age >= 18): 5\n",
      "‚úÖ Schemas match\n"
     ]
    }
   ],
   "source": [
    "# Compare original vs filtered data\n",
    "DataFrameDebugger.compare_dataframes(\n",
    "    customer_df, filtered_df, \"Original Data\", \"Filtered Data (Age >= 18)\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ 2.3 Unit Testing Framework\n",
    "\n",
    "Let's create a comprehensive testing framework for our Spark components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Unit testing framework created\n"
     ]
    }
   ],
   "source": [
    "# Unit testing framework for Spark applications\n",
    "import unittest\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class SparkTestCase(unittest.TestCase):\n",
    "    \"\"\"Base test case for Spark applications\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up Spark session for testing\"\"\"\n",
    "        cls.spark = (\n",
    "            SparkSession.builder.appName(\"test-spark-app\")\n",
    "            .master(\"local[2]\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        cls.spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        \"\"\"Clean up Spark session\"\"\"\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def create_test_dataframe(self, data: List[Tuple], columns: List[str]):\n",
    "        \"\"\"Helper to create test DataFrames\"\"\"\n",
    "        return self.spark.createDataFrame(data, columns)\n",
    "\n",
    "    def assert_dataframe_equal(\n",
    "        self, df1: DataFrame, df2: DataFrame, check_schema: bool = True\n",
    "    ):\n",
    "        \"\"\"Assert two DataFrames are equal\"\"\"\n",
    "        if check_schema:\n",
    "            self.assertEqual(df1.schema, df2.schema, \"Schemas don't match\")\n",
    "\n",
    "        # Convert to lists for comparison\n",
    "        rows1 = sorted(df1.collect())\n",
    "        rows2 = sorted(df2.collect())\n",
    "\n",
    "        self.assertEqual(rows1, rows2, \"DataFrames don't match\")\n",
    "\n",
    "\n",
    "# Test cases for our customer transformations\n",
    "class TestCustomerTransformations(SparkTestCase):\n",
    "    \"\"\"Test customer transformation functions\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test data\"\"\"\n",
    "        self.test_data = [\n",
    "            (\"C001\", \"Alice\", 25, 50000.0, \"New York\"),\n",
    "            (\"C002\", \"Bob\", 17, 30000.0, \"Chicago\"),  # Under 18\n",
    "            (\"C003\", \"Charlie\", 35, 75000.0, \"LA\"),\n",
    "            (\"C004\", \"Diana\", 45, 90000.0, \"Houston\"),\n",
    "            (\"C005\", \"Eve\", 16, 25000.0, \"Phoenix\"),  # Under 18\n",
    "        ]\n",
    "\n",
    "        self.columns = [\"customer_id\", \"name\", \"age\", \"income\", \"city\"]\n",
    "        self.df = self.create_test_dataframe(self.test_data, self.columns)\n",
    "\n",
    "    def test_filter_valid_customers(self):\n",
    "        \"\"\"Test filtering customers by minimum age\"\"\"\n",
    "        result = CustomerTransformations.filter_valid_customers(self.df, min_age=18)\n",
    "\n",
    "        # Should have 3 customers (Alice, Charlie, Diana)\n",
    "        self.assertEqual(result.count(), 3)\n",
    "\n",
    "        # All remaining customers should be >= 18\n",
    "        ages = [row.age for row in result.collect()]\n",
    "        self.assertTrue(all(age >= 18 for age in ages))\n",
    "\n",
    "    def test_add_age_group(self):\n",
    "        \"\"\"Test age group categorization\"\"\"\n",
    "        result = CustomerTransformations.add_age_group(self.df)\n",
    "\n",
    "        # Check that age_group column was added\n",
    "        self.assertIn(\"age_group\", result.columns)\n",
    "\n",
    "        # Check age group assignments\n",
    "        age_groups = {row.name: row.age_group for row in result.collect()}\n",
    "\n",
    "        self.assertEqual(age_groups[\"Alice\"], \"Young\")  # 25\n",
    "        self.assertEqual(age_groups[\"Bob\"], \"Young\")  # 17\n",
    "        self.assertEqual(age_groups[\"Charlie\"], \"Middle\")  # 35\n",
    "        self.assertEqual(age_groups[\"Diana\"], \"Middle\")  # 45\n",
    "        self.assertEqual(age_groups[\"Eve\"], \"Young\")  # 16\n",
    "\n",
    "    def test_calculate_age_group_stats(self):\n",
    "        \"\"\"Test age group statistics calculation\"\"\"\n",
    "        df_with_groups = CustomerTransformations.add_age_group(self.df)\n",
    "        result = CustomerTransformations.calculate_age_group_stats(df_with_groups)\n",
    "\n",
    "        # Should have Middle and Young groups\n",
    "        age_groups = [row.age_group for row in result.collect()]\n",
    "        self.assertIn(\"Young\", age_groups)\n",
    "        self.assertIn(\"Middle\", age_groups)\n",
    "\n",
    "        # Check statistics\n",
    "        stats = {\n",
    "            row.age_group: (row[\"count\"], row[\"avg_income\"]) for row in result.collect()\n",
    "        }\n",
    "\n",
    "        # Young: Alice (25, 50k), Bob (17, 30k), Eve (16, 25k)\n",
    "        young_count, young_avg = stats[\"Young\"]\n",
    "        self.assertEqual(young_count, 3)\n",
    "        self.assertAlmostEqual(young_avg, (50000 + 30000 + 25000) / 3, places=0)\n",
    "\n",
    "        # Middle: Charlie (35, 75k), Diana (45, 90k)\n",
    "        middle_count, middle_avg = stats[\"Middle\"]\n",
    "        self.assertEqual(middle_count, 2)\n",
    "        self.assertAlmostEqual(middle_avg, (75000 + 90000) / 2, places=0)\n",
    "\n",
    "\n",
    "print(\"üß™ Unit testing framework created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_add_age_group (__main__.TestCustomerTransformations.test_add_age_group)\n",
      "Test age group categorization ... ok\n",
      "test_calculate_age_group_stats (__main__.TestCustomerTransformations.test_calculate_age_group_stats)\n",
      "Test age group statistics calculation ... ok\n",
      "test_filter_valid_customers (__main__.TestCustomerTransformations.test_filter_valid_customers)\n",
      "Test filtering customers by minimum age ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 1.620s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Results:\n",
      "  Tests run: 3\n",
      "  Failures: 0\n",
      "  Errors: 0\n",
      "‚úÖ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Run the unit tests\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a test suite\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestCustomerTransformations)\n",
    "\n",
    "    # Run the tests\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "\n",
    "    print(\"\\nüìä Test Results:\")\n",
    "    print(f\"  Tests run: {result.testsRun}\")\n",
    "    print(f\"  Failures: {len(result.failures)}\")\n",
    "    print(f\"  Errors: {len(result.errors)}\")\n",
    "\n",
    "    if result.wasSuccessful():\n",
    "        print(\"‚úÖ All tests passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Some tests failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Module 3: Configuration Management\n",
    "\n",
    "Let's explore professional configuration management patterns for different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß 3.1 Hierarchical Configuration System\n",
    "\n",
    "We'll create a flexible configuration system that supports multiple environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Advanced configuration management system created\n"
     ]
    }
   ],
   "source": [
    "# Advanced configuration management system\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SparkConfig:\n",
    "    \"\"\"Spark-specific configuration\"\"\"\n",
    "\n",
    "    app_name: str = \"SparkApp\"\n",
    "    master: str = \"local[*]\"\n",
    "    sql_shuffle_partitions: int = 200\n",
    "    adaptive_enabled: bool = True\n",
    "    adaptive_coalesce_partitions: bool = True\n",
    "    serializer: str = \"org.apache.spark.serializer.KryoSerializer\"\n",
    "\n",
    "    def to_spark_conf(self) -> Dict[str, str]:\n",
    "        \"\"\"Convert to Spark configuration dictionary\"\"\"\n",
    "        return {\n",
    "            \"spark.sql.shuffle.partitions\": str(self.sql_shuffle_partitions),\n",
    "            \"spark.sql.adaptive.enabled\": str(self.adaptive_enabled).lower(),\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": str(\n",
    "                self.adaptive_coalesce_partitions\n",
    "            ).lower(),\n",
    "            \"spark.serializer\": self.serializer,\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Data-related configuration\"\"\"\n",
    "\n",
    "    input_path: str = \"data/raw\"\n",
    "    output_path: str = \"data/processed\"\n",
    "    input_format: str = \"parquet\"\n",
    "    output_format: str = \"delta\"\n",
    "    compression: str = \"snappy\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    \"\"\"Database configuration\"\"\"\n",
    "\n",
    "    host: str = \"localhost\"\n",
    "    port: int = 5432\n",
    "    name: str = \"spark_db\"\n",
    "    username: Optional[str] = None\n",
    "    password: Optional[str] = None\n",
    "\n",
    "    def get_jdbc_url(self) -> str:\n",
    "        \"\"\"Get JDBC connection URL\"\"\"\n",
    "        return f\"jdbc:postgresql://{self.host}:{self.port}/{self.name}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    \"\"\"Main application configuration\"\"\"\n",
    "\n",
    "    environment: str = \"dev\"\n",
    "    debug: bool = True\n",
    "    log_level: str = \"INFO\"\n",
    "\n",
    "    spark: SparkConfig = field(default_factory=SparkConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    \"\"\"Configuration loader with environment support\"\"\"\n",
    "\n",
    "    def __init__(self, config_dir: str = \"configs\"):\n",
    "        self.config_dir = Path(config_dir)\n",
    "\n",
    "    def load_config(self, environment: str = None) -> AppConfig:\n",
    "        \"\"\"Load configuration for specified environment\"\"\"\n",
    "        if environment is None:\n",
    "            environment = os.getenv(\"ENVIRONMENT\", \"dev\")\n",
    "\n",
    "        # Load base configuration\n",
    "        base_config = self._load_yaml_config(\"base.yaml\")\n",
    "\n",
    "        # Load environment-specific configuration\n",
    "        env_config = self._load_yaml_config(f\"{environment}.yaml\")\n",
    "\n",
    "        # Merge configurations (environment overrides base)\n",
    "        merged_config = self._merge_configs(base_config, env_config)\n",
    "\n",
    "        # Resolve environment variables\n",
    "        resolved_config = self._resolve_env_vars(merged_config)\n",
    "\n",
    "        # Convert to AppConfig object\n",
    "        return self._dict_to_config(resolved_config)\n",
    "\n",
    "    def _load_yaml_config(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load YAML configuration file\"\"\"\n",
    "        file_path = self.config_dir / filename\n",
    "\n",
    "        if not file_path.exists():\n",
    "            return {}\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return yaml.safe_load(file) or {}\n",
    "\n",
    "    def _merge_configs(self, base: Dict, override: Dict) -> Dict:\n",
    "        \"\"\"Deep merge two configuration dictionaries\"\"\"\n",
    "        result = base.copy()\n",
    "\n",
    "        for key, value in override.items():\n",
    "            if (\n",
    "                key in result\n",
    "                and isinstance(result[key], dict)\n",
    "                and isinstance(value, dict)\n",
    "            ):\n",
    "                result[key] = self._merge_configs(result[key], value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _resolve_env_vars(self, config: Dict) -> Dict:\n",
    "        \"\"\"Resolve environment variables in configuration\"\"\"\n",
    "\n",
    "        def resolve_value(value):\n",
    "            if (\n",
    "                isinstance(value, str)\n",
    "                and value.startswith(\"${\")\n",
    "                and value.endswith(\"}\")\n",
    "            ):\n",
    "                env_var = value[2:-1]\n",
    "                return os.getenv(env_var, value)\n",
    "            elif isinstance(value, dict):\n",
    "                return {k: resolve_value(v) for k, v in value.items()}\n",
    "            elif isinstance(value, list):\n",
    "                return [resolve_value(item) for item in value]\n",
    "            return value\n",
    "\n",
    "        return resolve_value(config)\n",
    "\n",
    "    def _dict_to_config(self, config_dict: Dict) -> AppConfig:\n",
    "        \"\"\"Convert dictionary to AppConfig object\"\"\"\n",
    "        # Extract sections\n",
    "        app_section = config_dict.get(\"app\", {})\n",
    "        spark_section = config_dict.get(\"spark\", {})\n",
    "        data_section = config_dict.get(\"data\", {})\n",
    "        database_section = config_dict.get(\"database\", {})\n",
    "\n",
    "        # Create configuration objects\n",
    "        spark_config = SparkConfig(**spark_section)\n",
    "        data_config = DataConfig(**data_section)\n",
    "        database_config = DatabaseConfig(**database_section)\n",
    "\n",
    "        # Create main config\n",
    "        return AppConfig(\n",
    "            spark=spark_config,\n",
    "            data=data_config,\n",
    "            database=database_config,\n",
    "            **app_section,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"‚öôÔ∏è Advanced configuration management system created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù 3.2 Creating Configuration Files\n",
    "\n",
    "Let's create sample configuration files for different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Configuration files created:\n",
      "  üìÑ configs/dev.yaml\n",
      "  üìÑ configs/base.yaml\n",
      "  üìÑ configs/prod.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create sample configuration files\n",
    "configs_dir = Path(\"configs\")\n",
    "configs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Base configuration\n",
    "base_config = {\n",
    "    \"app\": {\"log_level\": \"INFO\"},\n",
    "    \"spark\": {\n",
    "        \"app_name\": \"CustomerAnalytics\",\n",
    "        \"sql_shuffle_partitions\": 200,\n",
    "        \"adaptive_enabled\": True,\n",
    "        \"serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_format\": \"parquet\",\n",
    "        \"output_format\": \"delta\",\n",
    "        \"compression\": \"snappy\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Development configuration\n",
    "dev_config = {\n",
    "    \"app\": {\"environment\": \"dev\", \"debug\": True, \"log_level\": \"DEBUG\"},\n",
    "    \"spark\": {\"master\": \"local[2]\", \"sql_shuffle_partitions\": 4},\n",
    "    \"data\": {\"input_path\": \"data/dev/input\", \"output_path\": \"data/dev/output\"},\n",
    "    \"database\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 5432,\n",
    "        \"name\": \"dev_database\",\n",
    "        \"username\": \"dev_user\",\n",
    "        \"password\": \"dev_password\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Production configuration\n",
    "prod_config = {\n",
    "    \"app\": {\"environment\": \"prod\", \"debug\": False, \"log_level\": \"WARN\"},\n",
    "    \"spark\": {\"master\": \"yarn\", \"sql_shuffle_partitions\": 1000},\n",
    "    \"data\": {\n",
    "        \"input_path\": \"s3a://prod-data-lake/input\",\n",
    "        \"output_path\": \"s3a://prod-data-lake/output\",\n",
    "    },\n",
    "    \"database\": {\n",
    "        \"host\": \"${DB_HOST}\",\n",
    "        \"port\": \"${DB_PORT}\",\n",
    "        \"name\": \"${DB_NAME}\",\n",
    "        \"username\": \"${DB_USERNAME}\",\n",
    "        \"password\": \"${DB_PASSWORD}\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Write configuration files\n",
    "with open(configs_dir / \"base.yaml\", \"w\") as f:\n",
    "    yaml.dump(base_config, f, default_flow_style=False)\n",
    "\n",
    "with open(configs_dir / \"dev.yaml\", \"w\") as f:\n",
    "    yaml.dump(dev_config, f, default_flow_style=False)\n",
    "\n",
    "with open(configs_dir / \"prod.yaml\", \"w\") as f:\n",
    "    yaml.dump(prod_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"üìù Configuration files created:\")\n",
    "for config_file in configs_dir.glob(\"*.yaml\"):\n",
    "    print(f\"  üìÑ {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ 3.3 Testing Configuration Loading\n",
    "\n",
    "Let's test our configuration system with different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading Development Configuration:\n",
      "==================================================\n",
      "Environment: dev\n",
      "Debug mode: True\n",
      "Log level: DEBUG\n",
      "\n",
      "Spark configuration:\n",
      "  Master: local[2]\n",
      "  App name: CustomerAnalytics\n",
      "  Shuffle partitions: 4\n",
      "\n",
      "Data configuration:\n",
      "  Input path: data/dev/input\n",
      "  Output path: data/dev/output\n",
      "\n",
      "Database configuration:\n",
      "  JDBC URL: jdbc:postgresql://localhost:5432/dev_database\n",
      "  Username: dev_user\n"
     ]
    }
   ],
   "source": [
    "# Test configuration loading\n",
    "config_loader = ConfigLoader(\"configs\")\n",
    "\n",
    "# Load development configuration\n",
    "print(\"üîß Loading Development Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "dev_config = config_loader.load_config(\"dev\")\n",
    "\n",
    "print(f\"Environment: {dev_config.environment}\")\n",
    "print(f\"Debug mode: {dev_config.debug}\")\n",
    "print(f\"Log level: {dev_config.log_level}\")\n",
    "print(\"\\nSpark configuration:\")\n",
    "print(f\"  Master: {dev_config.spark.master}\")\n",
    "print(f\"  App name: {dev_config.spark.app_name}\")\n",
    "print(f\"  Shuffle partitions: {dev_config.spark.sql_shuffle_partitions}\")\n",
    "print(\"\\nData configuration:\")\n",
    "print(f\"  Input path: {dev_config.data.input_path}\")\n",
    "print(f\"  Output path: {dev_config.data.output_path}\")\n",
    "print(\"\\nDatabase configuration:\")\n",
    "print(f\"  JDBC URL: {dev_config.database.get_jdbc_url()}\")\n",
    "print(f\"  Username: {dev_config.database.username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè≠ Loading Production Configuration:\n",
      "==================================================\n",
      "Environment: prod\n",
      "Debug mode: False\n",
      "Log level: WARN\n",
      "\n",
      "Spark configuration:\n",
      "  Master: yarn\n",
      "  Shuffle partitions: 1000\n",
      "\n",
      "Data configuration:\n",
      "  Input path: s3a://prod-data-lake/input\n",
      "  Output path: s3a://prod-data-lake/output\n",
      "\n",
      "Database configuration:\n",
      "  JDBC URL: jdbc:postgresql://prod-db.company.com:5432/prod_analytics\n",
      "  Username: analytics_user\n",
      "  Password: *******************\n"
     ]
    }
   ],
   "source": [
    "# Load production configuration (with environment variables)\n",
    "print(\"\\nüè≠ Loading Production Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set some environment variables for demonstration\n",
    "os.environ[\"DB_HOST\"] = \"prod-db.company.com\"\n",
    "os.environ[\"DB_PORT\"] = \"5432\"\n",
    "os.environ[\"DB_NAME\"] = \"prod_analytics\"\n",
    "os.environ[\"DB_USERNAME\"] = \"analytics_user\"\n",
    "os.environ[\"DB_PASSWORD\"] = \"secure_password_123\"\n",
    "\n",
    "prod_config = config_loader.load_config(\"prod\")\n",
    "\n",
    "print(f\"Environment: {prod_config.environment}\")\n",
    "print(f\"Debug mode: {prod_config.debug}\")\n",
    "print(f\"Log level: {prod_config.log_level}\")\n",
    "print(\"\\nSpark configuration:\")\n",
    "print(f\"  Master: {prod_config.spark.master}\")\n",
    "print(f\"  Shuffle partitions: {prod_config.spark.sql_shuffle_partitions}\")\n",
    "print(\"\\nData configuration:\")\n",
    "print(f\"  Input path: {prod_config.data.input_path}\")\n",
    "print(f\"  Output path: {prod_config.data.output_path}\")\n",
    "print(\"\\nDatabase configuration:\")\n",
    "print(f\"  JDBC URL: {prod_config.database.get_jdbc_url()}\")\n",
    "print(f\"  Username: {prod_config.database.username}\")\n",
    "print(f\"  Password: {'*' * len(prod_config.database.password)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Spark Configuration for Development:\n",
      "==================================================\n",
      "  spark.sql.shuffle.partitions: 4\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "  spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "\n",
      "üöÄ Creating Spark session with configuration...\n",
      "‚úÖ Spark session created: CustomerAnalytics\n",
      "üìä Shuffle partitions: 4\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Spark configuration conversion\n",
    "print(\"\\n‚öôÔ∏è Spark Configuration for Development:\")\n",
    "print(\"=\" * 50)\n",
    "spark_conf = dev_config.spark.to_spark_conf()\n",
    "for key, value in spark_conf.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create Spark session with configuration\n",
    "print(\"\\nüöÄ Creating Spark session with configuration...\")\n",
    "builder = SparkSession.builder.appName(dev_config.spark.app_name).master(\n",
    "    dev_config.spark.master\n",
    ")\n",
    "\n",
    "for key, value in spark_conf.items():\n",
    "    builder = builder.config(key, value)\n",
    "\n",
    "configured_spark = builder.getOrCreate()\n",
    "print(f\"‚úÖ Spark session created: {configured_spark.sparkContext.appName}\")\n",
    "print(\n",
    "    f\"üìä Shuffle partitions: {configured_spark.conf.get('spark.sql.shuffle.partitions')}\"\n",
    ")\n",
    "\n",
    "configured_spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the interactive tutorial for professional Spark development setup. Let's summarize what you've learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ What You've Accomplished\n",
    "\n",
    "1. **üèóÔ∏è Project Structure Best Practices**\n",
    "   - Created modular Spark application architecture\n",
    "   - Implemented separation of concerns\n",
    "   - Built reusable components and utilities\n",
    "\n",
    "2. **üîß Development Workflow**\n",
    "   - Developed DataFrame debugging utilities\n",
    "   - Created comprehensive testing framework\n",
    "   - Implemented performance profiling tools\n",
    "\n",
    "3. **‚öôÔ∏è Configuration Management**\n",
    "   - Built hierarchical configuration system\n",
    "   - Implemented environment-specific settings\n",
    "   - Created secrets management patterns\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "- **Modularity**: Break your code into small, focused, testable components\n",
    "- **Configuration**: Use environment-specific configurations for flexibility\n",
    "- **Testing**: Write comprehensive tests for your transformations and logic\n",
    "- **Automation**: Use tools to enforce code quality and consistency\n",
    "- **Documentation**: Code should be self-documenting with clear structure\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Complete the exercises** in the `exercises/` directory\n",
    "2. **Explore the project templates** in the `templates/` directory\n",
    "3. **Set up your own project** using the patterns you've learned\n",
    "4. **Move on to Lesson 4**: File Formats Deep Dive\n",
    "\n",
    "### üìö Additional Practice\n",
    "\n",
    "Try these challenges to reinforce your learning:\n",
    "\n",
    "1. Create a new Spark project using the modular structure\n",
    "2. Implement configuration for staging environment\n",
    "3. Add more comprehensive test cases\n",
    "4. Set up a CI/CD pipeline using GitHub Actions\n",
    "5. Containerize your application with Docker\n",
    "\n",
    "### üÜò Getting Help\n",
    "\n",
    "If you encounter issues:\n",
    "1. Check the troubleshooting section in the README\n",
    "2. Run the validation scripts: `make validate-learning`\n",
    "3. Review the solution files in `solutions/`\n",
    "4. Use the debugging utilities you've learned\n",
    "\n",
    "**Happy coding! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
