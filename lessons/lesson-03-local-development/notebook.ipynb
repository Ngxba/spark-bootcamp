{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Local Development Setup\n",
    "\n",
    "## 🎯 Interactive Tutorial: Professional Spark Development\n",
    "\n",
    "Welcome to Lesson 3! In this notebook, we'll explore professional development practices for Apache Spark applications. You'll learn how to structure projects, manage configurations, implement testing, and integrate development tools.\n",
    "\n",
    "### 📋 What You'll Learn\n",
    "1. **Project Structure Best Practices** - Modular, maintainable Spark applications\n",
    "2. **Development Workflow** - Debugging, testing, and quality assurance\n",
    "3. **Configuration Management** - Environment-specific settings and secrets\n",
    "4. **Development Tools Integration** - Git, Docker, CI/CD foundations\n",
    "\n",
    "### 🔧 Setup\n",
    "Make sure you've completed the environment setup:\n",
    "```bash\n",
    "make setup\n",
    "make install-dev\n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 PySpark version: 3.5.7\n",
      "📁 Project root: /Users/liamnguyen/Documents/0.Coding/spark-bootcamp/lessons/lesson-03-local-development\n",
      "🐍 Python path: /Users/liamnguyen/Documents/0.Coding/spark-bootcamp/lessons/lesson-03-local-development/.venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "# Initial setup and imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().absolute()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Essential imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, avg, count\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "print(f\"🚀 PySpark version: {pyspark.__version__}\")\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"🐍 Python path: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏗️ Module 1: Project Structure Best Practices\n",
    "\n",
    "Let's start by understanding how to structure a professional Spark project. We'll demonstrate the key principles through practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📂 1.1 Demonstrating Project Structure\n",
    "\n",
    "Let's examine what a well-structured Spark project looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Recommended Project Structure:\n",
      "\n",
      "    my-spark-project/\n",
      "    ├── README.md                   # Project documentation\n",
      "    ├── pyproject.toml             # Dependencies and configuration\n",
      "    ├── Makefile                   # Development commands\n",
      "    ├── .env.example               # Environment variables template\n",
      "    ├── .gitignore                 # Git ignore patterns\n",
      "    │\n",
      "    ├── src/                       # Source code (production)\n",
      "    │   ├── config/                # Configuration management\n",
      "    │   │   ├── settings.py        # Application settings\n",
      "    │   │   └── environments/      # Environment-specific configs\n",
      "    │   ├── jobs/                  # Spark job definitions\n",
      "    │   │   ├── base_job.py        # Abstract base job class\n",
      "    │   │   └── etl_job.py         # ETL job implementation\n",
      "    │   ├── transformations/       # Data transformation functions\n",
      "    │   │   ├── cleaning.py        # Data cleaning functions\n",
      "    │   │   └── aggregations.py    # Aggregation functions\n",
      "    │   ├── utils/                 # Utility functions\n",
      "    │   │   ├── spark_utils.py     # Spark session and utilities\n",
      "    │   │   └── io_utils.py        # Input/output helpers\n",
      "    │   └── schemas/               # Data schemas\n",
      "    │       └── input_schemas.py   # Input data schemas\n",
      "    │\n",
      "    ├── tests/                     # Test suite\n",
      "    │   ├── conftest.py           # Pytest configuration\n",
      "    │   ├── unit/                 # Unit tests\n",
      "    │   └── integration/          # Integration tests\n",
      "    │\n",
      "    ├── data/                     # Local data directory\n",
      "    │   ├── raw/                  # Raw input data\n",
      "    │   └── processed/            # Processed data\n",
      "    │\n",
      "    └── scripts/                  # Utility scripts\n",
      "        ├── setup.sh             # Environment setup\n",
      "        └── run_job.py           # Job runner script\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Let's create a sample project structure to demonstrate\n",
    "def show_project_structure():\n",
    "    \"\"\"Display recommended project structure\"\"\"\n",
    "    structure = \"\"\"\n",
    "    my-spark-project/\n",
    "    ├── README.md                   # Project documentation\n",
    "    ├── pyproject.toml             # Dependencies and configuration\n",
    "    ├── Makefile                   # Development commands\n",
    "    ├── .env.example               # Environment variables template\n",
    "    ├── .gitignore                 # Git ignore patterns\n",
    "    │\n",
    "    ├── src/                       # Source code (production)\n",
    "    │   ├── config/                # Configuration management\n",
    "    │   │   ├── settings.py        # Application settings\n",
    "    │   │   └── environments/      # Environment-specific configs\n",
    "    │   ├── jobs/                  # Spark job definitions\n",
    "    │   │   ├── base_job.py        # Abstract base job class\n",
    "    │   │   └── etl_job.py         # ETL job implementation\n",
    "    │   ├── transformations/       # Data transformation functions\n",
    "    │   │   ├── cleaning.py        # Data cleaning functions\n",
    "    │   │   └── aggregations.py    # Aggregation functions\n",
    "    │   ├── utils/                 # Utility functions\n",
    "    │   │   ├── spark_utils.py     # Spark session and utilities\n",
    "    │   │   └── io_utils.py        # Input/output helpers\n",
    "    │   └── schemas/               # Data schemas\n",
    "    │       └── input_schemas.py   # Input data schemas\n",
    "    │\n",
    "    ├── tests/                     # Test suite\n",
    "    │   ├── conftest.py           # Pytest configuration\n",
    "    │   ├── unit/                 # Unit tests\n",
    "    │   └── integration/          # Integration tests\n",
    "    │\n",
    "    ├── data/                     # Local data directory\n",
    "    │   ├── raw/                  # Raw input data\n",
    "    │   └── processed/            # Processed data\n",
    "    │\n",
    "    └── scripts/                  # Utility scripts\n",
    "        ├── setup.sh             # Environment setup\n",
    "        └── run_job.py           # Job runner script\n",
    "    \"\"\"\n",
    "    print(\"📂 Recommended Project Structure:\")\n",
    "    print(structure)\n",
    "\n",
    "\n",
    "show_project_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 1.2 Separation of Concerns Example\n",
    "\n",
    "Let's see how to properly separate different concerns in a Spark application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Monolithic approach - everything mixed together\n"
     ]
    }
   ],
   "source": [
    "# Example: Proper separation of concerns\n",
    "\n",
    "# ❌ BAD: Everything in one function\n",
    "def monolithic_data_processing():\n",
    "    # Spark session creation\n",
    "    spark = SparkSession.builder.appName(\"MonolithicApp\").getOrCreate()\n",
    "\n",
    "    # Data loading\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"data/raw/customers.csv\")\n",
    "\n",
    "    # Business logic mixed with technical concerns\n",
    "    processed_df = (\n",
    "        df.filter(col(\"age\") > 18)\n",
    "        .withColumn(\n",
    "            \"age_group\",\n",
    "            when(col(\"age\") < 30, \"Young\")\n",
    "            .when(col(\"age\") < 50, \"Middle\")\n",
    "            .otherwise(\"Senior\"),\n",
    "        )\n",
    "        .groupBy(\"age_group\")\n",
    "        .agg(count(\"*\").alias(\"count\"), avg(\"income\").alias(\"avg_income\"))\n",
    "    )\n",
    "\n",
    "    # Output writing\n",
    "    processed_df.write.mode(\"overwrite\").parquet(\"data/processed/customer_analysis\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "monolithic_data_processing()\n",
    "print(\"❌ Monolithic approach - everything mixed together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modular approach - separated concerns with clear responsibilities\n"
     ]
    }
   ],
   "source": [
    "# ✅ GOOD: Separated concerns\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# 1. Configuration Management\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    app_name: str = \"CustomerAnalysis\"\n",
    "    input_path: str = \"data/raw/customers.csv\"\n",
    "    output_path: str = \"data/processed/customer_analysis\"\n",
    "    min_age: int = 18\n",
    "\n",
    "\n",
    "# 2. Spark Utilities\n",
    "class SparkUtils:\n",
    "    @staticmethod\n",
    "    def get_spark_session(app_name: str) -> SparkSession:\n",
    "        \"\"\"Create optimized Spark session\"\"\"\n",
    "        return (\n",
    "            SparkSession.builder.appName(app_name)\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "\n",
    "# 3. Data Schema Definition\n",
    "class CustomerSchema:\n",
    "    SCHEMA = StructType(\n",
    "        [\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"income\", DoubleType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# 4. Business Logic (Transformations)\n",
    "class CustomerTransformations:\n",
    "    @staticmethod\n",
    "    def filter_valid_customers(df, min_age: int = 18):\n",
    "        \"\"\"Filter customers with valid age\"\"\"\n",
    "        return df.filter(col(\"age\") >= min_age)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_age_group(df):\n",
    "        \"\"\"Add age group categorization\"\"\"\n",
    "        return df.withColumn(\n",
    "            \"age_group\",\n",
    "            when(col(\"age\") < 30, \"Young\")\n",
    "            .when(col(\"age\") < 50, \"Middle\")\n",
    "            .otherwise(\"Senior\"),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_age_group_stats(df):\n",
    "        \"\"\"Calculate statistics by age group\"\"\"\n",
    "        return (\n",
    "            df.groupBy(\"age_group\")\n",
    "            .agg(count(\"*\").alias(\"count\"), avg(\"income\").alias(\"avg_income\"))\n",
    "            .orderBy(\"age_group\")\n",
    "        )\n",
    "\n",
    "\n",
    "# 5. I/O Operations\n",
    "class DataIO:\n",
    "    @staticmethod\n",
    "    def read_customer_data(spark: SparkSession, path: str):\n",
    "        \"\"\"Read customer data with schema\"\"\"\n",
    "        return (\n",
    "            spark.read.schema(CustomerSchema.SCHEMA).option(\"header\", \"true\").csv(path)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def write_analysis_results(df, path: str):\n",
    "        \"\"\"Write analysis results\"\"\"\n",
    "        (df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(path))\n",
    "\n",
    "\n",
    "print(\"✅ Modular approach - separated concerns with clear responsibilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ 1.3 Putting It All Together\n",
    "\n",
    "Now let's see how these separated components work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Complete modular job structure defined\n"
     ]
    }
   ],
   "source": [
    "# 6. Main Job Class\n",
    "class CustomerAnalysisJob:\n",
    "    def __init__(self, config: AppConfig):\n",
    "        self.config = config\n",
    "        self.spark = SparkUtils.get_spark_session(config.app_name)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete analysis pipeline\"\"\"\n",
    "        try:\n",
    "            print(f\"🚀 Starting {self.config.app_name}\")\n",
    "\n",
    "            # Extract\n",
    "            print(\"📥 Loading customer data...\")\n",
    "            raw_data = DataIO.read_customer_data(self.spark, self.config.input_path)\n",
    "            print(f\"📊 Loaded {raw_data.count():,} customer records\")\n",
    "\n",
    "            # Transform\n",
    "            print(\"🔄 Applying transformations...\")\n",
    "            valid_customers = CustomerTransformations.filter_valid_customers(\n",
    "                raw_data, self.config.min_age\n",
    "            )\n",
    "            customers_with_groups = CustomerTransformations.add_age_group(\n",
    "                valid_customers\n",
    "            )\n",
    "            analysis_results = CustomerTransformations.calculate_age_group_stats(\n",
    "                customers_with_groups\n",
    "            )\n",
    "\n",
    "            print(\"📈 Analysis results:\")\n",
    "            analysis_results.show()\n",
    "\n",
    "            # Load\n",
    "            print(f\"💾 Saving results to {self.config.output_path}\")\n",
    "            DataIO.write_analysis_results(analysis_results, self.config.output_path)\n",
    "\n",
    "            print(\"✅ Job completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Job failed: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.spark.stop()\n",
    "\n",
    "\n",
    "print(\"✅ Complete modular job structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 1.4 Testing the Modular Structure\n",
    "\n",
    "Let's create some sample data and test our modular structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Created test data at: /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/input/customers.csv\n",
      "📊 Sample data:\n",
      "  customer_id           name  age  income          city\n",
      "0        C001  Alice Johnson   25   50000      New York\n",
      "1        C002      Bob Smith   35   75000       Chicago\n",
      "2        C003  Charlie Brown   45   90000   Los Angeles\n",
      "3        C004   Diana Wilson   55  120000       Houston\n",
      "4        C005      Eve Davis   17   25000       Phoenix\n",
      "5        C006   Frank Miller   30   60000  Philadelphia\n",
      "\n",
      "🔧 Configuration:\n",
      "  Input: /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/input/customers.csv\n",
      "  Output: /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/output/customer_analysis\n",
      "  Min age: 18\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for demonstration\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample customer data\n",
    "sample_data = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C004\", \"C005\", \"C006\"],\n",
    "        \"name\": [\n",
    "            \"Alice Johnson\",\n",
    "            \"Bob Smith\",\n",
    "            \"Charlie Brown\",\n",
    "            \"Diana Wilson\",\n",
    "            \"Eve Davis\",\n",
    "            \"Frank Miller\",\n",
    "        ],\n",
    "        \"age\": [25, 35, 45, 55, 17, 30],  # Note: one customer under 18\n",
    "        \"income\": [50000, 75000, 90000, 120000, 25000, 60000],\n",
    "        \"city\": [\n",
    "            \"New York\",\n",
    "            \"Chicago\",\n",
    "            \"Los Angeles\",\n",
    "            \"Houston\",\n",
    "            \"Phoenix\",\n",
    "            \"Philadelphia\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create temporary directories\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "input_dir = temp_dir / \"input\"\n",
    "output_dir = temp_dir / \"output\"\n",
    "input_dir.mkdir(exist_ok=True)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save sample data\n",
    "input_file = input_dir / \"customers.csv\"\n",
    "sample_data.to_csv(input_file, index=False)\n",
    "\n",
    "print(f\"📁 Created test data at: {input_file}\")\n",
    "print(\"📊 Sample data:\")\n",
    "print(sample_data)\n",
    "\n",
    "# Configure and run the job\n",
    "config = AppConfig(\n",
    "    input_path=str(input_file), output_path=str(output_dir / \"customer_analysis\")\n",
    ")\n",
    "\n",
    "print(\"\\n🔧 Configuration:\")\n",
    "print(f\"  Input: {config.input_path}\")\n",
    "print(f\"  Output: {config.output_path}\")\n",
    "print(f\"  Min age: {config.min_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting CustomerAnalysis\n",
      "📥 Loading customer data...\n",
      "📊 Loaded 6 customer records\n",
      "🔄 Applying transformations...\n",
      "📈 Analysis results:\n",
      "+---------+-----+----------+\n",
      "|age_group|count|avg_income|\n",
      "+---------+-----+----------+\n",
      "|   Middle|    3|   75000.0|\n",
      "|   Senior|    1|  120000.0|\n",
      "|    Young|    1|   50000.0|\n",
      "+---------+-----+----------+\n",
      "\n",
      "💾 Saving results to /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/tmpldw_67m5/output/customer_analysis\n",
      "✅ Job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run the modular job\n",
    "job = CustomerAnalysisJob(config)\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔧 Module 2: Development Workflow\n",
    "\n",
    "Now let's explore professional development workflows, including debugging techniques and testing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐛 2.1 DataFrame Debugging Utilities\n",
    "\n",
    "Debugging DataFrames can be challenging. Let's create utilities to make it easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 DataFrame debugging utilities created\n"
     ]
    }
   ],
   "source": [
    "# Advanced debugging utilities for DataFrames\n",
    "from pyspark.sql import DataFrame\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "class DataFrameDebugger:\n",
    "    \"\"\"Comprehensive DataFrame debugging utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_dataframe(\n",
    "        df: DataFrame,\n",
    "        name: str = \"DataFrame\",\n",
    "        show_rows: int = 10,\n",
    "        show_schema: bool = True,\n",
    "        show_count: bool = True,\n",
    "        show_sample: bool = True,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Comprehensive DataFrame debugging\"\"\"\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔍 DEBUG: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        if show_schema:\n",
    "            print(\"\\n📋 Schema:\")\n",
    "            df.printSchema()\n",
    "\n",
    "        if show_count:\n",
    "            count = df.count()\n",
    "            print(f\"\\n📊 Row Count: {count:,} rows\")\n",
    "\n",
    "        if show_sample and df.count() > 0:\n",
    "            print(f\"\\n🔍 Sample Data (first {show_rows} rows):\")\n",
    "            df.show(show_rows, truncate=False)\n",
    "\n",
    "            # Show data types and null counts\n",
    "            print(\"\\n📈 Column Statistics:\")\n",
    "            for column in df.columns:\n",
    "                null_count = df.filter(col(column).isNull()).count()\n",
    "                dtype = dict(df.dtypes)[column]\n",
    "                print(f\"  {column:20} | Type: {dtype:15} | Nulls: {null_count:,}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def profile_operation(operation_name: str):\n",
    "        \"\"\"Decorator to profile Spark operations\"\"\"\n",
    "\n",
    "        def decorator(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start_time = time.time()\n",
    "\n",
    "                print(f\"\\n🚀 Starting: {operation_name}\")\n",
    "                result = func(*args, **kwargs)\n",
    "\n",
    "                # Force action if result is DataFrame\n",
    "                if hasattr(result, \"count\"):\n",
    "                    count = result.count()\n",
    "                    execution_time = time.time() - start_time\n",
    "                    print(f\"✅ Completed: {operation_name}\")\n",
    "                    print(f\"⏱️  Execution time: {execution_time:.2f} seconds\")\n",
    "                    print(f\"📊 Result count: {count:,} rows\")\n",
    "                else:\n",
    "                    execution_time = time.time() - start_time\n",
    "                    print(f\"✅ Completed: {operation_name}\")\n",
    "                    print(f\"⏱️  Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "                return result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        return decorator\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_dataframes(\n",
    "        df1: DataFrame,\n",
    "        df2: DataFrame,\n",
    "        name1: str = \"DataFrame 1\",\n",
    "        name2: str = \"DataFrame 2\",\n",
    "    ):\n",
    "        \"\"\"Compare two DataFrames\"\"\"\n",
    "        print(f\"\\n🔍 Comparing {name1} vs {name2}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Compare counts\n",
    "        count1, count2 = df1.count(), df2.count()\n",
    "        print(f\"📊 Row counts: {name1}: {count1:,}, {name2}: {count2:,}\")\n",
    "\n",
    "        # Compare schemas\n",
    "        cols1, cols2 = set(df1.columns), set(df2.columns)\n",
    "        print(f\"📋 Column counts: {name1}: {len(cols1)}, {name2}: {len(cols2)}\")\n",
    "\n",
    "        if cols1 != cols2:\n",
    "            print(\"⚠️  Schema differences:\")\n",
    "            print(f\"  Only in {name1}: {cols1 - cols2}\")\n",
    "            print(f\"  Only in {name2}: {cols2 - cols1}\")\n",
    "        else:\n",
    "            print(\"✅ Schemas match\")\n",
    "\n",
    "\n",
    "print(\"🔧 DataFrame debugging utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 2.2 Testing Our Debug Utilities\n",
    "\n",
    "Let's test our debugging utilities with the customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔍 DEBUG: Raw Customer Data\n",
      "============================================================\n",
      "\n",
      "📋 Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "\n",
      "📊 Row Count: 6 rows\n",
      "\n",
      "🔍 Sample Data (first 10 rows):\n",
      "+-----------+-------------+---+--------+------------+\n",
      "|customer_id|name         |age|income  |city        |\n",
      "+-----------+-------------+---+--------+------------+\n",
      "|C001       |Alice Johnson|25 |50000.0 |New York    |\n",
      "|C002       |Bob Smith    |35 |75000.0 |Chicago     |\n",
      "|C003       |Charlie Brown|45 |90000.0 |Los Angeles |\n",
      "|C004       |Diana Wilson |55 |120000.0|Houston     |\n",
      "|C005       |Eve Davis    |17 |25000.0 |Phoenix     |\n",
      "|C006       |Frank Miller |30 |60000.0 |Philadelphia|\n",
      "+-----------+-------------+---+--------+------------+\n",
      "\n",
      "\n",
      "📈 Column Statistics:\n",
      "  customer_id          | Type: string          | Nulls: 0\n",
      "  name                 | Type: string          | Nulls: 0\n",
      "  age                  | Type: int             | Nulls: 0\n",
      "  income               | Type: double          | Nulls: 0\n",
      "  city                 | Type: string          | Nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a new Spark session for debugging demonstration\n",
    "spark = SparkUtils.get_spark_session(\"DebuggingDemo\")\n",
    "\n",
    "# Load and debug the customer data\n",
    "customer_df = DataIO.read_customer_data(spark, str(input_file))\n",
    "customer_df = DataFrameDebugger.debug_dataframe(customer_df, \"Raw Customer Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting: Filter Valid Customers\n",
      "✅ Completed: Filter Valid Customers\n",
      "⏱️  Execution time: 0.05 seconds\n",
      "📊 Result count: 5 rows\n",
      "\n",
      "🚀 Starting: Add Age Groups\n",
      "✅ Completed: Add Age Groups\n",
      "⏱️  Execution time: 0.05 seconds\n",
      "📊 Result count: 5 rows\n",
      "\n",
      "============================================================\n",
      "🔍 DEBUG: Customers with Age Groups\n",
      "============================================================\n",
      "\n",
      "📋 Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age_group: string (nullable = false)\n",
      "\n",
      "\n",
      "📊 Row Count: 5 rows\n",
      "\n",
      "🔍 Sample Data (first 10 rows):\n",
      "+-----------+-------------+---+--------+------------+---------+\n",
      "|customer_id|name         |age|income  |city        |age_group|\n",
      "+-----------+-------------+---+--------+------------+---------+\n",
      "|C001       |Alice Johnson|25 |50000.0 |New York    |Young    |\n",
      "|C002       |Bob Smith    |35 |75000.0 |Chicago     |Middle   |\n",
      "|C003       |Charlie Brown|45 |90000.0 |Los Angeles |Middle   |\n",
      "|C004       |Diana Wilson |55 |120000.0|Houston     |Senior   |\n",
      "|C006       |Frank Miller |30 |60000.0 |Philadelphia|Middle   |\n",
      "+-----------+-------------+---+--------+------------+---------+\n",
      "\n",
      "\n",
      "📈 Column Statistics:\n",
      "  customer_id          | Type: string          | Nulls: 0\n",
      "  name                 | Type: string          | Nulls: 0\n",
      "  age                  | Type: int             | Nulls: 0\n",
      "  income               | Type: double          | Nulls: 0\n",
      "  city                 | Type: string          | Nulls: 0\n",
      "  age_group            | Type: string          | Nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations with profiling\n",
    "@DataFrameDebugger.profile_operation(\"Filter Valid Customers\")\n",
    "def filter_customers_with_profiling(df):\n",
    "    return CustomerTransformations.filter_valid_customers(df)\n",
    "\n",
    "\n",
    "@DataFrameDebugger.profile_operation(\"Add Age Groups\")\n",
    "def add_age_groups_with_profiling(df):\n",
    "    return CustomerTransformations.add_age_group(df)\n",
    "\n",
    "\n",
    "# Apply transformations\n",
    "filtered_df = filter_customers_with_profiling(customer_df)\n",
    "grouped_df = add_age_groups_with_profiling(filtered_df)\n",
    "\n",
    "# Debug the final result\n",
    "final_df = DataFrameDebugger.debug_dataframe(grouped_df, \"Customers with Age Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Comparing Original Data vs Filtered Data (Age >= 18)\n",
      "==================================================\n",
      "📊 Row counts: Original Data: 6, Filtered Data (Age >= 18): 5\n",
      "📋 Column counts: Original Data: 5, Filtered Data (Age >= 18): 5\n",
      "✅ Schemas match\n"
     ]
    }
   ],
   "source": [
    "# Compare original vs filtered data\n",
    "DataFrameDebugger.compare_dataframes(\n",
    "    customer_df, filtered_df, \"Original Data\", \"Filtered Data (Age >= 18)\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 2.3 Unit Testing Framework\n",
    "\n",
    "Let's create a comprehensive testing framework for our Spark components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Unit testing framework created\n"
     ]
    }
   ],
   "source": [
    "# Unit testing framework for Spark applications\n",
    "import unittest\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class SparkTestCase(unittest.TestCase):\n",
    "    \"\"\"Base test case for Spark applications\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up Spark session for testing\"\"\"\n",
    "        cls.spark = (\n",
    "            SparkSession.builder.appName(\"test-spark-app\")\n",
    "            .master(\"local[2]\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        cls.spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        \"\"\"Clean up Spark session\"\"\"\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def create_test_dataframe(self, data: List[Tuple], columns: List[str]):\n",
    "        \"\"\"Helper to create test DataFrames\"\"\"\n",
    "        return self.spark.createDataFrame(data, columns)\n",
    "\n",
    "    def assert_dataframe_equal(\n",
    "        self, df1: DataFrame, df2: DataFrame, check_schema: bool = True\n",
    "    ):\n",
    "        \"\"\"Assert two DataFrames are equal\"\"\"\n",
    "        if check_schema:\n",
    "            self.assertEqual(df1.schema, df2.schema, \"Schemas don't match\")\n",
    "\n",
    "        # Convert to lists for comparison\n",
    "        rows1 = sorted(df1.collect())\n",
    "        rows2 = sorted(df2.collect())\n",
    "\n",
    "        self.assertEqual(rows1, rows2, \"DataFrames don't match\")\n",
    "\n",
    "\n",
    "# Test cases for our customer transformations\n",
    "class TestCustomerTransformations(SparkTestCase):\n",
    "    \"\"\"Test customer transformation functions\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test data\"\"\"\n",
    "        self.test_data = [\n",
    "            (\"C001\", \"Alice\", 25, 50000.0, \"New York\"),\n",
    "            (\"C002\", \"Bob\", 17, 30000.0, \"Chicago\"),  # Under 18\n",
    "            (\"C003\", \"Charlie\", 35, 75000.0, \"LA\"),\n",
    "            (\"C004\", \"Diana\", 45, 90000.0, \"Houston\"),\n",
    "            (\"C005\", \"Eve\", 16, 25000.0, \"Phoenix\"),  # Under 18\n",
    "        ]\n",
    "\n",
    "        self.columns = [\"customer_id\", \"name\", \"age\", \"income\", \"city\"]\n",
    "        self.df = self.create_test_dataframe(self.test_data, self.columns)\n",
    "\n",
    "    def test_filter_valid_customers(self):\n",
    "        \"\"\"Test filtering customers by minimum age\"\"\"\n",
    "        result = CustomerTransformations.filter_valid_customers(self.df, min_age=18)\n",
    "\n",
    "        # Should have 3 customers (Alice, Charlie, Diana)\n",
    "        self.assertEqual(result.count(), 3)\n",
    "\n",
    "        # All remaining customers should be >= 18\n",
    "        ages = [row.age for row in result.collect()]\n",
    "        self.assertTrue(all(age >= 18 for age in ages))\n",
    "\n",
    "    def test_add_age_group(self):\n",
    "        \"\"\"Test age group categorization\"\"\"\n",
    "        result = CustomerTransformations.add_age_group(self.df)\n",
    "\n",
    "        # Check that age_group column was added\n",
    "        self.assertIn(\"age_group\", result.columns)\n",
    "\n",
    "        # Check age group assignments\n",
    "        age_groups = {row.name: row.age_group for row in result.collect()}\n",
    "\n",
    "        self.assertEqual(age_groups[\"Alice\"], \"Young\")  # 25\n",
    "        self.assertEqual(age_groups[\"Bob\"], \"Young\")  # 17\n",
    "        self.assertEqual(age_groups[\"Charlie\"], \"Middle\")  # 35\n",
    "        self.assertEqual(age_groups[\"Diana\"], \"Middle\")  # 45\n",
    "        self.assertEqual(age_groups[\"Eve\"], \"Young\")  # 16\n",
    "\n",
    "    def test_calculate_age_group_stats(self):\n",
    "        \"\"\"Test age group statistics calculation\"\"\"\n",
    "        df_with_groups = CustomerTransformations.add_age_group(self.df)\n",
    "        result = CustomerTransformations.calculate_age_group_stats(df_with_groups)\n",
    "\n",
    "        # Should have Middle and Young groups\n",
    "        age_groups = [row.age_group for row in result.collect()]\n",
    "        self.assertIn(\"Young\", age_groups)\n",
    "        self.assertIn(\"Middle\", age_groups)\n",
    "\n",
    "        # Check statistics\n",
    "        stats = {\n",
    "            row.age_group: (row[\"count\"], row[\"avg_income\"]) for row in result.collect()\n",
    "        }\n",
    "\n",
    "        # Young: Alice (25, 50k), Bob (17, 30k), Eve (16, 25k)\n",
    "        young_count, young_avg = stats[\"Young\"]\n",
    "        self.assertEqual(young_count, 3)\n",
    "        self.assertAlmostEqual(young_avg, (50000 + 30000 + 25000) / 3, places=0)\n",
    "\n",
    "        # Middle: Charlie (35, 75k), Diana (45, 90k)\n",
    "        middle_count, middle_avg = stats[\"Middle\"]\n",
    "        self.assertEqual(middle_count, 2)\n",
    "        self.assertAlmostEqual(middle_avg, (75000 + 90000) / 2, places=0)\n",
    "\n",
    "\n",
    "print(\"🧪 Unit testing framework created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_add_age_group (__main__.TestCustomerTransformations.test_add_age_group)\n",
      "Test age group categorization ... ok\n",
      "test_calculate_age_group_stats (__main__.TestCustomerTransformations.test_calculate_age_group_stats)\n",
      "Test age group statistics calculation ... ok\n",
      "test_filter_valid_customers (__main__.TestCustomerTransformations.test_filter_valid_customers)\n",
      "Test filtering customers by minimum age ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 1.620s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test Results:\n",
      "  Tests run: 3\n",
      "  Failures: 0\n",
      "  Errors: 0\n",
      "✅ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Run the unit tests\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a test suite\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestCustomerTransformations)\n",
    "\n",
    "    # Run the tests\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "\n",
    "    print(\"\\n📊 Test Results:\")\n",
    "    print(f\"  Tests run: {result.testsRun}\")\n",
    "    print(f\"  Failures: {len(result.failures)}\")\n",
    "    print(f\"  Errors: {len(result.errors)}\")\n",
    "\n",
    "    if result.wasSuccessful():\n",
    "        print(\"✅ All tests passed!\")\n",
    "    else:\n",
    "        print(\"❌ Some tests failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ Module 3: Configuration Management\n",
    "\n",
    "Let's explore professional configuration management patterns for different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 3.1 Hierarchical Configuration System\n",
    "\n",
    "We'll create a flexible configuration system that supports multiple environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Advanced configuration management system created\n"
     ]
    }
   ],
   "source": [
    "# Advanced configuration management system\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SparkConfig:\n",
    "    \"\"\"Spark-specific configuration\"\"\"\n",
    "\n",
    "    app_name: str = \"SparkApp\"\n",
    "    master: str = \"local[*]\"\n",
    "    sql_shuffle_partitions: int = 200\n",
    "    adaptive_enabled: bool = True\n",
    "    adaptive_coalesce_partitions: bool = True\n",
    "    serializer: str = \"org.apache.spark.serializer.KryoSerializer\"\n",
    "\n",
    "    def to_spark_conf(self) -> Dict[str, str]:\n",
    "        \"\"\"Convert to Spark configuration dictionary\"\"\"\n",
    "        return {\n",
    "            \"spark.sql.shuffle.partitions\": str(self.sql_shuffle_partitions),\n",
    "            \"spark.sql.adaptive.enabled\": str(self.adaptive_enabled).lower(),\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": str(\n",
    "                self.adaptive_coalesce_partitions\n",
    "            ).lower(),\n",
    "            \"spark.serializer\": self.serializer,\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Data-related configuration\"\"\"\n",
    "\n",
    "    input_path: str = \"data/raw\"\n",
    "    output_path: str = \"data/processed\"\n",
    "    input_format: str = \"parquet\"\n",
    "    output_format: str = \"delta\"\n",
    "    compression: str = \"snappy\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    \"\"\"Database configuration\"\"\"\n",
    "\n",
    "    host: str = \"localhost\"\n",
    "    port: int = 5432\n",
    "    name: str = \"spark_db\"\n",
    "    username: Optional[str] = None\n",
    "    password: Optional[str] = None\n",
    "\n",
    "    def get_jdbc_url(self) -> str:\n",
    "        \"\"\"Get JDBC connection URL\"\"\"\n",
    "        return f\"jdbc:postgresql://{self.host}:{self.port}/{self.name}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    \"\"\"Main application configuration\"\"\"\n",
    "\n",
    "    environment: str = \"dev\"\n",
    "    debug: bool = True\n",
    "    log_level: str = \"INFO\"\n",
    "\n",
    "    spark: SparkConfig = field(default_factory=SparkConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    \"\"\"Configuration loader with environment support\"\"\"\n",
    "\n",
    "    def __init__(self, config_dir: str = \"configs\"):\n",
    "        self.config_dir = Path(config_dir)\n",
    "\n",
    "    def load_config(self, environment: str = None) -> AppConfig:\n",
    "        \"\"\"Load configuration for specified environment\"\"\"\n",
    "        if environment is None:\n",
    "            environment = os.getenv(\"ENVIRONMENT\", \"dev\")\n",
    "\n",
    "        # Load base configuration\n",
    "        base_config = self._load_yaml_config(\"base.yaml\")\n",
    "\n",
    "        # Load environment-specific configuration\n",
    "        env_config = self._load_yaml_config(f\"{environment}.yaml\")\n",
    "\n",
    "        # Merge configurations (environment overrides base)\n",
    "        merged_config = self._merge_configs(base_config, env_config)\n",
    "\n",
    "        # Resolve environment variables\n",
    "        resolved_config = self._resolve_env_vars(merged_config)\n",
    "\n",
    "        # Convert to AppConfig object\n",
    "        return self._dict_to_config(resolved_config)\n",
    "\n",
    "    def _load_yaml_config(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load YAML configuration file\"\"\"\n",
    "        file_path = self.config_dir / filename\n",
    "\n",
    "        if not file_path.exists():\n",
    "            return {}\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return yaml.safe_load(file) or {}\n",
    "\n",
    "    def _merge_configs(self, base: Dict, override: Dict) -> Dict:\n",
    "        \"\"\"Deep merge two configuration dictionaries\"\"\"\n",
    "        result = base.copy()\n",
    "\n",
    "        for key, value in override.items():\n",
    "            if (\n",
    "                key in result\n",
    "                and isinstance(result[key], dict)\n",
    "                and isinstance(value, dict)\n",
    "            ):\n",
    "                result[key] = self._merge_configs(result[key], value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _resolve_env_vars(self, config: Dict) -> Dict:\n",
    "        \"\"\"Resolve environment variables in configuration\"\"\"\n",
    "\n",
    "        def resolve_value(value):\n",
    "            if (\n",
    "                isinstance(value, str)\n",
    "                and value.startswith(\"${\")\n",
    "                and value.endswith(\"}\")\n",
    "            ):\n",
    "                env_var = value[2:-1]\n",
    "                return os.getenv(env_var, value)\n",
    "            elif isinstance(value, dict):\n",
    "                return {k: resolve_value(v) for k, v in value.items()}\n",
    "            elif isinstance(value, list):\n",
    "                return [resolve_value(item) for item in value]\n",
    "            return value\n",
    "\n",
    "        return resolve_value(config)\n",
    "\n",
    "    def _dict_to_config(self, config_dict: Dict) -> AppConfig:\n",
    "        \"\"\"Convert dictionary to AppConfig object\"\"\"\n",
    "        # Extract sections\n",
    "        app_section = config_dict.get(\"app\", {})\n",
    "        spark_section = config_dict.get(\"spark\", {})\n",
    "        data_section = config_dict.get(\"data\", {})\n",
    "        database_section = config_dict.get(\"database\", {})\n",
    "\n",
    "        # Create configuration objects\n",
    "        spark_config = SparkConfig(**spark_section)\n",
    "        data_config = DataConfig(**data_section)\n",
    "        database_config = DatabaseConfig(**database_section)\n",
    "\n",
    "        # Create main config\n",
    "        return AppConfig(\n",
    "            spark=spark_config,\n",
    "            data=data_config,\n",
    "            database=database_config,\n",
    "            **app_section,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"⚙️ Advanced configuration management system created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 3.2 Creating Configuration Files\n",
    "\n",
    "Let's create sample configuration files for different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Configuration files created:\n",
      "  📄 configs/dev.yaml\n",
      "  📄 configs/base.yaml\n",
      "  📄 configs/prod.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create sample configuration files\n",
    "configs_dir = Path(\"configs\")\n",
    "configs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Base configuration\n",
    "base_config = {\n",
    "    \"app\": {\"log_level\": \"INFO\"},\n",
    "    \"spark\": {\n",
    "        \"app_name\": \"CustomerAnalytics\",\n",
    "        \"sql_shuffle_partitions\": 200,\n",
    "        \"adaptive_enabled\": True,\n",
    "        \"serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_format\": \"parquet\",\n",
    "        \"output_format\": \"delta\",\n",
    "        \"compression\": \"snappy\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Development configuration\n",
    "dev_config = {\n",
    "    \"app\": {\"environment\": \"dev\", \"debug\": True, \"log_level\": \"DEBUG\"},\n",
    "    \"spark\": {\"master\": \"local[2]\", \"sql_shuffle_partitions\": 4},\n",
    "    \"data\": {\"input_path\": \"data/dev/input\", \"output_path\": \"data/dev/output\"},\n",
    "    \"database\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 5432,\n",
    "        \"name\": \"dev_database\",\n",
    "        \"username\": \"dev_user\",\n",
    "        \"password\": \"dev_password\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Production configuration\n",
    "prod_config = {\n",
    "    \"app\": {\"environment\": \"prod\", \"debug\": False, \"log_level\": \"WARN\"},\n",
    "    \"spark\": {\"master\": \"yarn\", \"sql_shuffle_partitions\": 1000},\n",
    "    \"data\": {\n",
    "        \"input_path\": \"s3a://prod-data-lake/input\",\n",
    "        \"output_path\": \"s3a://prod-data-lake/output\",\n",
    "    },\n",
    "    \"database\": {\n",
    "        \"host\": \"${DB_HOST}\",\n",
    "        \"port\": \"${DB_PORT}\",\n",
    "        \"name\": \"${DB_NAME}\",\n",
    "        \"username\": \"${DB_USERNAME}\",\n",
    "        \"password\": \"${DB_PASSWORD}\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Write configuration files\n",
    "with open(configs_dir / \"base.yaml\", \"w\") as f:\n",
    "    yaml.dump(base_config, f, default_flow_style=False)\n",
    "\n",
    "with open(configs_dir / \"dev.yaml\", \"w\") as f:\n",
    "    yaml.dump(dev_config, f, default_flow_style=False)\n",
    "\n",
    "with open(configs_dir / \"prod.yaml\", \"w\") as f:\n",
    "    yaml.dump(prod_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"📝 Configuration files created:\")\n",
    "for config_file in configs_dir.glob(\"*.yaml\"):\n",
    "    print(f\"  📄 {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 3.3 Testing Configuration Loading\n",
    "\n",
    "Let's test our configuration system with different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading Development Configuration:\n",
      "==================================================\n",
      "Environment: dev\n",
      "Debug mode: True\n",
      "Log level: DEBUG\n",
      "\n",
      "Spark configuration:\n",
      "  Master: local[2]\n",
      "  App name: CustomerAnalytics\n",
      "  Shuffle partitions: 4\n",
      "\n",
      "Data configuration:\n",
      "  Input path: data/dev/input\n",
      "  Output path: data/dev/output\n",
      "\n",
      "Database configuration:\n",
      "  JDBC URL: jdbc:postgresql://localhost:5432/dev_database\n",
      "  Username: dev_user\n"
     ]
    }
   ],
   "source": [
    "# Test configuration loading\n",
    "config_loader = ConfigLoader(\"configs\")\n",
    "\n",
    "# Load development configuration\n",
    "print(\"🔧 Loading Development Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "dev_config = config_loader.load_config(\"dev\")\n",
    "\n",
    "print(f\"Environment: {dev_config.environment}\")\n",
    "print(f\"Debug mode: {dev_config.debug}\")\n",
    "print(f\"Log level: {dev_config.log_level}\")\n",
    "print(\"\\nSpark configuration:\")\n",
    "print(f\"  Master: {dev_config.spark.master}\")\n",
    "print(f\"  App name: {dev_config.spark.app_name}\")\n",
    "print(f\"  Shuffle partitions: {dev_config.spark.sql_shuffle_partitions}\")\n",
    "print(\"\\nData configuration:\")\n",
    "print(f\"  Input path: {dev_config.data.input_path}\")\n",
    "print(f\"  Output path: {dev_config.data.output_path}\")\n",
    "print(\"\\nDatabase configuration:\")\n",
    "print(f\"  JDBC URL: {dev_config.database.get_jdbc_url()}\")\n",
    "print(f\"  Username: {dev_config.database.username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏭 Loading Production Configuration:\n",
      "==================================================\n",
      "Environment: prod\n",
      "Debug mode: False\n",
      "Log level: WARN\n",
      "\n",
      "Spark configuration:\n",
      "  Master: yarn\n",
      "  Shuffle partitions: 1000\n",
      "\n",
      "Data configuration:\n",
      "  Input path: s3a://prod-data-lake/input\n",
      "  Output path: s3a://prod-data-lake/output\n",
      "\n",
      "Database configuration:\n",
      "  JDBC URL: jdbc:postgresql://prod-db.company.com:5432/prod_analytics\n",
      "  Username: analytics_user\n",
      "  Password: *******************\n"
     ]
    }
   ],
   "source": [
    "# Load production configuration (with environment variables)\n",
    "print(\"\\n🏭 Loading Production Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set some environment variables for demonstration\n",
    "os.environ[\"DB_HOST\"] = \"prod-db.company.com\"\n",
    "os.environ[\"DB_PORT\"] = \"5432\"\n",
    "os.environ[\"DB_NAME\"] = \"prod_analytics\"\n",
    "os.environ[\"DB_USERNAME\"] = \"analytics_user\"\n",
    "os.environ[\"DB_PASSWORD\"] = \"secure_password_123\"\n",
    "\n",
    "prod_config = config_loader.load_config(\"prod\")\n",
    "\n",
    "print(f\"Environment: {prod_config.environment}\")\n",
    "print(f\"Debug mode: {prod_config.debug}\")\n",
    "print(f\"Log level: {prod_config.log_level}\")\n",
    "print(\"\\nSpark configuration:\")\n",
    "print(f\"  Master: {prod_config.spark.master}\")\n",
    "print(f\"  Shuffle partitions: {prod_config.spark.sql_shuffle_partitions}\")\n",
    "print(\"\\nData configuration:\")\n",
    "print(f\"  Input path: {prod_config.data.input_path}\")\n",
    "print(f\"  Output path: {prod_config.data.output_path}\")\n",
    "print(\"\\nDatabase configuration:\")\n",
    "print(f\"  JDBC URL: {prod_config.database.get_jdbc_url()}\")\n",
    "print(f\"  Username: {prod_config.database.username}\")\n",
    "print(f\"  Password: {'*' * len(prod_config.database.password)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Spark Configuration for Development:\n",
      "==================================================\n",
      "  spark.sql.shuffle.partitions: 4\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "  spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "\n",
      "🚀 Creating Spark session with configuration...\n",
      "✅ Spark session created: CustomerAnalytics\n",
      "📊 Shuffle partitions: 4\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Spark configuration conversion\n",
    "print(\"\\n⚙️ Spark Configuration for Development:\")\n",
    "print(\"=\" * 50)\n",
    "spark_conf = dev_config.spark.to_spark_conf()\n",
    "for key, value in spark_conf.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create Spark session with configuration\n",
    "print(\"\\n🚀 Creating Spark session with configuration...\")\n",
    "builder = SparkSession.builder.appName(dev_config.spark.app_name).master(\n",
    "    dev_config.spark.master\n",
    ")\n",
    "\n",
    "for key, value in spark_conf.items():\n",
    "    builder = builder.config(key, value)\n",
    "\n",
    "configured_spark = builder.getOrCreate()\n",
    "print(f\"✅ Spark session created: {configured_spark.sparkContext.appName}\")\n",
    "print(\n",
    "    f\"📊 Shuffle partitions: {configured_spark.conf.get('spark.sql.shuffle.partitions')}\"\n",
    ")\n",
    "\n",
    "configured_spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the interactive tutorial for professional Spark development setup. Let's summarize what you've learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ What You've Accomplished\n",
    "\n",
    "1. **🏗️ Project Structure Best Practices**\n",
    "   - Created modular Spark application architecture\n",
    "   - Implemented separation of concerns\n",
    "   - Built reusable components and utilities\n",
    "\n",
    "2. **🔧 Development Workflow**\n",
    "   - Developed DataFrame debugging utilities\n",
    "   - Created comprehensive testing framework\n",
    "   - Implemented performance profiling tools\n",
    "\n",
    "3. **⚙️ Configuration Management**\n",
    "   - Built hierarchical configuration system\n",
    "   - Implemented environment-specific settings\n",
    "   - Created secrets management patterns\n",
    "\n",
    "### 🎓 Key Takeaways\n",
    "\n",
    "- **Modularity**: Break your code into small, focused, testable components\n",
    "- **Configuration**: Use environment-specific configurations for flexibility\n",
    "- **Testing**: Write comprehensive tests for your transformations and logic\n",
    "- **Automation**: Use tools to enforce code quality and consistency\n",
    "- **Documentation**: Code should be self-documenting with clear structure\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Complete the exercises** in the `exercises/` directory\n",
    "2. **Explore the project templates** in the `templates/` directory\n",
    "3. **Set up your own project** using the patterns you've learned\n",
    "4. **Move on to Lesson 4**: File Formats Deep Dive\n",
    "\n",
    "### 📚 Additional Practice\n",
    "\n",
    "Try these challenges to reinforce your learning:\n",
    "\n",
    "1. Create a new Spark project using the modular structure\n",
    "2. Implement configuration for staging environment\n",
    "3. Add more comprehensive test cases\n",
    "4. Set up a CI/CD pipeline using GitHub Actions\n",
    "5. Containerize your application with Docker\n",
    "\n",
    "### 🆘 Getting Help\n",
    "\n",
    "If you encounter issues:\n",
    "1. Check the troubleshooting section in the README\n",
    "2. Run the validation scripts: `make validate-learning`\n",
    "3. Review the solution files in `solutions/`\n",
    "4. Use the debugging utilities you've learned\n",
    "\n",
    "**Happy coding! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
