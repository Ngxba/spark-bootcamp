{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Local Development Setup\n",
    "\n",
    "## 🎯 Interactive Tutorial: Professional Spark Development\n",
    "\n",
    "Welcome to Lesson 3! In this notebook, we'll explore professional development practices for Apache Spark applications. You'll learn how to structure projects, manage configurations, implement testing, and integrate development tools.\n",
    "\n",
    "### 📋 What You'll Learn\n",
    "1. **Project Structure Best Practices** - Modular, maintainable Spark applications\n",
    "2. **Development Workflow** - Debugging, testing, and quality assurance\n",
    "3. **Configuration Management** - Environment-specific settings and secrets\n",
    "4. **Development Tools Integration** - Git, Docker, CI/CD foundations\n",
    "\n",
    "### 🔧 Setup\n",
    "Make sure you've completed the environment setup:\n",
    "```bash\n",
    "make setup\n",
    "make install-dev\n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup and imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().absolute()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Essential imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, avg, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "print(f\"🚀 PySpark version: {pyspark.__version__}\")\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"🐍 Python path: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏗️ Module 1: Project Structure Best Practices\n",
    "\n",
    "Let's start by understanding how to structure a professional Spark project. We'll demonstrate the key principles through practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📂 1.1 Demonstrating Project Structure\n",
    "\n",
    "Let's examine what a well-structured Spark project looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a sample project structure to demonstrate\n",
    "def show_project_structure():\n",
    "    \"\"\"Display recommended project structure\"\"\"\n",
    "    structure = \"\"\"\n",
    "    my-spark-project/\n",
    "    ├── README.md                   # Project documentation\n",
    "    ├── pyproject.toml             # Dependencies and configuration\n",
    "    ├── Makefile                   # Development commands\n",
    "    ├── .env.example               # Environment variables template\n",
    "    ├── .gitignore                 # Git ignore patterns\n",
    "    │\n",
    "    ├── src/                       # Source code (production)\n",
    "    │   ├── config/                # Configuration management\n",
    "    │   │   ├── settings.py        # Application settings\n",
    "    │   │   └── environments/      # Environment-specific configs\n",
    "    │   ├── jobs/                  # Spark job definitions\n",
    "    │   │   ├── base_job.py        # Abstract base job class\n",
    "    │   │   └── etl_job.py         # ETL job implementation\n",
    "    │   ├── transformations/       # Data transformation functions\n",
    "    │   │   ├── cleaning.py        # Data cleaning functions\n",
    "    │   │   └── aggregations.py    # Aggregation functions\n",
    "    │   ├── utils/                 # Utility functions\n",
    "    │   │   ├── spark_utils.py     # Spark session and utilities\n",
    "    │   │   └── io_utils.py        # Input/output helpers\n",
    "    │   └── schemas/               # Data schemas\n",
    "    │       └── input_schemas.py   # Input data schemas\n",
    "    │\n",
    "    ├── tests/                     # Test suite\n",
    "    │   ├── conftest.py           # Pytest configuration\n",
    "    │   ├── unit/                 # Unit tests\n",
    "    │   └── integration/          # Integration tests\n",
    "    │\n",
    "    ├── data/                     # Local data directory\n",
    "    │   ├── raw/                  # Raw input data\n",
    "    │   └── processed/            # Processed data\n",
    "    │\n",
    "    └── scripts/                  # Utility scripts\n",
    "        ├── setup.sh             # Environment setup\n",
    "        └── run_job.py           # Job runner script\n",
    "    \"\"\"\n",
    "    print(\"📂 Recommended Project Structure:\")\n",
    "    print(structure)\n",
    "\n",
    "show_project_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 1.2 Separation of Concerns Example\n",
    "\n",
    "Let's see how to properly separate different concerns in a Spark application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Proper separation of concerns\n",
    "\n",
    "# ❌ BAD: Everything in one function\n",
    "def monolithic_data_processing():\n",
    "    # Spark session creation\n",
    "    spark = SparkSession.builder.appName(\"MonolithicApp\").getOrCreate()\n",
    "    \n",
    "    # Data loading\n",
    "    df = spark.read.option(\"header\", \"true\").csv(\"data/raw/customers.csv\")\n",
    "    \n",
    "    # Business logic mixed with technical concerns\n",
    "    processed_df = (df\n",
    "                   .filter(col(\"age\") > 18)\n",
    "                   .withColumn(\"age_group\", when(col(\"age\") < 30, \"Young\")\n",
    "                              .when(col(\"age\") < 50, \"Middle\")\n",
    "                              .otherwise(\"Senior\"))\n",
    "                   .groupBy(\"age_group\")\n",
    "                   .agg(count(\"*\").alias(\"count\"), avg(\"income\").alias(\"avg_income\")))\n",
    "    \n",
    "    # Output writing\n",
    "    processed_df.write.mode(\"overwrite\").parquet(\"data/processed/customer_analysis\")\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "print(\"❌ Monolithic approach - everything mixed together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ GOOD: Separated concerns\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 1. Configuration Management\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    app_name: str = \"CustomerAnalysis\"\n",
    "    input_path: str = \"data/raw/customers.csv\"\n",
    "    output_path: str = \"data/processed/customer_analysis\"\n",
    "    min_age: int = 18\n",
    "\n",
    "# 2. Spark Utilities\n",
    "class SparkUtils:\n",
    "    @staticmethod\n",
    "    def get_spark_session(app_name: str) -> SparkSession:\n",
    "        \"\"\"Create optimized Spark session\"\"\"\n",
    "        return (SparkSession.builder\n",
    "                .appName(app_name)\n",
    "                .master(\"local[*]\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "                .getOrCreate())\n",
    "\n",
    "# 3. Data Schema Definition\n",
    "class CustomerSchema:\n",
    "    SCHEMA = StructType([\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"income\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "# 4. Business Logic (Transformations)\n",
    "class CustomerTransformations:\n",
    "    @staticmethod\n",
    "    def filter_valid_customers(df, min_age: int = 18):\n",
    "        \"\"\"Filter customers with valid age\"\"\"\n",
    "        return df.filter(col(\"age\") >= min_age)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_age_group(df):\n",
    "        \"\"\"Add age group categorization\"\"\"\n",
    "        return df.withColumn(\n",
    "            \"age_group\",\n",
    "            when(col(\"age\") < 30, \"Young\")\n",
    "            .when(col(\"age\") < 50, \"Middle\")\n",
    "            .otherwise(\"Senior\")\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_age_group_stats(df):\n",
    "        \"\"\"Calculate statistics by age group\"\"\"\n",
    "        return (df.groupBy(\"age_group\")\n",
    "                .agg(\n",
    "                    count(\"*\").alias(\"count\"),\n",
    "                    avg(\"income\").alias(\"avg_income\")\n",
    "                )\n",
    "                .orderBy(\"age_group\"))\n",
    "\n",
    "# 5. I/O Operations\n",
    "class DataIO:\n",
    "    @staticmethod\n",
    "    def read_customer_data(spark: SparkSession, path: str):\n",
    "        \"\"\"Read customer data with schema\"\"\"\n",
    "        return (spark.read\n",
    "                .schema(CustomerSchema.SCHEMA)\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(path))\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_analysis_results(df, path: str):\n",
    "        \"\"\"Write analysis results\"\"\"\n",
    "        (df.write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"compression\", \"snappy\")\n",
    "         .parquet(path))\n",
    "\n",
    "print(\"✅ Modular approach - separated concerns with clear responsibilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ 1.3 Putting It All Together\n",
    "\n",
    "Now let's see how these separated components work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Main Job Class\n",
    "class CustomerAnalysisJob:\n",
    "    def __init__(self, config: AppConfig):\n",
    "        self.config = config\n",
    "        self.spark = SparkUtils.get_spark_session(config.app_name)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete analysis pipeline\"\"\"\n",
    "        try:\n",
    "            print(f\"🚀 Starting {self.config.app_name}\")\n",
    "            \n",
    "            # Extract\n",
    "            print(\"📥 Loading customer data...\")\n",
    "            raw_data = DataIO.read_customer_data(self.spark, self.config.input_path)\n",
    "            print(f\"📊 Loaded {raw_data.count():,} customer records\")\n",
    "            \n",
    "            # Transform\n",
    "            print(\"🔄 Applying transformations...\")\n",
    "            valid_customers = CustomerTransformations.filter_valid_customers(raw_data, self.config.min_age)\n",
    "            customers_with_groups = CustomerTransformations.add_age_group(valid_customers)\n",
    "            analysis_results = CustomerTransformations.calculate_age_group_stats(customers_with_groups)\n",
    "            \n",
    "            print(\"📈 Analysis results:\")\n",
    "            analysis_results.show()\n",
    "            \n",
    "            # Load\n",
    "            print(f\"💾 Saving results to {self.config.output_path}\")\n",
    "            DataIO.write_analysis_results(analysis_results, self.config.output_path)\n",
    "            \n",
    "            print(\"✅ Job completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Job failed: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.spark.stop()\n",
    "\n",
    "print(\"✅ Complete modular job structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 1.4 Testing the Modular Structure\n",
    "\n",
    "Let's create some sample data and test our modular structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample customer data\n",
    "sample_data = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006'],\n",
    "    'name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Wilson', 'Eve Davis', 'Frank Miller'],\n",
    "    'age': [25, 35, 45, 55, 17, 30],  # Note: one customer under 18\n",
    "    'income': [50000, 75000, 90000, 120000, 25000, 60000],\n",
    "    'city': ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Phoenix', 'Philadelphia']\n",
    "})\n",
    "\n",
    "# Create temporary directories\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "input_dir = temp_dir / \"input\"\n",
    "output_dir = temp_dir / \"output\"\n",
    "input_dir.mkdir(exist_ok=True)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save sample data\n",
    "input_file = input_dir / \"customers.csv\"\n",
    "sample_data.to_csv(input_file, index=False)\n",
    "\n",
    "print(f\"📁 Created test data at: {input_file}\")\n",
    "print(f\"📊 Sample data:\")\n",
    "print(sample_data)\n",
    "\n",
    "# Configure and run the job\n",
    "config = AppConfig(\n",
    "    input_path=str(input_file),\n",
    "    output_path=str(output_dir / \"customer_analysis\")\n",
    ")\n",
    "\n",
    "print(f\"\\n🔧 Configuration:\")\n",
    "print(f\"  Input: {config.input_path}\")\n",
    "print(f\"  Output: {config.output_path}\")\n",
    "print(f\"  Min age: {config.min_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the modular job\n",
    "job = CustomerAnalysisJob(config)\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔧 Module 2: Development Workflow\n",
    "\n",
    "Now let's explore professional development workflows, including debugging techniques and testing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐛 2.1 DataFrame Debugging Utilities\n",
    "\n",
    "Debugging DataFrames can be challenging. Let's create utilities to make it easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced debugging utilities for DataFrames\n",
    "from pyspark.sql import DataFrame\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "class DataFrameDebugger:\n",
    "    \"\"\"Comprehensive DataFrame debugging utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_dataframe(df: DataFrame, \n",
    "                       name: str = \"DataFrame\",\n",
    "                       show_rows: int = 10,\n",
    "                       show_schema: bool = True,\n",
    "                       show_count: bool = True,\n",
    "                       show_sample: bool = True) -> DataFrame:\n",
    "        \"\"\"Comprehensive DataFrame debugging\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔍 DEBUG: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if show_schema:\n",
    "            print(f\"\\n📋 Schema:\")\n",
    "            df.printSchema()\n",
    "        \n",
    "        if show_count:\n",
    "            count = df.count()\n",
    "            print(f\"\\n📊 Row Count: {count:,} rows\")\n",
    "        \n",
    "        if show_sample and df.count() > 0:\n",
    "            print(f\"\\n🔍 Sample Data (first {show_rows} rows):\")\n",
    "            df.show(show_rows, truncate=False)\n",
    "            \n",
    "            # Show data types and null counts\n",
    "            print(f\"\\n📈 Column Statistics:\")\n",
    "            for column in df.columns:\n",
    "                null_count = df.filter(col(column).isNull()).count()\n",
    "                dtype = dict(df.dtypes)[column]\n",
    "                print(f\"  {column:20} | Type: {dtype:15} | Nulls: {null_count:,}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_operation(operation_name: str):\n",
    "        \"\"\"Decorator to profile Spark operations\"\"\"\n",
    "        def decorator(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                print(f\"\\n🚀 Starting: {operation_name}\")\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # Force action if result is DataFrame\n",
    "                if hasattr(result, 'count'):\n",
    "                    count = result.count()\n",
    "                    execution_time = time.time() - start_time\n",
    "                    print(f\"✅ Completed: {operation_name}\")\n",
    "                    print(f\"⏱️  Execution time: {execution_time:.2f} seconds\")\n",
    "                    print(f\"📊 Result count: {count:,} rows\")\n",
    "                else:\n",
    "                    execution_time = time.time() - start_time\n",
    "                    print(f\"✅ Completed: {operation_name}\")\n",
    "                    print(f\"⏱️  Execution time: {execution_time:.2f} seconds\")\n",
    "                \n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_dataframes(df1: DataFrame, df2: DataFrame, \n",
    "                          name1: str = \"DataFrame 1\", \n",
    "                          name2: str = \"DataFrame 2\"):\n",
    "        \"\"\"Compare two DataFrames\"\"\"\n",
    "        print(f\"\\n🔍 Comparing {name1} vs {name2}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Compare counts\n",
    "        count1, count2 = df1.count(), df2.count()\n",
    "        print(f\"📊 Row counts: {name1}: {count1:,}, {name2}: {count2:,}\")\n",
    "        \n",
    "        # Compare schemas\n",
    "        cols1, cols2 = set(df1.columns), set(df2.columns)\n",
    "        print(f\"📋 Column counts: {name1}: {len(cols1)}, {name2}: {len(cols2)}\")\n",
    "        \n",
    "        if cols1 != cols2:\n",
    "            print(f\"⚠️  Schema differences:\")\n",
    "            print(f\"  Only in {name1}: {cols1 - cols2}\")\n",
    "            print(f\"  Only in {name2}: {cols2 - cols1}\")\n",
    "        else:\n",
    "            print(f\"✅ Schemas match\")\n",
    "\n",
    "print(\"🔧 DataFrame debugging utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 2.2 Testing Our Debug Utilities\n",
    "\n",
    "Let's test our debugging utilities with the customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Spark session for debugging demonstration\n",
    "spark = SparkUtils.get_spark_session(\"DebuggingDemo\")\n",
    "\n",
    "# Load and debug the customer data\n",
    "customer_df = DataIO.read_customer_data(spark, str(input_file))\n",
    "customer_df = DataFrameDebugger.debug_dataframe(customer_df, \"Raw Customer Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations with profiling\n",
    "@DataFrameDebugger.profile_operation(\"Filter Valid Customers\")\n",
    "def filter_customers_with_profiling(df):\n",
    "    return CustomerTransformations.filter_valid_customers(df)\n",
    "\n",
    "@DataFrameDebugger.profile_operation(\"Add Age Groups\")\n",
    "def add_age_groups_with_profiling(df):\n",
    "    return CustomerTransformations.add_age_group(df)\n",
    "\n",
    "# Apply transformations\n",
    "filtered_df = filter_customers_with_profiling(customer_df)\n",
    "grouped_df = add_age_groups_with_profiling(filtered_df)\n",
    "\n",
    "# Debug the final result\n",
    "final_df = DataFrameDebugger.debug_dataframe(grouped_df, \"Customers with Age Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs filtered data\n",
    "DataFrameDebugger.compare_dataframes(\n",
    "    customer_df, filtered_df, \n",
    "    \"Original Data\", \"Filtered Data (Age >= 18)\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 2.3 Unit Testing Framework\n",
    "\n",
    "Let's create a comprehensive testing framework for our Spark components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit testing framework for Spark applications\n",
    "import unittest\n",
    "from typing import List, Tuple\n",
    "\n",
    "class SparkTestCase(unittest.TestCase):\n",
    "    \"\"\"Base test case for Spark applications\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up Spark session for testing\"\"\"\n",
    "        cls.spark = (SparkSession.builder\n",
    "                    .appName(\"test-spark-app\")\n",
    "                    .master(\"local[2]\")\n",
    "                    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "                    .config(\"spark.ui.enabled\", \"false\")\n",
    "                    .getOrCreate())\n",
    "        cls.spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        \"\"\"Clean up Spark session\"\"\"\n",
    "        cls.spark.stop()\n",
    "    \n",
    "    def create_test_dataframe(self, data: List[Tuple], columns: List[str]):\n",
    "        \"\"\"Helper to create test DataFrames\"\"\"\n",
    "        return self.spark.createDataFrame(data, columns)\n",
    "    \n",
    "    def assert_dataframe_equal(self, df1: DataFrame, df2: DataFrame, \n",
    "                              check_schema: bool = True):\n",
    "        \"\"\"Assert two DataFrames are equal\"\"\"\n",
    "        if check_schema:\n",
    "            self.assertEqual(df1.schema, df2.schema, \"Schemas don't match\")\n",
    "        \n",
    "        # Convert to lists for comparison\n",
    "        rows1 = sorted(df1.collect())\n",
    "        rows2 = sorted(df2.collect())\n",
    "        \n",
    "        self.assertEqual(rows1, rows2, \"DataFrames don't match\")\n",
    "\n",
    "# Test cases for our customer transformations\n",
    "class TestCustomerTransformations(SparkTestCase):\n",
    "    \"\"\"Test customer transformation functions\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test data\"\"\"\n",
    "        self.test_data = [\n",
    "            (\"C001\", \"Alice\", 25, 50000.0, \"New York\"),\n",
    "            (\"C002\", \"Bob\", 17, 30000.0, \"Chicago\"),      # Under 18\n",
    "            (\"C003\", \"Charlie\", 35, 75000.0, \"LA\"),\n",
    "            (\"C004\", \"Diana\", 45, 90000.0, \"Houston\"),\n",
    "            (\"C005\", \"Eve\", 16, 25000.0, \"Phoenix\"),       # Under 18\n",
    "        ]\n",
    "        \n",
    "        self.columns = [\"customer_id\", \"name\", \"age\", \"income\", \"city\"]\n",
    "        self.df = self.create_test_dataframe(self.test_data, self.columns)\n",
    "    \n",
    "    def test_filter_valid_customers(self):\n",
    "        \"\"\"Test filtering customers by minimum age\"\"\"\n",
    "        result = CustomerTransformations.filter_valid_customers(self.df, min_age=18)\n",
    "        \n",
    "        # Should have 3 customers (Alice, Charlie, Diana)\n",
    "        self.assertEqual(result.count(), 3)\n",
    "        \n",
    "        # All remaining customers should be >= 18\n",
    "        ages = [row.age for row in result.collect()]\n",
    "        self.assertTrue(all(age >= 18 for age in ages))\n",
    "    \n",
    "    def test_add_age_group(self):\n",
    "        \"\"\"Test age group categorization\"\"\"\n",
    "        result = CustomerTransformations.add_age_group(self.df)\n",
    "        \n",
    "        # Check that age_group column was added\n",
    "        self.assertIn(\"age_group\", result.columns)\n",
    "        \n",
    "        # Check age group assignments\n",
    "        age_groups = {row.name: row.age_group for row in result.collect()}\n",
    "        \n",
    "        self.assertEqual(age_groups[\"Alice\"], \"Young\")    # 25\n",
    "        self.assertEqual(age_groups[\"Bob\"], \"Young\")      # 17\n",
    "        self.assertEqual(age_groups[\"Charlie\"], \"Middle\") # 35\n",
    "        self.assertEqual(age_groups[\"Diana\"], \"Middle\")   # 45\n",
    "        self.assertEqual(age_groups[\"Eve\"], \"Young\")      # 16\n",
    "    \n",
    "    def test_calculate_age_group_stats(self):\n",
    "        \"\"\"Test age group statistics calculation\"\"\"\n",
    "        df_with_groups = CustomerTransformations.add_age_group(self.df)\n",
    "        result = CustomerTransformations.calculate_age_group_stats(df_with_groups)\n",
    "        \n",
    "        # Should have Middle and Young groups\n",
    "        age_groups = [row.age_group for row in result.collect()]\n",
    "        self.assertIn(\"Young\", age_groups)\n",
    "        self.assertIn(\"Middle\", age_groups)\n",
    "        \n",
    "        # Check statistics\n",
    "        stats = {row.age_group: (row.count, row.avg_income) for row in result.collect()}\n",
    "        \n",
    "        # Young: Alice (25, 50k), Bob (17, 30k), Eve (16, 25k)\n",
    "        young_count, young_avg = stats[\"Young\"]\n",
    "        self.assertEqual(young_count, 3)\n",
    "        self.assertAlmostEqual(young_avg, (50000 + 30000 + 25000) / 3, places=0)\n",
    "        \n",
    "        # Middle: Charlie (35, 75k), Diana (45, 90k)\n",
    "        middle_count, middle_avg = stats[\"Middle\"]\n",
    "        self.assertEqual(middle_count, 2)\n",
    "        self.assertAlmostEqual(middle_avg, (75000 + 90000) / 2, places=0)\n",
    "\n",
    "print(\"🧪 Unit testing framework created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the unit tests\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a test suite\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestCustomerTransformations)\n",
    "    \n",
    "    # Run the tests\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    print(f\"\\n📊 Test Results:\")\n",
    "    print(f\"  Tests run: {result.testsRun}\")\n",
    "    print(f\"  Failures: {len(result.failures)}\")\n",
    "    print(f\"  Errors: {len(result.errors)}\")\n",
    "    \n",
    "    if result.wasSuccessful():\n",
    "        print(\"✅ All tests passed!\")\n",
    "    else:\n",
    "        print(\"❌ Some tests failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ Module 3: Configuration Management\n",
    "\n",
    "Let's explore professional configuration management patterns for different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 3.1 Hierarchical Configuration System\n",
    "\n",
    "We'll create a flexible configuration system that supports multiple environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced configuration management system\n",
    "import yaml\n",
    "import os\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class SparkConfig:\n",
    "    \"\"\"Spark-specific configuration\"\"\"\n",
    "    app_name: str = \"SparkApp\"\n",
    "    master: str = \"local[*]\"\n",
    "    sql_shuffle_partitions: int = 200\n",
    "    adaptive_enabled: bool = True\n",
    "    adaptive_coalesce_partitions: bool = True\n",
    "    serializer: str = \"org.apache.spark.serializer.KryoSerializer\"\n",
    "    \n",
    "    def to_spark_conf(self) -> Dict[str, str]:\n",
    "        \"\"\"Convert to Spark configuration dictionary\"\"\"\n",
    "        return {\n",
    "            \"spark.sql.shuffle.partitions\": str(self.sql_shuffle_partitions),\n",
    "            \"spark.sql.adaptive.enabled\": str(self.adaptive_enabled).lower(),\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": str(self.adaptive_coalesce_partitions).lower(),\n",
    "            \"spark.serializer\": self.serializer\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Data-related configuration\"\"\"\n",
    "    input_path: str = \"data/raw\"\n",
    "    output_path: str = \"data/processed\"\n",
    "    input_format: str = \"parquet\"\n",
    "    output_format: str = \"delta\"\n",
    "    compression: str = \"snappy\"\n",
    "\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    \"\"\"Database configuration\"\"\"\n",
    "    host: str = \"localhost\"\n",
    "    port: int = 5432\n",
    "    name: str = \"spark_db\"\n",
    "    username: Optional[str] = None\n",
    "    password: Optional[str] = None\n",
    "    \n",
    "    def get_jdbc_url(self) -> str:\n",
    "        \"\"\"Get JDBC connection URL\"\"\"\n",
    "        return f\"jdbc:postgresql://{self.host}:{self.port}/{self.name}\"\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    \"\"\"Main application configuration\"\"\"\n",
    "    environment: str = \"dev\"\n",
    "    debug: bool = True\n",
    "    log_level: str = \"INFO\"\n",
    "    \n",
    "    spark: SparkConfig = field(default_factory=SparkConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n",
    "\n",
    "class ConfigLoader:\n",
    "    \"\"\"Configuration loader with environment support\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir: str = \"configs\"):\n",
    "        self.config_dir = Path(config_dir)\n",
    "    \n",
    "    def load_config(self, environment: str = None) -> AppConfig:\n",
    "        \"\"\"Load configuration for specified environment\"\"\"\n",
    "        if environment is None:\n",
    "            environment = os.getenv(\"ENVIRONMENT\", \"dev\")\n",
    "        \n",
    "        # Load base configuration\n",
    "        base_config = self._load_yaml_config(\"base.yaml\")\n",
    "        \n",
    "        # Load environment-specific configuration\n",
    "        env_config = self._load_yaml_config(f\"{environment}.yaml\")\n",
    "        \n",
    "        # Merge configurations (environment overrides base)\n",
    "        merged_config = self._merge_configs(base_config, env_config)\n",
    "        \n",
    "        # Resolve environment variables\n",
    "        resolved_config = self._resolve_env_vars(merged_config)\n",
    "        \n",
    "        # Convert to AppConfig object\n",
    "        return self._dict_to_config(resolved_config)\n",
    "    \n",
    "    def _load_yaml_config(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load YAML configuration file\"\"\"\n",
    "        file_path = self.config_dir / filename\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            return {}\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            return yaml.safe_load(file) or {}\n",
    "    \n",
    "    def _merge_configs(self, base: Dict, override: Dict) -> Dict:\n",
    "        \"\"\"Deep merge two configuration dictionaries\"\"\"\n",
    "        result = base.copy()\n",
    "        \n",
    "        for key, value in override.items():\n",
    "            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
    "                result[key] = self._merge_configs(result[key], value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _resolve_env_vars(self, config: Dict) -> Dict:\n",
    "        \"\"\"Resolve environment variables in configuration\"\"\"\n",
    "        def resolve_value(value):\n",
    "            if isinstance(value, str) and value.startswith(\"${\" ) and value.endswith(\"}\"):\n",
    "                env_var = value[2:-1]\n",
    "                return os.getenv(env_var, value)\n",
    "            elif isinstance(value, dict):\n",
    "                return {k: resolve_value(v) for k, v in value.items()}\n",
    "            elif isinstance(value, list):\n",
    "                return [resolve_value(item) for item in value]\n",
    "            return value\n",
    "        \n",
    "        return resolve_value(config)\n",
    "    \n",
    "    def _dict_to_config(self, config_dict: Dict) -> AppConfig:\n",
    "        \"\"\"Convert dictionary to AppConfig object\"\"\"\n",
    "        # Extract sections\n",
    "        app_section = config_dict.get(\"app\", {})\n",
    "        spark_section = config_dict.get(\"spark\", {})\n",
    "        data_section = config_dict.get(\"data\", {})\n",
    "        database_section = config_dict.get(\"database\", {})\n",
    "        \n",
    "        # Create configuration objects\n",
    "        spark_config = SparkConfig(**spark_section)\n",
    "        data_config = DataConfig(**data_section)\n",
    "        database_config = DatabaseConfig(**database_section)\n",
    "        \n",
    "        # Create main config\n",
    "        return AppConfig(\n",
    "            spark=spark_config,\n",
    "            data=data_config,\n",
    "            database=database_config,\n",
    "            **app_section\n",
    "        )\n",
    "\n",
    "print(\"⚙️ Advanced configuration management system created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 3.2 Creating Configuration Files\n",
    "\n",
    "Let's create sample configuration files for different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample configuration files\n",
    "configs_dir = Path(\"configs\")\n",
    "configs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Base configuration\n",
    "base_config = {\n",
    "    \"app\": {\n",
    "        \"log_level\": \"INFO\"\n",
    "    },\n",
    "    \"spark\": {\n",
    "        \"app_name\": \"CustomerAnalytics\",\n",
    "        \"sql_shuffle_partitions\": 200,\n",
    "        \"adaptive_enabled\": True,\n",
    "        \"serializer\": \"org.apache.spark.serializer.KryoSerializer\"\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_format\": \"parquet\",\n",
    "        \"output_format\": \"delta\",\n",
    "        \"compression\": \"snappy\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Development configuration\n",
    "dev_config = {\n",
    "    \"app\": {\n",
    "        \"environment\": \"dev\",\n",
    "        \"debug\": True,\n",
    "        \"log_level\": \"DEBUG\"\n",
    "    },\n",
    "    \"spark\": {\n",
    "        \"master\": \"local[2]\",\n",
    "        \"sql_shuffle_partitions\": 4\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_path\": \"data/dev/input\",\n",
    "        \"output_path\": \"data/dev/output\"\n",
    "    },\n",
    "    \"database\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 5432,\n",
    "        \"name\": \"dev_database\",\n",
    "        \"username\": \"dev_user\",\n",
    "        \"password\": \"dev_password\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Production configuration\n",
    "prod_config = {\n",
    "    \"app\": {\n",
    "        \"environment\": \"prod\",\n",
    "        \"debug\": False,\n",
    "        \"log_level\": \"WARN\"\n",
    "    },\n",
    "    \"spark\": {\n",
    "        \"master\": \"yarn\",\n",
    "        \"sql_shuffle_partitions\": 1000\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"input_path\": \"s3a://prod-data-lake/input\",\n",
    "        \"output_path\": \"s3a://prod-data-lake/output\"\n",
    "    },\n",
    "    \"database\": {\n",
    "        \"host\": \"${DB_HOST}\",\n",
    "        \"port\": \"${DB_PORT}\",\n",
    "        \"name\": \"${DB_NAME}\",\n",
    "        \"username\": \"${DB_USERNAME}\",\n",
    "        \"password\": \"${DB_PASSWORD}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write configuration files\n",
    "with open(configs_dir / \"base.yaml\", 'w') as f:\n",
    "    yaml.dump(base_config, f, default_flow_style=False)\n",
    "\n",
    "with open(configs_dir / \"dev.yaml\", 'w') as f:\n",
    "    yaml.dump(dev_config, f, default_flow_style=False)\n",
    "\n",
    "with open(configs_dir / \"prod.yaml\", 'w') as f:\n",
    "    yaml.dump(prod_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"📝 Configuration files created:\")\n",
    "for config_file in configs_dir.glob(\"*.yaml\"):\n",
    "    print(f\"  📄 {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 3.3 Testing Configuration Loading\n",
    "\n",
    "Let's test our configuration system with different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration loading\n",
    "config_loader = ConfigLoader(\"configs\")\n",
    "\n",
    "# Load development configuration\n",
    "print(\"🔧 Loading Development Configuration:\")\n",
    "print(\"=\"*50)\n",
    "dev_config = config_loader.load_config(\"dev\")\n",
    "\n",
    "print(f\"Environment: {dev_config.environment}\")\n",
    "print(f\"Debug mode: {dev_config.debug}\")\n",
    "print(f\"Log level: {dev_config.log_level}\")\n",
    "print(f\"\\nSpark configuration:\")\n",
    "print(f\"  Master: {dev_config.spark.master}\")\n",
    "print(f\"  App name: {dev_config.spark.app_name}\")\n",
    "print(f\"  Shuffle partitions: {dev_config.spark.sql_shuffle_partitions}\")\n",
    "print(f\"\\nData configuration:\")\n",
    "print(f\"  Input path: {dev_config.data.input_path}\")\n",
    "print(f\"  Output path: {dev_config.data.output_path}\")\n",
    "print(f\"\\nDatabase configuration:\")\n",
    "print(f\"  JDBC URL: {dev_config.database.get_jdbc_url()}\")\n",
    "print(f\"  Username: {dev_config.database.username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load production configuration (with environment variables)\n",
    "print(\"\\n🏭 Loading Production Configuration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set some environment variables for demonstration\n",
    "os.environ[\"DB_HOST\"] = \"prod-db.company.com\"\n",
    "os.environ[\"DB_PORT\"] = \"5432\"\n",
    "os.environ[\"DB_NAME\"] = \"prod_analytics\"\n",
    "os.environ[\"DB_USERNAME\"] = \"analytics_user\"\n",
    "os.environ[\"DB_PASSWORD\"] = \"secure_password_123\"\n",
    "\n",
    "prod_config = config_loader.load_config(\"prod\")\n",
    "\n",
    "print(f\"Environment: {prod_config.environment}\")\n",
    "print(f\"Debug mode: {prod_config.debug}\")\n",
    "print(f\"Log level: {prod_config.log_level}\")\n",
    "print(f\"\\nSpark configuration:\")\n",
    "print(f\"  Master: {prod_config.spark.master}\")\n",
    "print(f\"  Shuffle partitions: {prod_config.spark.sql_shuffle_partitions}\")\n",
    "print(f\"\\nData configuration:\")\n",
    "print(f\"  Input path: {prod_config.data.input_path}\")\n",
    "print(f\"  Output path: {prod_config.data.output_path}\")\n",
    "print(f\"\\nDatabase configuration:\")\n",
    "print(f\"  JDBC URL: {prod_config.database.get_jdbc_url()}\")\n",
    "print(f\"  Username: {prod_config.database.username}\")\n",
    "print(f\"  Password: {'*' * len(prod_config.database.password)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Spark configuration conversion\n",
    "print(\"\\n⚙️ Spark Configuration for Development:\")\n",
    "print(\"=\"*50)\n",
    "spark_conf = dev_config.spark.to_spark_conf()\n",
    "for key, value in spark_conf.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create Spark session with configuration\n",
    "print(\"\\n🚀 Creating Spark session with configuration...\")\n",
    "builder = SparkSession.builder.appName(dev_config.spark.app_name).master(dev_config.spark.master)\n",
    "\n",
    "for key, value in spark_conf.items():\n",
    "    builder = builder.config(key, value)\n",
    "\n",
    "configured_spark = builder.getOrCreate()\n",
    "print(f\"✅ Spark session created: {configured_spark.sparkContext.appName}\")\n",
    "print(f\"📊 Shuffle partitions: {configured_spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "configured_spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔗 Module 4: Development Tools Integration\n",
    "\n",
    "Let's explore how to integrate modern development tools into our Spark workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌿 4.1 Git Workflow Examples\n",
    "\n",
    "Let's demonstrate Git best practices for Spark projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git workflow demonstration\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def run_git_command(command: str, cwd: str = \".\") -> str:\n",
    "    \"\"\"Run a git command and return the output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command.split(),\n",
    "            cwd=cwd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error: {e.stderr.strip()}\"\n",
    "\n",
    "# Check current git status\n",
    "print(\"🌿 Current Git Status:\")\n",
    "print(\"=\"*30)\n",
    "try:\n",
    "    status = run_git_command(\"git status --porcelain\")\n",
    "    if status:\n",
    "        print(\"Modified files:\")\n",
    "        for line in status.split('\\n'):\n",
    "            print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"✅ Working directory clean\")\n",
    "        \n",
    "    # Show current branch\n",
    "    branch = run_git_command(\"git branch --show-current\")\n",
    "    print(f\"\\n📍 Current branch: {branch}\")\n",
    "    \n",
    "    # Show recent commits\n",
    "    commits = run_git_command(\"git log --oneline -5\")\n",
    "    print(f\"\\n📝 Recent commits:\")\n",
    "    for line in commits.split('\\n'):\n",
    "        print(f\"  {line}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Git not available or not in a git repository: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 4.2 Pre-commit Configuration\n",
    "\n",
    "Let's create a comprehensive pre-commit configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pre-commit configuration\n",
    "pre_commit_config = \"\"\"\n",
    "# .pre-commit-config.yaml\n",
    "repos:\n",
    "  - repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v4.4.0\n",
    "    hooks:\n",
    "      - id: trailing-whitespace\n",
    "      - id: end-of-file-fixer\n",
    "      - id: check-yaml\n",
    "      - id: check-added-large-files\n",
    "        args: ['--maxkb=1000']\n",
    "      - id: check-merge-conflict\n",
    "      - id: debug-statements\n",
    "\n",
    "  - repo: https://github.com/psf/black\n",
    "    rev: 23.7.0\n",
    "    hooks:\n",
    "      - id: black\n",
    "        language_version: python3\n",
    "        args: [--line-length=100]\n",
    "\n",
    "  - repo: https://github.com/pycqa/isort\n",
    "    rev: 5.12.0\n",
    "    hooks:\n",
    "      - id: isort\n",
    "        args: [--profile=black, --line-length=100]\n",
    "\n",
    "  - repo: https://github.com/pycqa/flake8\n",
    "    rev: 6.0.0\n",
    "    hooks:\n",
    "      - id: flake8\n",
    "        args: [--max-line-length=100, --extend-ignore=E203,W503]\n",
    "\n",
    "  - repo: https://github.com/pre-commit/mirrors-mypy\n",
    "    rev: v1.5.1\n",
    "    hooks:\n",
    "      - id: mypy\n",
    "        additional_dependencies: [types-PyYAML, pydantic]\n",
    "\n",
    "  - repo: local\n",
    "    hooks:\n",
    "      - id: pytest\n",
    "        name: pytest\n",
    "        entry: pytest\n",
    "        language: python\n",
    "        pass_filenames: false\n",
    "        always_run: true\n",
    "        args: [tests/, --tb=short]\n",
    "\"\"\"\n",
    "\n",
    "# Write pre-commit configuration\n",
    "with open(\".pre-commit-config.yaml\", \"w\") as f:\n",
    "    f.write(pre_commit_config.strip())\n",
    "\n",
    "print(\"📋 Pre-commit configuration created:\")\n",
    "print(pre_commit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐳 4.3 Docker Development Environment\n",
    "\n",
    "Let's create a Docker setup for consistent development environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for development\n",
    "dockerfile_content = \"\"\"\n",
    "# Dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Install Java (required for Spark)\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y openjdk-11-jdk curl git && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set Java environment\n",
    "ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
    "\n",
    "# Install uv\n",
    "RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "ENV PATH=\"/root/.cargo/bin:$PATH\"\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy dependency files\n",
    "COPY pyproject.toml uv.lock ./\n",
    "\n",
    "# Install dependencies\n",
    "RUN uv sync --extra dev --extra docker\n",
    "\n",
    "# Copy source code\n",
    "COPY . .\n",
    "\n",
    "# Set Python path\n",
    "ENV PYTHONPATH=/app\n",
    "\n",
    "# Expose ports\n",
    "EXPOSE 4040 8888\n",
    "\n",
    "# Default command\n",
    "CMD [\"uv\", \"run\", \"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\"]\n",
    "\"\"\"\n",
    "\n",
    "# Create Docker Compose for development environment\n",
    "docker_compose_content = \"\"\"\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  spark-app:\n",
    "    build: .\n",
    "    volumes:\n",
    "      - .:/app\n",
    "      - ./data:/app/data\n",
    "    environment:\n",
    "      - ENVIRONMENT=dev\n",
    "      - PYTHONPATH=/app\n",
    "    ports:\n",
    "      - \"4040:4040\"  # Spark UI\n",
    "      - \"8888:8888\"  # Jupyter\n",
    "    depends_on:\n",
    "      - postgres\n",
    "      - minio\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:15\n",
    "    environment:\n",
    "      POSTGRES_DB: spark_dev\n",
    "      POSTGRES_USER: spark_user\n",
    "      POSTGRES_PASSWORD: spark_password\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "  minio:\n",
    "    image: minio/minio\n",
    "    ports:\n",
    "      - \"9000:9000\"\n",
    "      - \"9001:9001\"\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: minioadmin\n",
    "      MINIO_ROOT_PASSWORD: minioadmin\n",
    "    command: server /data --console-address \":9001\"\n",
    "    volumes:\n",
    "      - minio_data:/data\n",
    "\n",
    "  jupyter:\n",
    "    build: .\n",
    "    command: uv run jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "    ports:\n",
    "      - \"8889:8888\"\n",
    "    volumes:\n",
    "      - .:/app\n",
    "    environment:\n",
    "      - PYTHONPATH=/app\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  minio_data:\n",
    "\"\"\"\n",
    "\n",
    "# Write Docker files\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content.strip())\n",
    "\n",
    "with open(\"docker-compose.yml\", \"w\") as f:\n",
    "    f.write(docker_compose_content.strip())\n",
    "\n",
    "print(\"🐳 Docker configuration created:\")\n",
    "print(\"  📄 Dockerfile\")\n",
    "print(\"  📄 docker-compose.yml\")\n",
    "print(\"\\n🚀 To start the development environment:\")\n",
    "print(\"  docker-compose up -d\")\n",
    "print(\"\\n📱 Access points:\")\n",
    "print(\"  Jupyter: http://localhost:8888\")\n",
    "print(\"  Spark UI: http://localhost:4040\")\n",
    "print(\"  MinIO: http://localhost:9001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the interactive tutorial for professional Spark development setup. Let's summarize what you've learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ What You've Accomplished\n",
    "\n",
    "1. **🏗️ Project Structure Best Practices**\n",
    "   - Created modular Spark application architecture\n",
    "   - Implemented separation of concerns\n",
    "   - Built reusable components and utilities\n",
    "\n",
    "2. **🔧 Development Workflow**\n",
    "   - Developed DataFrame debugging utilities\n",
    "   - Created comprehensive testing framework\n",
    "   - Implemented performance profiling tools\n",
    "\n",
    "3. **⚙️ Configuration Management**\n",
    "   - Built hierarchical configuration system\n",
    "   - Implemented environment-specific settings\n",
    "   - Created secrets management patterns\n",
    "\n",
    "4. **🔗 Development Tools Integration**\n",
    "   - Explored Git workflow best practices\n",
    "   - Created pre-commit hook configuration\n",
    "   - Set up Docker development environment\n",
    "\n",
    "### 🎓 Key Takeaways\n",
    "\n",
    "- **Modularity**: Break your code into small, focused, testable components\n",
    "- **Configuration**: Use environment-specific configurations for flexibility\n",
    "- **Testing**: Write comprehensive tests for your transformations and logic\n",
    "- **Automation**: Use tools to enforce code quality and consistency\n",
    "- **Documentation**: Code should be self-documenting with clear structure\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Complete the exercises** in the `exercises/` directory\n",
    "2. **Explore the project templates** in the `templates/` directory\n",
    "3. **Set up your own project** using the patterns you've learned\n",
    "4. **Move on to Lesson 4**: File Formats Deep Dive\n",
    "\n",
    "### 📚 Additional Practice\n",
    "\n",
    "Try these challenges to reinforce your learning:\n",
    "\n",
    "1. Create a new Spark project using the modular structure\n",
    "2. Implement configuration for staging environment\n",
    "3. Add more comprehensive test cases\n",
    "4. Set up a CI/CD pipeline using GitHub Actions\n",
    "5. Containerize your application with Docker\n",
    "\n",
    "### 🆘 Getting Help\n",
    "\n",
    "If you encounter issues:\n",
    "1. Check the troubleshooting section in the README\n",
    "2. Run the validation scripts: `make validate-learning`\n",
    "3. Review the solution files in `solutions/`\n",
    "4. Use the debugging utilities you've learned\n",
    "\n",
    "**Happy coding! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}