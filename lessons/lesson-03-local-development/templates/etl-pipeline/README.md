# Spark ETL Pipeline Template

A production-ready Apache Spark ETL (Extract, Transform, Load) pipeline template with modern development practices, comprehensive testing, and deployment automation.

## ğŸš€ Features

- **Modular Architecture**: Clean separation of concerns with dedicated modules for extraction, transformation, and loading
- **Configuration Management**: Environment-specific configurations with validation
- **Data Quality**: Built-in data validation and quality checks
- **Error Handling**: Comprehensive error handling with retry mechanisms
- **Monitoring**: Integrated metrics and logging
- **Testing**: Unit, integration, and data quality tests
- **CI/CD Ready**: GitHub Actions workflows and Docker support
- **Cloud Support**: AWS S3, Azure Blob Storage, Google Cloud Storage

## ğŸ“ Project Structure

```
etl-pipeline/
â”œâ”€â”€ etl_pipeline/              # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                 # Command-line interface
â”‚   â”œâ”€â”€ config/                # Configuration management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py        # Settings classes
â”‚   â”‚   â””â”€â”€ environments/      # Environment configs
â”‚   â”œâ”€â”€ extractors/            # Data extraction modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # Base extractor class
â”‚   â”‚   â”œâ”€â”€ database.py       # Database extractors
â”‚   â”‚   â”œâ”€â”€ file.py           # File extractors
â”‚   â”‚   â””â”€â”€ api.py            # API extractors
â”‚   â”œâ”€â”€ transformers/          # Data transformation modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # Base transformer class
â”‚   â”‚   â”œâ”€â”€ cleaners.py       # Data cleaning
â”‚   â”‚   â”œâ”€â”€ validators.py     # Data validation
â”‚   â”‚   â””â”€â”€ enrichers.py      # Data enrichment
â”‚   â”œâ”€â”€ loaders/              # Data loading modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # Base loader class
â”‚   â”‚   â”œâ”€â”€ database.py       # Database loaders
â”‚   â”‚   â”œâ”€â”€ file.py           # File loaders
â”‚   â”‚   â””â”€â”€ delta.py          # Delta Lake loader
â”‚   â”œâ”€â”€ jobs/                 # ETL job definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # Base job class
â”‚   â”‚   â””â”€â”€ customer_etl.py   # Example ETL job
â”‚   â”œâ”€â”€ utils/                # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spark.py          # Spark utilities
â”‚   â”‚   â”œâ”€â”€ logging.py        # Logging setup
â”‚   â”‚   â””â”€â”€ metrics.py        # Metrics collection
â”‚   â””â”€â”€ schemas/              # Data schemas
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ customer.py       # Example schema
â”œâ”€â”€ tests/                    # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py          # Test configuration
â”‚   â”œâ”€â”€ unit/                # Unit tests
â”‚   â”œâ”€â”€ integration/         # Integration tests
â”‚   â””â”€â”€ data/                # Test data
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ dev.yaml
â”‚   â”œâ”€â”€ staging.yaml
â”‚   â””â”€â”€ prod.yaml
â”œâ”€â”€ docker/                  # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .github/                 # GitHub Actions
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ cd.yml
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ deploy.sh
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ pyproject.toml          # Project configuration
â”œâ”€â”€ Makefile               # Development commands
â””â”€â”€ README.md              # This file
```

## ğŸ› ï¸ Setup

### Prerequisites

- Python 3.9+
- Java 11+ (for Spark)
- Docker (optional)

### Installation

1. **Clone or copy this template**
2. **Install dependencies:**
   ```bash
   # Install with uv (recommended)
   uv sync --extra dev

   # Or with pip
   pip install -e ".[dev]"
   ```
3. **Setup pre-commit hooks:**
   ```bash
   pre-commit install
   ```
4. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

## ğŸš¦ Usage

### Command Line Interface

```bash
# Run ETL pipeline
etl-pipeline run --job customer_etl --env dev

# Validate configuration
etl-pipeline validate --env prod

# List available jobs
etl-pipeline list-jobs

# Run data quality checks
etl-pipeline quality-check --input data/customers.parquet
```

### Programmatic Usage

```python
from etl_pipeline.jobs.customer_etl import CustomerETLJob
from etl_pipeline.config.settings import load_config

# Load configuration
config = load_config("dev")

# Create and run ETL job
job = CustomerETLJob(config)
job.run()
```

## ğŸ”§ Configuration

Configuration is managed through YAML files in the `config/` directory:

```yaml
# config/dev.yaml
app:
  name: "customer-etl"
  environment: "dev"

spark:
  master: "local[2]"
  app_name: "CustomerETL-Dev"

data:
  input_path: "data/raw/customers"
  output_path: "data/processed/customers"
  format: "parquet"

database:
  host: "localhost"
  port: 5432
  database: "analytics_dev"
```

Environment variables can be used for sensitive data:

```yaml
database:
  username: "${DB_USERNAME}"
  password: "${DB_PASSWORD}"
```

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test types
make test-unit
make test-integration

# Run with coverage
make test-coverage

# Generate test report
make test-report
```

### Test Structure

- **Unit Tests**: Test individual functions and classes
- **Integration Tests**: Test component interactions
- **Data Quality Tests**: Validate data transformations

Example test:

```python
def test_customer_data_cleaning(spark_session):
    # Given
    raw_data = [
        ("C001", "John Doe", 25, "john@email.com"),
        ("C002", None, 16, "invalid-email"),  # Invalid data
    ]

    # When
    result = clean_customer_data(spark_session.createDataFrame(raw_data, schema))

    # Then
    assert result.count() == 1
    assert result.first().name == "John Doe"
```

## ğŸ“Š Monitoring

### Metrics

The pipeline collects metrics for:
- Record counts processed
- Processing time per stage
- Data quality scores
- Error rates

### Logging

Structured logging with different levels:
- INFO: General pipeline progress
- WARN: Data quality issues
- ERROR: Pipeline failures

```python
import structlog

logger = structlog.get_logger(__name__)

logger.info("Processing batch", batch_id=batch_id, record_count=count)
logger.error("Validation failed", error=str(e), records_failed=failed_count)
```

## ğŸ³ Docker

### Build and Run

```bash
# Build image
docker build -f docker/Dockerfile -t etl-pipeline .

# Run pipeline
docker run --rm \
  -v $(pwd)/data:/app/data \
  -e ENVIRONMENT=dev \
  etl-pipeline run --job customer_etl
```

### Docker Compose

```bash
# Start full stack (pipeline + dependencies)
docker-compose -f docker/docker-compose.yml up
```

## â˜ï¸ Cloud Deployment

### AWS

```bash
# Deploy to EMR
aws emr create-cluster \
  --name "ETL-Pipeline" \
  --applications Name=Spark \
  --ec2-attributes KeyName=my-key \
  --instance-type m5.xlarge \
  --instance-count 3

# Submit job
aws emr add-steps \
  --cluster-id j-XXXXX \
  --steps Type=Spark,Name="Customer ETL",ActionOnFailure=TERMINATE_CLUSTER,Args=[--class,CustomerETLJob,s3://my-bucket/etl-pipeline.jar]
```

### Azure

```bash
# Deploy to Azure Synapse
az synapse spark job submit \
  --workspace-name my-workspace \
  --spark-pool-name my-pool \
  --main-definition-file customer_etl.py
```

### Google Cloud

```bash
# Deploy to Dataproc
gcloud dataproc jobs submit pyspark \
  --cluster my-cluster \
  --region us-central1 \
  customer_etl.py
```

## ğŸ”„ CI/CD

GitHub Actions workflows are included:

- **CI Pipeline** (`.github/workflows/ci.yml`):
  - Code quality checks (black, isort, flake8)
  - Type checking (mypy)
  - Unit and integration tests
  - Security scanning

- **CD Pipeline** (`.github/workflows/cd.yml`):
  - Build Docker images
  - Deploy to staging/production
  - Run smoke tests

## ğŸ“ˆ Data Pipeline Best Practices

### Error Handling

```python
from etl_pipeline.utils.retry import retry_on_failure

@retry_on_failure(max_attempts=3, backoff_factor=2)
def extract_data(source_config):
    try:
        return extract_from_source(source_config)
    except ConnectionError as e:
        logger.error("Connection failed", error=str(e))
        raise
```

### Data Quality

```python
from etl_pipeline.transformers.validators import DataValidator

validator = DataValidator()
quality_report = validator.validate(
    df,
    rules={
        "completeness": {"min_threshold": 0.95},
        "uniqueness": {"columns": ["customer_id"]},
        "range": {"age": {"min": 0, "max": 120}}
    }
)
```

### Performance Optimization

```python
# Partition data for optimal performance
df.write \
  .partitionBy("year", "month") \
  .mode("overwrite") \
  .option("compression", "snappy") \
  .parquet(output_path)

# Cache frequently accessed data
df.cache()
```

## ğŸš¨ Troubleshooting

### Common Issues

1. **OutOfMemoryError**
   ```bash
   # Increase driver memory
   export SPARK_DRIVER_MEMORY=4g
   ```

2. **Connection timeouts**
   ```python
   # Increase connection timeout
   spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
   ```

3. **Data skew**
   ```python
   # Use salting for skewed joins
   df.withColumn("salt", (rand() * 10).cast("int"))
   ```

## ğŸ“š Additional Resources

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Delta Lake Guide](https://docs.delta.io/latest/index.html)
- [Spark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.