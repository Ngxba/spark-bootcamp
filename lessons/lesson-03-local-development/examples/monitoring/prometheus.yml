# Prometheus configuration for Spark application monitoring

global:
  scrape_interval: 15s
  evaluation_interval: 15s

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# Load rules once and periodically evaluate them
rule_files:
  - "spark_rules.yml"
  - "application_rules.yml"

# Scrape configurations
scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Spark Master
  - job_name: 'spark-master'
    static_configs:
      - targets: ['spark-master:8080']
    metrics_path: '/metrics/master/prometheus'
    scrape_interval: 30s

  # Spark Workers
  - job_name: 'spark-worker'
    static_configs:
      - targets: ['spark-worker:8081']
    metrics_path: '/metrics/worker/prometheus'
    scrape_interval: 30s

  # Spark Applications (Dynamic)
  - job_name: 'spark-applications'
    static_configs:
      - targets: ['spark-dev:4040']
    metrics_path: '/metrics/prometheus'
    scrape_interval: 10s
    honor_labels: true

  # Custom application metrics
  - job_name: 'spark-app-custom'
    static_configs:
      - targets: ['spark-dev:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s

  # PostgreSQL metrics
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']
    metrics_path: '/metrics'
    scrape_interval: 30s

  # Redis metrics
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    metrics_path: '/metrics'
    scrape_interval: 30s

  # MinIO metrics
  - job_name: 'minio'
    static_configs:
      - targets: ['minio:9000']
    metrics_path: '/minio/prometheus/metrics'
    scrape_interval: 30s

  # Kafka metrics (via JMX exporter)
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka:9308']
    scrape_interval: 30s

  # Node/Container metrics
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 30s

  # cAdvisor for container metrics
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    scrape_interval: 30s

# Service discovery for dynamic Spark applications
  - job_name: 'spark-apps-discovery'
    consul_sd_configs:
      - server: 'consul:8500'
        services: ['spark-app']
    relabel_configs:
      - source_labels: [__meta_consul_service_address]
        target_label: __address__
      - source_labels: [__meta_consul_service_port]
        target_label: __address__
        regex: (.+)
        replacement: ${1}:4040

# Recording rules for computed metrics
recording_rules:
  - name: spark_performance_rules
    rules:
      - record: spark:job_duration_seconds:rate5m
        expr: rate(spark_job_duration_seconds_total[5m])

      - record: spark:executor_memory_utilization
        expr: |
          (
            spark_executor_memory_used_bytes /
            spark_executor_memory_total_bytes
          ) * 100

      - record: spark:task_failure_rate
        expr: |
          rate(spark_task_result_total{result="failure"}[5m]) /
          rate(spark_task_result_total[5m])

      - record: spark:stage_duration_p95
        expr: |
          histogram_quantile(0.95,
            rate(spark_stage_duration_seconds_bucket[5m])
          )

# Custom metric relabeling
metric_relabel_configs:
  # Add environment label
  - target_label: environment
    replacement: development

  # Add application label from Spark context
  - source_labels: [__name__]
    regex: spark_(.+)
    target_label: component
    replacement: spark

  # Normalize job names
  - source_labels: [job]
    regex: (.+)-\d+
    target_label: job_name
    replacement: ${1}