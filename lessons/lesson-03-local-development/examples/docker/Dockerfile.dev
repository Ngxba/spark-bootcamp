# Development Dockerfile for Spark applications
# Optimized for local development with debugging tools and hot reload

FROM python:3.9-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Install system dependencies
RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    wget \
    curl \
    git \
    vim \
    htop \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Install Spark
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && tar xzf "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -C /opt/ \
    && ln -s "/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" /opt/spark \
    && rm "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_PYTHON=python3

# Create app directory
WORKDIR /app

# Create non-root user for development
RUN groupadd -r sparkuser && useradd -r -g sparkuser sparkuser
RUN chown -R sparkuser:sparkuser /app

# Install Python dependencies
COPY requirements.txt requirements-dev.txt ./
RUN pip install --upgrade pip && \
    pip install -r requirements.txt && \
    pip install -r requirements-dev.txt

# Install development tools
RUN pip install \
    ipython \
    jupyter \
    jupyterlab \
    black \
    isort \
    flake8 \
    mypy \
    pytest \
    pytest-cov \
    pre-commit

# Copy application code
COPY --chown=sparkuser:sparkuser . .

# Switch to non-root user
USER sparkuser

# Install pre-commit hooks
RUN pre-commit install || true

# Create directories for Spark
RUN mkdir -p /app/spark-warehouse /app/logs

# Development-specific Spark configuration
ENV SPARK_CONF_DIR=/app/conf
RUN mkdir -p $SPARK_CONF_DIR

# Create development Spark configuration
RUN echo "spark.sql.warehouse.dir=/app/spark-warehouse" > $SPARK_CONF_DIR/spark-defaults.conf && \
    echo "spark.eventLog.enabled=true" >> $SPARK_CONF_DIR/spark-defaults.conf && \
    echo "spark.eventLog.dir=/app/logs" >> $SPARK_CONF_DIR/spark-defaults.conf && \
    echo "spark.history.fs.logDirectory=/app/logs" >> $SPARK_CONF_DIR/spark-defaults.conf && \
    echo "spark.sql.adaptive.enabled=true" >> $SPARK_CONF_DIR/spark-defaults.conf && \
    echo "spark.sql.adaptive.coalescePartitions.enabled=true" >> $SPARK_CONF_DIR/spark-defaults.conf

# Expose ports for development
EXPOSE 4040 4041 4042 4043 8080 8081 18080 8888

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:4040/api/v1/applications || exit 1

# Default command for development
CMD ["python", "-m", "IPython"]

# Development volume mounts (used with docker-compose)
# - ./src:/app/src (source code hot reload)
# - ./tests:/app/tests (test files)
# - ./data:/app/data (data files)
# - ./notebooks:/app/notebooks (Jupyter notebooks)
# - ./logs:/app/logs (Spark logs)

# Development aliases for common commands
RUN echo 'alias ll="ls -la"' >> ~/.bashrc && \
    echo 'alias spark-shell="$SPARK_HOME/bin/spark-shell"' >> ~/.bashrc && \
    echo 'alias pyspark="$SPARK_HOME/bin/pyspark"' >> ~/.bashrc && \
    echo 'alias spark-submit="$SPARK_HOME/bin/spark-submit"' >> ~/.bashrc

# Development environment information
RUN echo "Development Environment Information:" && \
    echo "Python version: $(python --version)" && \
    echo "Java version: $(java -version 2>&1 | head -n 1)" && \
    echo "Spark version: $SPARK_VERSION" && \
    echo "Spark home: $SPARK_HOME"