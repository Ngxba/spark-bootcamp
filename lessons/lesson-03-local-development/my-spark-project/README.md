# My Spark Project

A production-ready Apache Spark application demonstrating industry best practices for local development setup, modular architecture, and data processing workflows.

## ğŸ¯ Project Overview

This project showcases modern Spark development practices including:

- **Modular Architecture**: Clean separation of concerns with dedicated modules for jobs, transformations, utilities, and configuration
- **Data Processing Pipeline**: Complete ETL workflow with data cleaning, transformation, and analytics
- **Testing Framework**: Comprehensive unit and integration tests
- **Configuration Management**: Environment-specific configuration with YAML and environment variables
- **Industry Standards**: Following Python packaging conventions and Spark optimization patterns

## ğŸ“ Project Structure

```
my-spark-project/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ pyproject.toml           # Project configuration and dependencies
â”œâ”€â”€ Makefile                 # Development workflow automation
â”œâ”€â”€ .env.example             # Environment variables template
â”œâ”€â”€ .gitignore              # Git ignore patterns
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ config/             # Configuration management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py     # Application settings and config loading
â”‚   â”œâ”€â”€ jobs/               # Spark job implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_job.py     # Abstract base job class
â”‚   â”‚   â””â”€â”€ etl_job.py      # ETL pipeline implementation
â”‚   â”œâ”€â”€ transformations/    # Data transformation modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cleaning.py     # Data cleaning utilities
â”‚   â”‚   â””â”€â”€ aggregations.py # Data aggregation functions
â”‚   â”œâ”€â”€ utils/              # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spark_utils.py  # Spark optimization utilities
â”‚   â”‚   â””â”€â”€ file_utils.py   # File operation utilities
â”‚   â””â”€â”€ schemas/            # Data schema definitions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ sales_schema.py # Schema definitions for sales data
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py         # Test configuration and fixtures
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â”‚   â”œâ”€â”€ test_data_cleaning.py
â”‚   â”‚   â””â”€â”€ test_data_aggregations.py
â”‚   â””â”€â”€ integration/        # Integration tests
â”‚       â””â”€â”€ test_etl_job.py
â”œâ”€â”€ data/                   # Data directory
â”‚   â”œâ”€â”€ raw/                # Raw input data
â”‚   â”œâ”€â”€ processed/          # Processed data
â”‚   â””â”€â”€ output/             # Output data
â””â”€â”€ scripts/                # Utility scripts
    â””â”€â”€ run_etl.py          # ETL job runner script
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Java 8 or 11 (for Spark)
- uv package manager

### Installation

1. **Clone and setup the project:**
   ```bash
   # Navigate to the project directory
   cd my-spark-project

   # Create virtual environment and install dependencies
   make setup
   make install
   ```

2. **Activate the virtual environment:**
   ```bash
   source .venv/bin/activate
   ```

3. **Verify the setup:**
   ```bash
   make verify-setup
   ```

## ğŸ”§ Development Workflow

### Available Make Commands

```bash
# Environment Management
make setup                  # Create virtual environment
make install               # Install dependencies
make activate              # Show activation command
make clean                 # Clean generated files

# Development
make test                  # Run tests
make run-etl              # Run ETL pipeline
make shell                # Start Python shell with environment

# Quality Assurance
make lint                 # Run code linting (if configured)
make format               # Format code (if configured)

# Verification
make verify-setup         # Verify complete setup
make verify-spark         # Verify Spark installation
```

### Running the ETL Pipeline

1. **Prepare sample data:**
   ```bash
   # The project includes sample data in the main lesson directory
   # Copy it to the project data directory
   cp ../data/*.csv data/
   ```

2. **Run the ETL job:**
   ```bash
   python scripts/run_etl.py
   ```

3. **Check the output:**
   ```bash
   ls -la data/output/
   ```

## ğŸ“Š Data Processing Pipeline

The ETL pipeline processes sales, customer, and product data through several stages:

### 1. Data Ingestion
- Reads CSV files with schema validation
- Supports multiple input formats
- Configurable input paths

### 2. Data Cleaning
- **Customer Data**: Standardizes gender values, validates age ranges, cleans text fields
- **Product Data**: Validates prices, standardizes categories, cleans product names
- **Sales Data**: Validates quantities and prices, ensures data consistency

### 3. Data Transformation
- **Enrichment**: Joins sales data with customer and product information
- **Aggregations**: Creates customer analytics, product performance metrics, time-based analysis
- **Segmentation**: Implements RFM analysis for customer segmentation

### 4. Data Quality Checks
- Monitors null values and data completeness
- Validates business rules and constraints
- Generates data quality reports

### 5. Output Generation
- Saves results in multiple formats (Parquet, CSV)
- Optimized partitioning for performance
- Comprehensive analytics datasets

## ğŸ§ª Testing

The project includes comprehensive testing:

### Running Tests

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/unit/          # Unit tests only
pytest tests/integration/   # Integration tests only

# Run with coverage
pytest --cov=src
```

### Test Structure

- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test complete workflows end-to-end
- **Fixtures**: Shared test data and Spark session setup

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file from the template:

```bash
cp .env.example .env
```

Key configuration options:

```bash
# Spark Configuration
SPARK_MASTER=local[*]
SPARK_EXECUTOR_MEMORY=2g
SPARK_DRIVER_MEMORY=1g

# Data Paths
DATA_INPUT_PATH=data
DATA_OUTPUT_PATH=data/output

# Application Settings
ENVIRONMENT=development
LOG_LEVEL=INFO
DEBUG_MODE=true
```

### Configuration Files

The project supports environment-specific configuration:

- `config/base.yaml`: Base configuration
- `config/development.yaml`: Development environment
- `config/production.yaml`: Production environment

## ğŸ—ï¸ Architecture Patterns

### Job Architecture

All Spark jobs inherit from `BaseSparkJob` which provides:

- Spark session management
- Configuration handling
- Logging setup
- Error handling
- Resource cleanup

### Transformation Modules

Reusable transformation functions organized by purpose:

- **DataCleaner**: Data validation and cleaning
- **DataAggregator**: Business analytics and aggregations
- **SchemaUtils**: Schema validation and management

### Utility Classes

Helper classes for common operations:

- **SparkUtils**: Spark optimization and performance utilities
- **FileUtils**: File system operations and path management
- **ConfigManager**: Configuration loading and validation

## ğŸ“ˆ Performance Optimization

The project includes several performance optimizations:

### Spark Configuration

- Adaptive Query Execution enabled
- Optimized memory settings
- Arrow-based operations for Pandas integration
- Dynamic partition coalescing

### Data Processing

- Broadcast joins for small datasets
- Optimal partitioning strategies
- Caching for reused DataFrames
- Column pruning and predicate pushdown

### Development Best Practices

- Schema enforcement for type safety
- Lazy evaluation optimization
- Resource management and cleanup
- Memory-efficient operations

## ğŸ” Monitoring and Debugging

### Spark UI

Access the Spark UI at `http://localhost:4040` when running jobs to monitor:

- Job execution progress
- Stage and task details
- Storage and memory usage
- SQL query plans

### Logging

Comprehensive logging throughout the application:

- Configurable log levels
- Structured logging with timestamps
- Error tracking and debugging information
- Performance metrics

## ğŸ¤ Contributing

1. Follow the existing code structure and patterns
2. Add tests for new functionality
3. Update documentation for changes
4. Run the test suite before submitting changes
5. Follow Python PEP 8 style guidelines

## ğŸ“š Learning Objectives

This project demonstrates:

1. **Production-Ready Architecture**: Modular design patterns for maintainable Spark applications
2. **Testing Strategies**: Unit and integration testing for data pipelines
3. **Configuration Management**: Environment-specific settings and deployment patterns
4. **Performance Optimization**: Spark tuning and optimization techniques
5. **Data Quality**: Validation, cleaning, and monitoring practices
6. **Development Workflow**: Automation, testing, and deployment practices

## ğŸ”— Related Resources

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Python Packaging User Guide](https://packaging.python.org/)

## ğŸ“„ License

This project is part of the Spark Bootcamp educational materials.

---

**Made with â¤ï¸ for learning Apache Spark development best practices**