{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames & SQL - Interactive Learning Notebook\n",
    "\n",
    "Welcome to Lesson 2 of the Spark Bootcamp! This notebook provides hands-on experience with Spark DataFrames and SQL operations.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand DataFrame fundamentals and advantages over RDDs\n",
    "- Create DataFrames from various sources\n",
    "- Perform data transformations and aggregations\n",
    "- Use Spark SQL for complex queries\n",
    "- Apply window functions and analytics\n",
    "- Optimize performance with caching and query plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setting Up Spark DataFrame Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 22:41:05 WARN Utils: Your hostname, Liams-MacBook-Pro.local resolves to a loopback address: 127.0.2.3; using 192.168.1.240 instead (on interface en0)\n",
      "25/09/29 22:41:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/29 22:41:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.7\n",
      "Spark context available: True\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrames-Learning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark context available: {spark.sparkContext is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: DataFrame Creation Fundamentals\n",
    "\n",
    "Let's start by creating DataFrames using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created from list of tuples:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-----------+------+\n",
      "|           name|age| department|salary|\n",
      "+---------------+---+-----------+------+\n",
      "|  Alice Johnson| 28|Engineering| 85000|\n",
      "|       Bob Chen| 34|  Marketing| 65000|\n",
      "|  Charlie Davis| 29|Engineering| 78000|\n",
      "|Diana Rodriguez| 31|      Sales| 72000|\n",
      "|     Eve Wilson| 26|Engineering| 92000|\n",
      "+---------------+---+-----------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 22:41:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Create DataFrame from Python data structures\n",
    "employee_data = [\n",
    "    (\"Alice Johnson\", 28, \"Engineering\", 85000),\n",
    "    (\"Bob Chen\", 34, \"Marketing\", 65000),\n",
    "    (\"Charlie Davis\", 29, \"Engineering\", 78000),\n",
    "    (\"Diana Rodriguez\", 31, \"Sales\", 72000),\n",
    "    (\"Eve Wilson\", 26, \"Engineering\", 92000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"department\", \"salary\"]\n",
    "employees_df = spark.createDataFrame(employee_data, columns)\n",
    "\n",
    "print(\"DataFrame created from list of tuples:\")\n",
    "employees_df.show()\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with explicit schema:\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "Row count: 5\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Create DataFrame with explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employees_typed_df = spark.createDataFrame(employee_data, schema)\n",
    "\n",
    "print(\"DataFrame with explicit schema:\")\n",
    "employees_typed_df.printSchema()\n",
    "print(f\"Row count: {employees_typed_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from dictionaries:\n",
      "+-----------+-------+------+------+-----+\n",
      "|   category|     id|  name| price|stock|\n",
      "+-----------+-------+------+------+-----+\n",
      "|Electronics|PROD001|Laptop|999.99|   50|\n",
      "|Electronics|PROD002| Mouse| 29.99|  200|\n",
      "|  Furniture|PROD003| Chair|199.99|   30|\n",
      "+-----------+-------+------+------+-----+\n",
      "\n",
      "Columns: ['category', 'id', 'name', 'price', 'stock']\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Create DataFrame from dictionaries\n",
    "product_dicts = [\n",
    "    {\"id\": \"PROD001\", \"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99, \"stock\": 50},\n",
    "    {\"id\": \"PROD002\", \"name\": \"Mouse\", \"category\": \"Electronics\", \"price\": 29.99, \"stock\": 200},\n",
    "    {\"id\": \"PROD003\", \"name\": \"Chair\", \"category\": \"Furniture\", \"price\": 199.99, \"stock\": 30}\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(product_dicts)\n",
    "\n",
    "print(\"DataFrame from dictionaries:\")\n",
    "products_df.show()\n",
    "print(f\"Columns: {products_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Basic DataFrame Operations\n",
    "\n",
    "Learn the fundamental operations for working with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic DataFrame Operations ===\n",
      "\n",
      "1. Select name and salary:\n",
      "+---------------+------+\n",
      "|           name|salary|\n",
      "+---------------+------+\n",
      "|  Alice Johnson| 85000|\n",
      "|       Bob Chen| 65000|\n",
      "|  Charlie Davis| 78000|\n",
      "|Diana Rodriguez| 72000|\n",
      "|     Eve Wilson| 92000|\n",
      "+---------------+------+\n",
      "\n",
      "\n",
      "2. Filter Engineering employees:\n",
      "+-------------+---+-----------+------+\n",
      "|         name|age| department|salary|\n",
      "+-------------+---+-----------+------+\n",
      "|Alice Johnson| 28|Engineering| 85000|\n",
      "|Charlie Davis| 29|Engineering| 78000|\n",
      "|   Eve Wilson| 26|Engineering| 92000|\n",
      "+-------------+---+-----------+------+\n",
      "\n",
      "\n",
      "3. Sort by salary descending:\n",
      "+---------------+---+-----------+------+\n",
      "|           name|age| department|salary|\n",
      "+---------------+---+-----------+------+\n",
      "|     Eve Wilson| 26|Engineering| 92000|\n",
      "|  Alice Johnson| 28|Engineering| 85000|\n",
      "|  Charlie Davis| 29|Engineering| 78000|\n",
      "|Diana Rodriguez| 31|      Sales| 72000|\n",
      "|       Bob Chen| 34|  Marketing| 65000|\n",
      "+---------------+---+-----------+------+\n",
      "\n",
      "\n",
      "4. Add annual bonus column (10% of salary):\n",
      "+---------------+------+------------+\n",
      "|           name|salary|annual_bonus|\n",
      "+---------------+------+------------+\n",
      "|  Alice Johnson| 85000|      8500.0|\n",
      "|       Bob Chen| 65000|      6500.0|\n",
      "|  Charlie Davis| 78000|      7800.0|\n",
      "|Diana Rodriguez| 72000|      7200.0|\n",
      "|     Eve Wilson| 92000|      9200.0|\n",
      "+---------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "print(\"=== Basic DataFrame Operations ===\")\n",
    "\n",
    "# Select specific columns\n",
    "print(\"\\n1. Select name and salary:\")\n",
    "employees_df.select(\"name\", \"salary\").show()\n",
    "\n",
    "# Filter data\n",
    "print(\"\\n2. Filter Engineering employees:\")\n",
    "engineers = employees_df.filter(col(\"department\") == \"Engineering\")\n",
    "engineers.show()\n",
    "\n",
    "# Sort data\n",
    "print(\"\\n3. Sort by salary descending:\")\n",
    "employees_df.orderBy(desc(\"salary\")).show()\n",
    "\n",
    "# Add calculated columns\n",
    "print(\"\\n4. Add annual bonus column (10% of salary):\")\n",
    "with_bonus = employees_df.withColumn(\"annual_bonus\", col(\"salary\") * 0.1)\n",
    "with_bonus.select(\"name\", \"salary\", \"annual_bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aggregation Operations ===\n",
      "\n",
      "1. Overall statistics:\n",
      "+---------------+----------+----------+----------+\n",
      "|total_employees|avg_salary|max_salary|min_salary|\n",
      "+---------------+----------+----------+----------+\n",
      "|              5|   78400.0|     92000|     65000|\n",
      "+---------------+----------+----------+----------+\n",
      "\n",
      "\n",
      "2. Statistics by department:\n",
      "+-----------+--------------+----------+-------------+\n",
      "| department|employee_count|avg_salary|total_payroll|\n",
      "+-----------+--------------+----------+-------------+\n",
      "|Engineering|             3|   85000.0|       255000|\n",
      "|  Marketing|             1|   65000.0|        65000|\n",
      "|      Sales|             1|   72000.0|        72000|\n",
      "+-----------+--------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations\n",
    "print(\"=== Aggregation Operations ===\")\n",
    "\n",
    "# Basic aggregations\n",
    "print(\"\\n1. Overall statistics:\")\n",
    "stats = employees_df.agg(\n",
    "    count(\"*\").alias(\"total_employees\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\")\n",
    ")\n",
    "stats.show()\n",
    "\n",
    "# Group by aggregations\n",
    "print(\"\\n2. Statistics by department:\")\n",
    "dept_stats = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    round(avg(\"salary\"), 2).alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_payroll\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Column Functions and Transformations\n",
    "\n",
    "Explore the rich set of column functions available in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== String Functions ===\n",
      "+---------------+---------------+-----------+----------+---------+\n",
      "|name           |name_upper     |name_length|first_name|last_name|\n",
      "+---------------+---------------+-----------+----------+---------+\n",
      "|Alice Johnson  |ALICE JOHNSON  |13         |Alice     |Johnson  |\n",
      "|Bob Chen       |BOB CHEN       |8          |Bob       |Chen     |\n",
      "|Charlie Davis  |CHARLIE DAVIS  |13         |Charlie   |Davis    |\n",
      "|Diana Rodriguez|DIANA RODRIGUEZ|15         |Diana     |Rodriguez|\n",
      "|Eve Wilson     |EVE WILSON     |10         |Eve       |Wilson   |\n",
      "+---------------+---------------+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String functions\n",
    "print(\"=== String Functions ===\")\n",
    "\n",
    "string_demo_df = employees_df.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"name_upper\"),\n",
    "    length(col(\"name\")).alias(\"name_length\"),\n",
    "    split(col(\"name\"), \" \").getItem(0).alias(\"first_name\"),\n",
    "    split(col(\"name\"), \" \").getItem(1).alias(\"last_name\")\n",
    ")\n",
    "\n",
    "string_demo_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conditional Logic ===\n",
      "+---------------+---+------+----------+-----------+\n",
      "|           name|age|salary| age_group|salary_band|\n",
      "+---------------+---+------+----------+-----------+\n",
      "|  Alice Johnson| 28| 85000|     Young|       High|\n",
      "|       Bob Chen| 34| 65000|Mid-career|   Standard|\n",
      "|  Charlie Davis| 29| 78000|     Young|     Medium|\n",
      "|Diana Rodriguez| 31| 72000|Mid-career|     Medium|\n",
      "|     Eve Wilson| 26| 92000|     Young|       High|\n",
      "+---------------+---+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conditional logic with when/otherwise\n",
    "print(\"=== Conditional Logic ===\")\n",
    "\n",
    "conditional_df = employees_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"salary\"),\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "        .when(col(\"age\") < 40, \"Mid-career\")\n",
    "        .otherwise(\"Experienced\").alias(\"age_group\"),\n",
    "    when(col(\"salary\") > 80000, \"High\")\n",
    "        .when(col(\"salary\") > 70000, \"Medium\")\n",
    "        .otherwise(\"Standard\").alias(\"salary_band\")\n",
    ")\n",
    "\n",
    "conditional_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Spark SQL Integration\n",
    "\n",
    "Use SQL syntax for data analysis with temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spark SQL Queries ===\n",
      "\n",
      "1. Basic SELECT with WHERE:\n",
      "+-------------+-----------+------+\n",
      "|         name| department|salary|\n",
      "+-------------+-----------+------+\n",
      "|   Eve Wilson|Engineering| 92000|\n",
      "|Alice Johnson|Engineering| 85000|\n",
      "|Charlie Davis|Engineering| 78000|\n",
      "+-------------+-----------+------+\n",
      "\n",
      "\n",
      "2. Aggregation with GROUP BY:\n",
      "+-----------+--------------+----------+----------+\n",
      "| department|employee_count|avg_salary|max_salary|\n",
      "+-----------+--------------+----------+----------+\n",
      "|Engineering|             3|   85000.0|     92000|\n",
      "|      Sales|             1|   72000.0|     72000|\n",
      "|  Marketing|             1|   65000.0|     65000|\n",
      "+-----------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary views for SQL queries\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"=== Spark SQL Queries ===\")\n",
    "\n",
    "# Basic SQL query\n",
    "print(\"\\n1. Basic SELECT with WHERE:\")\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 75000\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "result1.show()\n",
    "\n",
    "# Aggregation with GROUP BY\n",
    "print(\"\\n2. Aggregation with GROUP BY:\")\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Window Functions and Advanced Analytics\n",
    "\n",
    "Perform sophisticated analytics using window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Advanced Window Functions ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 22:48:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/29 22:48:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/29 22:48:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/29 22:48:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+------+---------+------------+---------------+--------+-----------------+\n",
      "|           name| department|salary|dept_rank|overall_rank|dept_dense_rank|dept_avg|next_lower_salary|\n",
      "+---------------+-----------+------+---------+------------+---------------+--------+-----------------+\n",
      "|     Eve Wilson|Engineering| 92000|        1|           1|              1| 85000.0|             NULL|\n",
      "|  Alice Johnson|Engineering| 85000|        2|           2|              2| 85000.0|            92000|\n",
      "|  Charlie Davis|Engineering| 78000|        3|           3|              3| 85000.0|            85000|\n",
      "|       Bob Chen|  Marketing| 65000|        1|           5|              1| 65000.0|             NULL|\n",
      "|Diana Rodriguez|      Sales| 72000|        1|           4|              1| 72000.0|             NULL|\n",
      "+---------------+-----------+------+---------+------------+---------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window functions for ranking and analytics\n",
    "print(\"=== Advanced Window Functions ===\")\n",
    "\n",
    "# Define windows\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "overall_window = Window.orderBy(desc(\"salary\"))\n",
    "\n",
    "# Apply window functions\n",
    "windowed_analysis = employees_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"department\"),\n",
    "    col(\"salary\"),\n",
    "    \n",
    "    # Ranking functions\n",
    "    row_number().over(dept_window).alias(\"dept_rank\"),\n",
    "    rank().over(overall_window).alias(\"overall_rank\"),\n",
    "    dense_rank().over(dept_window).alias(\"dept_dense_rank\"),\n",
    "    \n",
    "    # Analytical functions\n",
    "    round(avg(\"salary\").over(Window.partitionBy(\"department\")), 2).alias(\"dept_avg\"),\n",
    "    lag(\"salary\", 1).over(dept_window).alias(\"next_lower_salary\")\n",
    ")\n",
    "\n",
    "windowed_analysis.orderBy(\"department\", \"dept_rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Performance Optimization\n",
    "\n",
    "Learn about caching and query optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching for performance\n",
    "print(\"=== Caching and Performance ===\")\n",
    "\n",
    "# Cache DataFrame for reuse\n",
    "employees_df.cache()\n",
    "employees_df.count()  # Force caching\n",
    "\n",
    "print(\"DataFrame cached for better performance in repeated operations\")\n",
    "\n",
    "# Query execution plan\n",
    "print(\"\\n=== Query Execution Plan ===\")\n",
    "complex_query = employees_df.filter(col(\"salary\") > 75000).groupBy(\"department\").avg(\"salary\")\n",
    "print(\"\\nExecution plan for complex query:\")\n",
    "complex_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Working with Complex Data Types\n",
    "\n",
    "Handle arrays, maps, and nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with arrays and complex types\n",
    "print(\"=== Complex Data Types ===\")\n",
    "\n",
    "# Create data with arrays\n",
    "complex_data = [\n",
    "    (\"Alice\", [\"Python\", \"SQL\", \"Spark\"]),\n",
    "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
    "    (\"Charlie\", [\"Python\", \"R\", \"SQL\", \"Tableau\"])\n",
    "]\n",
    "\n",
    "complex_df = spark.createDataFrame(complex_data, [\"name\", \"skills\"])\n",
    "\n",
    "print(\"\\nOriginal complex data:\")\n",
    "complex_df.show(truncate=False)\n",
    "\n",
    "# Work with arrays\n",
    "print(\"\\nArray operations:\")\n",
    "array_ops = complex_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"skills\"),\n",
    "    size(col(\"skills\")).alias(\"skill_count\"),\n",
    "    array_contains(col(\"skills\"), \"Python\").alias(\"knows_python\"),\n",
    "    col(\"skills\")[0].alias(\"primary_skill\")\n",
    ")\n",
    "array_ops.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Integration with Pandas\n",
    "\n",
    "Convert between Spark DataFrames and Pandas for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "print(\"=== Spark to Pandas Integration ===\")\n",
    "\n",
    "# Get department statistics\n",
    "dept_stats_spark = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    round(avg(\"salary\"), 0).alias(\"avg_salary\")\n",
    ")\n",
    "\n",
    "# Convert to Pandas\n",
    "dept_stats_pandas = dept_stats_spark.toPandas()\n",
    "\n",
    "print(\"\\nDepartment statistics (Pandas):\")\n",
    "print(dept_stats_pandas)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Employee count by department\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(dept_stats_pandas['department'], dept_stats_pandas['employee_count'])\n",
    "plt.title('Employee Count by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Employee Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Average salary by department\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(dept_stats_pandas['department'], dept_stats_pandas['avg_salary'])\n",
    "plt.title('Average Salary by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Average Salary ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Spark DataFrames & SQL interactive tutorial. You've learned:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **DataFrame Creation** - From various data sources and with different schemas\n",
    "2. **Basic Operations** - Select, filter, sort, and aggregate data\n",
    "3. **Column Functions** - String, numeric, date, and conditional operations\n",
    "4. **Spark SQL** - Using SQL syntax for complex queries\n",
    "5. **Window Functions** - Advanced analytics and ranking\n",
    "6. **Performance Optimization** - Caching and execution plan analysis\n",
    "7. **Complex Data Types** - Arrays, maps, and nested structures\n",
    "8. **Integration** - Working with Pandas and visualization\n",
    "\n",
    "### Next Steps:\n",
    "1. **Practice with Exercises** - Complete the 7 hands-on exercises\n",
    "2. **Explore Solutions** - Study the provided solution files\n",
    "3. **Run Tests** - Validate your understanding with the test suite\n",
    "4. **Advanced Topics** - Move on to streaming, MLlib, or advanced optimizations\n",
    "\n",
    "### Resources:\n",
    "- Exercise files in the `exercises/` directory\n",
    "- Solution files in the `solutions/` directory\n",
    "- Test your code with `make test`\n",
    "- Use `make help` to see all available commands\n",
    "\n",
    "Happy Spark learning! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
