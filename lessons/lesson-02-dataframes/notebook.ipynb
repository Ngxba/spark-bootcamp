{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames & SQL - Interactive Learning Notebook\n",
    "\n",
    "Welcome to Lesson 2 of the Spark Bootcamp! This notebook provides hands-on experience with Spark DataFrames and SQL operations.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand DataFrame fundamentals and advantages over RDDs\n",
    "- Create DataFrames from various sources\n",
    "- Perform data transformations and aggregations\n",
    "- Use Spark SQL for complex queries\n",
    "- Apply window functions and analytics\n",
    "- Optimize performance with caching and query plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setting Up Spark DataFrame Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrames-Learning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark context available: {spark.sparkContext is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: DataFrame Creation Fundamentals\n",
    "\n",
    "Let's start by creating DataFrames using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Create DataFrame from Python data structures\n",
    "employee_data = [\n",
    "    (\"Alice Johnson\", 28, \"Engineering\", 85000),\n",
    "    (\"Bob Chen\", 34, \"Marketing\", 65000),\n",
    "    (\"Charlie Davis\", 29, \"Engineering\", 78000),\n",
    "    (\"Diana Rodriguez\", 31, \"Sales\", 72000),\n",
    "    (\"Eve Wilson\", 26, \"Engineering\", 92000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"department\", \"salary\"]\n",
    "employees_df = spark.createDataFrame(employee_data, columns)\n",
    "\n",
    "print(\"DataFrame created from list of tuples:\")\n",
    "employees_df.show()\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Create DataFrame with explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "employees_typed_df = spark.createDataFrame(employee_data, schema)\n",
    "\n",
    "print(\"DataFrame with explicit schema:\")\n",
    "employees_typed_df.printSchema()\n",
    "print(f\"Row count: {employees_typed_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Create DataFrame from dictionaries\n",
    "product_dicts = [\n",
    "    {\"id\": \"PROD001\", \"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99, \"stock\": 50},\n",
    "    {\"id\": \"PROD002\", \"name\": \"Mouse\", \"category\": \"Electronics\", \"price\": 29.99, \"stock\": 200},\n",
    "    {\"id\": \"PROD003\", \"name\": \"Chair\", \"category\": \"Furniture\", \"price\": 199.99, \"stock\": 30}\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(product_dicts)\n",
    "\n",
    "print(\"DataFrame from dictionaries:\")\n",
    "products_df.show()\n",
    "print(f\"Columns: {products_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Basic DataFrame Operations\n",
    "\n",
    "Learn the fundamental operations for working with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations\n",
    "print(\"=== Basic DataFrame Operations ===\")\n",
    "\n",
    "# Select specific columns\n",
    "print(\"\\n1. Select name and salary:\")\n",
    "employees_df.select(\"name\", \"salary\").show()\n",
    "\n",
    "# Filter data\n",
    "print(\"\\n2. Filter Engineering employees:\")\n",
    "engineers = employees_df.filter(col(\"department\") == \"Engineering\")\n",
    "engineers.show()\n",
    "\n",
    "# Sort data\n",
    "print(\"\\n3. Sort by salary descending:\")\n",
    "employees_df.orderBy(desc(\"salary\")).show()\n",
    "\n",
    "# Add calculated columns\n",
    "print(\"\\n4. Add annual bonus column (10% of salary):\")\n",
    "with_bonus = employees_df.withColumn(\"annual_bonus\", col(\"salary\") * 0.1)\n",
    "with_bonus.select(\"name\", \"salary\", \"annual_bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "print(\"=== Aggregation Operations ===\")\n",
    "\n",
    "# Basic aggregations\n",
    "print(\"\\n1. Overall statistics:\")\n",
    "stats = employees_df.agg(\n",
    "    count(\"*\").alias(\"total_employees\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\")\n",
    ")\n",
    "stats.show()\n",
    "\n",
    "# Group by aggregations\n",
    "print(\"\\n2. Statistics by department:\")\n",
    "dept_stats = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    round(avg(\"salary\"), 2).alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_payroll\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Column Functions and Transformations\n",
    "\n",
    "Explore the rich set of column functions available in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String functions\n",
    "print(\"=== String Functions ===\")\n",
    "\n",
    "string_demo_df = employees_df.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"name_upper\"),\n",
    "    length(col(\"name\")).alias(\"name_length\"),\n",
    "    split(col(\"name\"), \" \").getItem(0).alias(\"first_name\"),\n",
    "    split(col(\"name\"), \" \").getItem(1).alias(\"last_name\")\n",
    ")\n",
    "\n",
    "string_demo_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional logic with when/otherwise\n",
    "print(\"=== Conditional Logic ===\")\n",
    "\n",
    "conditional_df = employees_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"salary\"),\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "        .when(col(\"age\") < 40, \"Mid-career\")\n",
    "        .otherwise(\"Experienced\").alias(\"age_group\"),\n",
    "    when(col(\"salary\") > 80000, \"High\")\n",
    "        .when(col(\"salary\") > 70000, \"Medium\")\n",
    "        .otherwise(\"Standard\").alias(\"salary_band\")\n",
    ")\n",
    "\n",
    "conditional_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Spark SQL Integration\n",
    "\n",
    "Use SQL syntax for data analysis with temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views for SQL queries\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"=== Spark SQL Queries ===\")\n",
    "\n",
    "# Basic SQL query\n",
    "print(\"\\n1. Basic SELECT with WHERE:\")\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 75000\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "result1.show()\n",
    "\n",
    "# Aggregation with GROUP BY\n",
    "print(\"\\n2. Aggregation with GROUP BY:\")\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Window Functions and Advanced Analytics\n",
    "\n",
    "Perform sophisticated analytics using window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions for ranking and analytics\n",
    "print(\"=== Advanced Window Functions ===\")\n",
    "\n",
    "# Define windows\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "overall_window = Window.orderBy(desc(\"salary\"))\n",
    "\n",
    "# Apply window functions\n",
    "windowed_analysis = employees_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"department\"),\n",
    "    col(\"salary\"),\n",
    "    \n",
    "    # Ranking functions\n",
    "    row_number().over(dept_window).alias(\"dept_rank\"),\n",
    "    rank().over(overall_window).alias(\"overall_rank\"),\n",
    "    dense_rank().over(dept_window).alias(\"dept_dense_rank\"),\n",
    "    \n",
    "    # Analytical functions\n",
    "    round(avg(\"salary\").over(Window.partitionBy(\"department\")), 2).alias(\"dept_avg\"),\n",
    "    lag(\"salary\", 1).over(dept_window).alias(\"next_lower_salary\")\n",
    ")\n",
    "\n",
    "windowed_analysis.orderBy(\"department\", \"dept_rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Performance Optimization\n",
    "\n",
    "Learn about caching and query optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching for performance\n",
    "print(\"=== Caching and Performance ===\")\n",
    "\n",
    "# Cache DataFrame for reuse\n",
    "employees_df.cache()\n",
    "employees_df.count()  # Force caching\n",
    "\n",
    "print(\"DataFrame cached for better performance in repeated operations\")\n",
    "\n",
    "# Query execution plan\n",
    "print(\"\\n=== Query Execution Plan ===\")\n",
    "complex_query = employees_df.filter(col(\"salary\") > 75000).groupBy(\"department\").avg(\"salary\")\n",
    "print(\"\\nExecution plan for complex query:\")\n",
    "complex_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Working with Complex Data Types\n",
    "\n",
    "Handle arrays, maps, and nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with arrays and complex types\n",
    "print(\"=== Complex Data Types ===\")\n",
    "\n",
    "# Create data with arrays\n",
    "complex_data = [\n",
    "    (\"Alice\", [\"Python\", \"SQL\", \"Spark\"]),\n",
    "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
    "    (\"Charlie\", [\"Python\", \"R\", \"SQL\", \"Tableau\"])\n",
    "]\n",
    "\n",
    "complex_df = spark.createDataFrame(complex_data, [\"name\", \"skills\"])\n",
    "\n",
    "print(\"\\nOriginal complex data:\")\n",
    "complex_df.show(truncate=False)\n",
    "\n",
    "# Work with arrays\n",
    "print(\"\\nArray operations:\")\n",
    "array_ops = complex_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"skills\"),\n",
    "    size(col(\"skills\")).alias(\"skill_count\"),\n",
    "    array_contains(col(\"skills\"), \"Python\").alias(\"knows_python\"),\n",
    "    col(\"skills\")[0].alias(\"primary_skill\")\n",
    ")\n",
    "array_ops.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Integration with Pandas\n",
    "\n",
    "Convert between Spark DataFrames and Pandas for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "print(\"=== Spark to Pandas Integration ===\")\n",
    "\n",
    "# Get department statistics\n",
    "dept_stats_spark = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    round(avg(\"salary\"), 0).alias(\"avg_salary\")\n",
    ")\n",
    "\n",
    "# Convert to Pandas\n",
    "dept_stats_pandas = dept_stats_spark.toPandas()\n",
    "\n",
    "print(\"\\nDepartment statistics (Pandas):\")\n",
    "print(dept_stats_pandas)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Employee count by department\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(dept_stats_pandas['department'], dept_stats_pandas['employee_count'])\n",
    "plt.title('Employee Count by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Employee Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Average salary by department\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(dept_stats_pandas['department'], dept_stats_pandas['avg_salary'])\n",
    "plt.title('Average Salary by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Average Salary ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Spark DataFrames & SQL interactive tutorial. You've learned:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **DataFrame Creation** - From various data sources and with different schemas\n",
    "2. **Basic Operations** - Select, filter, sort, and aggregate data\n",
    "3. **Column Functions** - String, numeric, date, and conditional operations\n",
    "4. **Spark SQL** - Using SQL syntax for complex queries\n",
    "5. **Window Functions** - Advanced analytics and ranking\n",
    "6. **Performance Optimization** - Caching and execution plan analysis\n",
    "7. **Complex Data Types** - Arrays, maps, and nested structures\n",
    "8. **Integration** - Working with Pandas and visualization\n",
    "\n",
    "### Next Steps:\n",
    "1. **Practice with Exercises** - Complete the 7 hands-on exercises\n",
    "2. **Explore Solutions** - Study the provided solution files\n",
    "3. **Run Tests** - Validate your understanding with the test suite\n",
    "4. **Advanced Topics** - Move on to streaming, MLlib, or advanced optimizations\n",
    "\n",
    "### Resources:\n",
    "- Exercise files in the `exercises/` directory\n",
    "- Solution files in the `solutions/` directory\n",
    "- Test your code with `make test`\n",
    "- Use `make help` to see all available commands\n",
    "\n",
    "Happy Spark learning! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}