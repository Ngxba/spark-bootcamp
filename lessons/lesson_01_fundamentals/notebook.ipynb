{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Apache Spark Fundamentals\n",
    "\n",
    "Welcome to your first Spark lesson! In this notebook, you'll learn the core concepts of Apache Spark and get hands-on experience with Resilient Distributed Datasets (RDDs).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Spark architecture and components\n",
    "- Create and manipulate RDDs\n",
    "- Learn the difference between transformations and actions\n",
    "- Explore lazy evaluation in Spark\n",
    "- Navigate the Spark UI\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Spark Session\n",
    "\n",
    "First, let's import the necessary libraries and create a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/28 16:43:40 WARN Utils: Your hostname, Liams-MacBook-Pro.local resolves to a loopback address: 127.0.2.2; using 192.168.1.240 instead (on interface en0)\n",
      "25/09/28 16:43:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/28 16:43:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.7\n",
      "Python Version: 3.11\n",
      "Master: local[*]\n",
      "Application Name: Lesson1-SparkFundamentals\n",
      "Default Parallelism: 10\n",
      "\n",
      "Spark UI available at: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Lesson1-SparkFundamentals\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Get SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Display Spark version and configuration\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Python Version: {sc.pythonVer}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "\n",
    "# Spark UI URL\n",
    "print(\"\\nSpark UI available at: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸŽ¯ Exercise Checkpoint 1**: Open the Spark UI in your browser (http://localhost:4040) and explore the interface. You should see your application listed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Your First RDD\n",
    "\n",
    "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark. Let's create some RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RDD from Collection ===\n",
      "Type: <class 'pyspark.rdd.RDD'>\n",
      "Number of partitions: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 10) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions content: [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
      "\n",
      "With 4 partitions: [[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]\n",
      "\n",
      "Range RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Method 1: Create RDD from a Python collection\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(\"=== RDD from Collection ===\")\n",
    "print(f\"Type: {type(numbers_rdd)}\")\n",
    "print(f\"Number of partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"Partitions content: {numbers_rdd.glom().collect()}\")\n",
    "\n",
    "# Method 2: Create RDD with specific number of partitions\n",
    "numbers_rdd_4_partitions = sc.parallelize(numbers, 4)\n",
    "print(f\"\\nWith 4 partitions: {numbers_rdd_4_partitions.glom().collect()}\")\n",
    "\n",
    "# Method 3: Create RDD from range\n",
    "range_rdd = sc.range(1, 11)  # equivalent to range(1, 11) in Python\n",
    "print(f\"\\nRange RDD: {range_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Partitions\n",
    "\n",
    "Partitions are how Spark divides data across multiple cores/machines. Let's explore this concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Partition Analysis ===\n",
      "Total elements: 100\n",
      "Number of partitions: 8\n",
      "Elements per partition: [12, 12, 12, 12, 12, 12, 12, 16]\n",
      "Partition 0: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Partition 1: [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Partition 2: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Partition 3: [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "Partition 4: [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]\n",
      "Partition 5: [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]\n",
      "Partition 6: [73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "Partition 7: [85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "# Create a larger dataset to see partitioning\n",
    "large_data = list(range(1, 101))  # 1 to 100\n",
    "large_rdd = sc.parallelize(large_data, 8)  # 8 partitions\n",
    "\n",
    "print(\"=== Partition Analysis ===\")\n",
    "print(f\"Total elements: {large_rdd.count()}\")\n",
    "print(f\"Number of partitions: {large_rdd.getNumPartitions()}\")\n",
    "\n",
    "# See how data is distributed across partitions\n",
    "partition_sizes = large_rdd.mapPartitions(lambda x: [len(list(x))]).collect()\n",
    "print(f\"Elements per partition: {partition_sizes}\")\n",
    "\n",
    "# Visualize partition distribution\n",
    "partitions_data = large_rdd.glom().collect()\n",
    "for i, partition in enumerate(partitions_data):\n",
    "    if partition:  # Only show non-empty partitions\n",
    "        print(f\"Partition {i}: {partition[:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Transformations vs Actions\n",
    "\n",
    "Understanding the difference between transformations and actions is crucial in Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations (Lazy Evaluation)\n",
    "\n",
    "Transformations create new RDDs from existing ones but don't execute immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Created numbers_rdd\n",
      "Type: <class 'pyspark.rdd.RDD'>\n",
      "No computation has happened yet!\n",
      "\n",
      "Step 2: Applied map transformation (squared)\n",
      "Type: <class 'pyspark.rdd.PipelinedRDD'>\n",
      "Still no computation - it's lazy!\n",
      "\n",
      "Step 3: Applied filter transformation (even squares)\n",
      "Type: <class 'pyspark.rdd.PipelinedRDD'>\n",
      "Still lazy - no actual processing yet!\n",
      "\n",
      "=== LINEAGE (Execution Plan) ===\n",
      "(10) PythonRDD[16] at RDD at PythonRDD.scala:53 []\n",
      " |   ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:289 []\n",
      "\n",
      "=== END LINEAGE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(\"Step 1: Created numbers_rdd\")\n",
    "print(f\"Type: {type(numbers_rdd)}\")\n",
    "print(\"No computation has happened yet!\\n\")\n",
    "\n",
    "# Transformation 1: map() - apply function to each element\n",
    "squared_rdd = numbers_rdd.map(lambda x: x**2)\n",
    "print(\"Step 2: Applied map transformation (squared)\")\n",
    "print(f\"Type: {type(squared_rdd)}\")\n",
    "print(\"Still no computation - it's lazy!\\n\")\n",
    "\n",
    "# Transformation 2: filter() - keep elements that match condition\n",
    "even_squares_rdd = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"Step 3: Applied filter transformation (even squares)\")\n",
    "print(f\"Type: {type(even_squares_rdd)}\")\n",
    "print(\"Still lazy - no actual processing yet!\\n\")\n",
    "\n",
    "# Let's examine the lineage BEFORE triggering execution\n",
    "print(\"=== LINEAGE (Execution Plan) ===\")\n",
    "lineage = even_squares_rdd.toDebugString().decode(\"utf-8\")\n",
    "print(lineage)\n",
    "print(\"\\n=== END LINEAGE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note:\n",
    "What you expected:\n",
    "[Data] â†’ [Map Stage] â†’ [Filter Stage] â†’ [Result]\n",
    "\n",
    "What Spark actually does:\n",
    "[Data] â†’ [Combined Map+Filter Stage] â†’ [Result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions (Eager Evaluation)\n",
    "\n",
    "Actions trigger the execution of the entire RDD lineage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Actions Trigger Execution ===\n",
      "Even squares: [4, 16, 36, 64, 100]\n",
      "Execution time: 0.0943 seconds\n",
      "Count of even squares: 5\n",
      "First even square: 4\n",
      "First three even squares: [4, 16, 36]\n",
      "Sum of even squares: 220\n"
     ]
    }
   ],
   "source": [
    "# Action 1: collect() - bring all data to driver\n",
    "print(\"=== Actions Trigger Execution ===\")\n",
    "start_time = time.time()\n",
    "result = even_squares_rdd.collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Even squares: {result}\")\n",
    "print(f\"Execution time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# Action 2: count() - count elements\n",
    "count = even_squares_rdd.count()\n",
    "print(f\"Count of even squares: {count}\")\n",
    "\n",
    "# Action 3: first() - get first element\n",
    "first_element = even_squares_rdd.first()\n",
    "print(f\"First even square: {first_element}\")\n",
    "\n",
    "# Action 4: take(n) - get first n elements\n",
    "first_three = even_squares_rdd.take(3)\n",
    "print(f\"First three even squares: {first_three}\")\n",
    "\n",
    "# Action 5: reduce() - aggregate elements\n",
    "sum_of_even_squares = even_squares_rdd.reduce(lambda a, b: a + b)\n",
    "print(f\"Sum of even squares: {sum_of_even_squares}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸŽ¯ Exercise Checkpoint 2**: Check the Spark UI again. Look at the \"Jobs\" tab to see the jobs that were executed. Notice how multiple actions created separate jobs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Common RDD Transformations\n",
    "\n",
    "Let's explore the most commonly used RDD transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "map (x * 2): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "filter (even numbers): [2, 4, 6, 8, 10]\n",
      "flatMap (duplicate each): [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10]\n",
      "distinct(): [1, 2, 3, 4]\n",
      "sample (50%): [1, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for demonstrations\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "print(\"Original data:\", rdd.collect())\n",
    "print()\n",
    "\n",
    "# 1. map() - transform each element\n",
    "doubled = rdd.map(lambda x: x * 2)\n",
    "print(\"map (x * 2):\", doubled.collect())\n",
    "\n",
    "# 2. filter() - keep elements matching condition\n",
    "evens = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"filter (even numbers):\", evens.collect())\n",
    "\n",
    "# 3. flatMap() - transform and flatten\n",
    "pairs = rdd.flatMap(lambda x: [x, x])\n",
    "print(\"flatMap (duplicate each):\", pairs.collect())\n",
    "\n",
    "# 4. distinct() - remove duplicates\n",
    "duplicates = sc.parallelize([1, 1, 2, 2, 3, 3, 4, 4])\n",
    "unique = duplicates.distinct()\n",
    "print(\"distinct():\", unique.collect())\n",
    "\n",
    "# 5. sample() - random sample\n",
    "sample = rdd.sample(withReplacement=False, fraction=0.5, seed=42)\n",
    "print(\"sample (50%):\", sample.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Pairs (Key-Value RDDs)\n",
    "\n",
    "Many Spark operations work with key-value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pairs: [('apple', 1), ('banana', 2), ('apple', 3), ('cherry', 1), ('banana', 4)]\n",
      "\n",
      "Raw grouped data:\n",
      "  Key 'apple' -> Values: <class 'pyspark.resultiterable.ResultIterable'> (iterator)\n",
      "  Key 'banana' -> Values: <class 'pyspark.resultiterable.ResultIterable'> (iterator)\n",
      "  Key 'cherry' -> Values: <class 'pyspark.resultiterable.ResultIterable'> (iterator)\n",
      "\n",
      "groupByKey(): [('apple', [1, 3]), ('banana', [2, 4]), ('cherry', [1])]\n",
      "reduceByKey() [sum]: [('apple', 4), ('banana', 6), ('cherry', 1)]\n",
      "mapValues() [x * 2]: [('apple', 2), ('banana', 4), ('apple', 6), ('cherry', 2), ('banana', 8)]\n",
      "keys(): ['apple', 'banana', 'cherry']\n",
      "values(): [1, 2, 3, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "# Create key-value pairs\n",
    "pairs_data = [(\"apple\", 1), (\"banana\", 2), (\"apple\", 3), (\"cherry\", 1), (\"banana\", 4)]\n",
    "pairs_rdd = sc.parallelize(pairs_data)\n",
    "\n",
    "print(\"Original pairs:\", pairs_rdd.collect())\n",
    "print()\n",
    "\n",
    "# 1. groupByKey() - group values by key\n",
    "grouped = pairs_rdd.groupByKey()\n",
    "print(\"Raw grouped data:\")\n",
    "raw_grouped = grouped.collect()\n",
    "for key, values in raw_grouped:\n",
    "    print(f\"  Key '{key}' -> Values: {type(values)} (iterator)\")\n",
    "    # Note: values is an iterator, not a list!\n",
    "print()\n",
    "grouped_result = grouped.map(lambda x: (x[0], list(x[1]))).collect()\n",
    "print(\"groupByKey():\", grouped_result)\n",
    "\n",
    "# 2. reduceByKey() - combine/aggrigate values for each key\n",
    "reduced = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"reduceByKey() [sum]:\", reduced.collect())\n",
    "\n",
    "# 3. mapValues() - transform only values 1 by 1\n",
    "doubled_values = pairs_rdd.mapValues(lambda x: x * 2)\n",
    "print(\"mapValues() [x * 2]:\", doubled_values.collect())\n",
    "\n",
    "# 4. keys() and values()\n",
    "keys_only = pairs_rdd.keys().distinct()\n",
    "values_only = pairs_rdd.values()\n",
    "print(\"keys():\", keys_only.collect())\n",
    "print(\"values():\", values_only.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Working with Text Data\n",
    "\n",
    "Let's work with some real text data to implement a classic word count example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text lines:\n",
      "1. Apache Spark is a unified analytics engine for large-scale data processing\n",
      "2. Spark provides high-level APIs in Java, Scala, Python and R\n",
      "3. Spark supports SQL queries, streaming data, machine learning and graph processing\n",
      "4. The Spark engine supports multiple programming languages\n",
      "5. Spark can run on Apache Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample text data\n",
    "text_data = [\n",
    "    \"Apache Spark is a unified analytics engine for large-scale data processing\",\n",
    "    \"Spark provides high-level APIs in Java, Scala, Python and R\",\n",
    "    \"Spark supports SQL queries, streaming data, machine learning and graph processing\",\n",
    "    \"The Spark engine supports multiple programming languages\",\n",
    "    \"Spark can run on Apache Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud\",\n",
    "]\n",
    "\n",
    "text_rdd = sc.parallelize(text_data)\n",
    "print(\"Original text lines:\")\n",
    "for i, line in enumerate(text_rdd.collect()):\n",
    "    print(f\"{i+1}. {line}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 53\n",
      "First 10 words: ['Apache', 'Spark', 'is', 'a', 'unified', 'analytics', 'engine', 'for', 'large-scale', 'data']\n",
      "\n",
      "Clean words count: 53\n",
      "Sample clean words: ['apache', 'spark', 'is', 'a', 'unified', 'analytics', 'engine', 'for', 'largescale', 'data', 'processing', 'spark', 'provides', 'highlevel', 'apis']\n",
      "\n",
      "Word pairs sample: [('apache', 1), ('spark', 1), ('is', 1), ('a', 1), ('unified', 1), ('analytics', 1), ('engine', 1), ('for', 1), ('largescale', 1), ('data', 1)]\n",
      "\n",
      "Word counts:\n",
      "and: 2\n",
      "learning: 1\n",
      "unified: 1\n",
      "in: 2\n",
      "can: 1\n",
      "run: 1\n",
      "standalone: 1\n",
      "largescale: 1\n",
      "machine: 1\n",
      "spark: 5\n",
      "engine: 2\n",
      "apis: 1\n",
      "on: 1\n",
      "kubernetes: 1\n",
      "supports: 2\n",
      "mesos: 1\n",
      "cloud: 1\n",
      "is: 1\n",
      "provides: 1\n",
      "scala: 1\n",
      "for: 1\n",
      "java: 1\n",
      "python: 1\n",
      "queries: 1\n",
      "multiple: 1\n",
      "processing: 2\n",
      "sql: 1\n",
      "the: 2\n",
      "programming: 1\n",
      "languages: 1\n",
      "apache: 3\n",
      "analytics: 1\n",
      "streaming: 1\n",
      "graph: 1\n",
      "hadoop: 1\n",
      "a: 1\n",
      "data: 2\n",
      "highlevel: 1\n",
      "r: 1\n",
      "or: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split lines into words\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
    "print(f\"Total words: {words_rdd.count()}\")\n",
    "print(f\"First 10 words: {words_rdd.take(10)}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Clean words (lowercase, remove punctuation)\n",
    "\n",
    "clean_words_rdd = words_rdd.map(\n",
    "    lambda word: re.sub(r\"[^a-zA-Z]\", \"\", word.lower())\n",
    ").filter(lambda word: len(word) > 0)\n",
    "\n",
    "print(f\"Clean words count: {clean_words_rdd.count()}\")\n",
    "print(f\"Sample clean words: {clean_words_rdd.take(15)}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Create word-count pairs\n",
    "word_pairs_rdd = clean_words_rdd.map(lambda word: (word, 1))\n",
    "print(f\"Word pairs sample: {word_pairs_rdd.take(10)}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Count words\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Word counts:\")\n",
    "for word, count in word_counts_rdd.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note:\n",
    "```python\n",
    "Explain flatMap:\n",
    "# Using map\n",
    ".map(lambda x: x.split())\n",
    "# Result: [[\"Hello\", \"world\"], [\"Spark\", \"rocks\"]]  â† Still nested!\n",
    "\n",
    "# Using flatMap  \n",
    ".flatMap(lambda x: x.split())\n",
    "# Result: [\"Hello\", \"world\", \"Spark\", \"rocks\"]      â† Flattened!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent words:\n",
      "spark: 5\n",
      "apache: 3\n",
      "and: 2\n",
      "in: 2\n",
      "engine: 2\n",
      "supports: 2\n",
      "processing: 2\n",
      "the: 2\n",
      "data: 2\n",
      "learning: 1\n",
      "\n",
      "Using takeOrdered (top 5):\n",
      "spark: 5\n",
      "apache: 3\n",
      "and: 2\n",
      "in: 2\n",
      "engine: 2\n"
     ]
    }
   ],
   "source": [
    "# Let's find the most common words\n",
    "# Sort by count (descending)\n",
    "sorted_word_counts = (\n",
    "    word_counts_rdd.map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    ")\n",
    "\n",
    "print(\"Top 10 most frequent words:\")\n",
    "top_words = sorted_word_counts.take(10)\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Alternative: using takeOrdered for top N\n",
    "print(\"\\nUsing takeOrdered (top 5):\")\n",
    "top_5 = word_counts_rdd.takeOrdered(5, key=lambda x: -x[1])\n",
    "for word, count in top_5:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸŽ¯ Exercise Checkpoint 3**: Look at the Spark UI \"Stages\" tab. You should see multiple stages corresponding to different operations. Notice how Spark optimized the execution plan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching and Persistence\n",
    "\n",
    "When you need to reuse an RDD multiple times, caching can significantly improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Without Caching ===\n",
      "First count: 100, Time: 0.38s\n",
      "Sum: 338350, Time: 0.33s\n",
      "Total time without caching: 0.71s\n"
     ]
    }
   ],
   "source": [
    "# Create a computationally expensive RDD\n",
    "def expensive_computation(x):\n",
    "    # Simulate expensive computation\n",
    "    import time\n",
    "\n",
    "    time.sleep(0.01)  # 10ms delay per element\n",
    "    return x**2\n",
    "\n",
    "\n",
    "large_data = sc.parallelize(range(1, 101), 4)\n",
    "expensive_rdd = large_data.map(expensive_computation)\n",
    "\n",
    "print(\"=== Without Caching ===\")\n",
    "# First computation\n",
    "start_time = time.time()\n",
    "result1 = expensive_rdd.count()\n",
    "time1 = time.time() - start_time\n",
    "print(f\"First count: {result1}, Time: {time1:.2f}s\")\n",
    "\n",
    "# Second computation (will recompute everything)\n",
    "start_time = time.time()\n",
    "result2 = expensive_rdd.reduce(lambda a, b: a + b)\n",
    "time2 = time.time() - start_time\n",
    "print(f\"Sum: {result2}, Time: {time2:.2f}s\")\n",
    "\n",
    "print(f\"Total time without caching: {time1 + time2:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== With Caching ===\n",
      "First count (caching): 100, Time: 0.40s\n",
      "Sum (from cache): 338350, Time: 0.05s\n",
      "Total time with caching: 0.44s\n",
      "\n",
      "Is cached: True\n",
      "Storage level: Memory Serialized 1x Replicated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[107] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== With Caching ===\")\n",
    "# Cache the RDD in memory\n",
    "expensive_rdd.cache()  # or use .persist()\n",
    "\n",
    "# First computation (will cache the result)\n",
    "start_time = time.time()\n",
    "result1 = expensive_rdd.count()\n",
    "time1 = time.time() - start_time\n",
    "print(f\"First count (caching): {result1}, Time: {time1:.2f}s\")\n",
    "\n",
    "# Second computation (will use cached data)\n",
    "start_time = time.time()\n",
    "result2 = expensive_rdd.reduce(lambda a, b: a + b)\n",
    "time2 = time.time() - start_time\n",
    "print(f\"Sum (from cache): {result2}, Time: {time2:.2f}s\")\n",
    "\n",
    "print(f\"Total time with caching: {time1 + time2:.2f}s\")\n",
    "\n",
    "# Check cache status\n",
    "print(f\"\\nIs cached: {expensive_rdd.is_cached}\")\n",
    "print(f\"Storage level: {expensive_rdd.getStorageLevel()}\")\n",
    "\n",
    "# Clean up cache\n",
    "expensive_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Storage Levels\n",
    "\n",
    "Spark offers different storage levels for persistence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY_ONLY: Memory Serialized 1x Replicated\n",
      "MEMORY_ONLY_2: Memory Serialized 2x Replicated\n",
      "MEMORY_AND_DISK: Disk Memory Serialized 1x Replicated\n",
      "MEMORY_AND_DISK_2: Disk Memory Serialized 2x Replicated\n",
      "DISK_ONLY: Disk Serialized 1x Replicated\n",
      "DISK_ONLY_2: Disk Serialized 2x Replicated\n",
      "OFF_HEAP: Disk Memory OffHeap Serialized 1x Replicated\n",
      "OFF_HEAP: Disk Memory OffHeap Serialized 1x Replicated\n",
      "\n",
      "Choose storage level based on:\n",
      "- MEMORY_ONLY: Fast, but limited by memory (always serialized in PySpark)\n",
      "- MEMORY_AND_DISK: Spills to disk when memory is full (recommended)\n",
      "- DISK_ONLY: Slower but handles large datasets\n",
      "- MEMORY_ONLY_2: Fast + fault tolerance (replicated across 2 nodes)\n",
      "- MEMORY_AND_DISK_2: Spills to disk + replicated for fault tolerance\n",
      "- DISK_ONLY_2: Disk storage + replicated for fault tolerance\n",
      "- OFF_HEAP: Uses off-heap memory (requires configuration)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Create a test RDD\n",
    "test_rdd = sc.parallelize(range(1, 1000), 4)\n",
    "\n",
    "# Basic memory and disk storage\n",
    "print(f\"MEMORY_ONLY: {StorageLevel.MEMORY_ONLY}\")\n",
    "print(f\"MEMORY_ONLY_2: {StorageLevel.MEMORY_ONLY_2}\")\n",
    "print(f\"MEMORY_AND_DISK: {StorageLevel.MEMORY_AND_DISK}\")\n",
    "print(f\"MEMORY_AND_DISK_2: {StorageLevel.MEMORY_AND_DISK_2}\")\n",
    "\n",
    "# Disk-only storage\n",
    "print(f\"DISK_ONLY: {StorageLevel.DISK_ONLY}\")\n",
    "print(f\"DISK_ONLY_2: {StorageLevel.DISK_ONLY_2}\")\n",
    "\n",
    "# Off-heap storage\n",
    "print(f\"OFF_HEAP: {StorageLevel.OFF_HEAP}\")\n",
    "print(f\"OFF_HEAP: {StorageLevel.OFF_HEAP}\")\n",
    "\n",
    "# Example of using different storage levels\n",
    "# test_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# test_rdd.persist(StorageLevel.MEMORY_ONLY_SER)  # Serialized in memory\n",
    "# test_rdd.persist(StorageLevel.DISK_ONLY)        # Only on disk\n",
    "\n",
    "print(\"\\nChoose storage level based on:\")\n",
    "print(\"- MEMORY_ONLY: Fast, but limited by memory (always serialized in PySpark)\")\n",
    "print(\"- MEMORY_AND_DISK: Spills to disk when memory is full (recommended)\")\n",
    "print(\"- DISK_ONLY: Slower but handles large datasets\")\n",
    "print(\"- MEMORY_ONLY_2: Fast + fault tolerance (replicated across 2 nodes)\")\n",
    "print(\"- MEMORY_AND_DISK_2: Spills to disk + replicated for fault tolerance\")\n",
    "print(\"- DISK_ONLY_2: Disk storage + replicated for fault tolerance\")\n",
    "print(\"- OFF_HEAP: Uses off-heap memory (requires configuration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. RDD Lineage and Fault Tolerance\n",
    "\n",
    "One of Spark's key features is automatic fault tolerance through RDD lineage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After narrow transforms only:\n",
      "(4) PythonRDD[125] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[120] at readRDDFromFile at PythonRDD.scala:289 []\n",
      "=== RDD Lineage ===\n",
      "Final RDD lineage:\n",
      "(4) PythonRDD[126] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[124] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[123] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(4) PairwiseRDD[122] at reduceByKey at /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/ipykernel_74672/1071599196.py:6 []\n",
      "    |  PythonRDD[121] at reduceByKey at /var/folders/hj/ljckbnbd51d_x_p3c23zp6j40000gn/T/ipykernel_74672/1071599196.py:6 []\n",
      "    |  ParallelCollectionRDD[120] at readRDDFromFile at PythonRDD.scala:289 []\n",
      "\n",
      "=== Understanding Lineage ===\n",
      "Each line shows:\n",
      "- Stage ID and partition count\n",
      "- Transformation applied\n",
      "- Dependencies on parent RDDs\n",
      "\n",
      "If a partition is lost, Spark can recompute it using this lineage!\n"
     ]
    }
   ],
   "source": [
    "# Create a complex RDD with multiple transformations\n",
    "data = sc.parallelize(range(1, 21), 4)\n",
    "step1 = data.map(lambda x: x * 2)\n",
    "step2 = step1.filter(lambda x: x > 10)\n",
    "step3 = step2.map(lambda x: (x % 3, x))\n",
    "step4 = step3.reduceByKey(lambda a, b: a + b)\n",
    "# Check lineage after narrow transformations only\n",
    "step3_lineage = step3.toDebugString().decode(\"utf-8\")\n",
    "print(\"After narrow transforms only:\")\n",
    "print(step3_lineage)\n",
    "\n",
    "print(\"=== RDD Lineage ===\")\n",
    "print(\"Final RDD lineage:\")\n",
    "lineage = step4.toDebugString().decode(\"utf-8\")\n",
    "print(lineage)\n",
    "\n",
    "print(\"\\n=== Understanding Lineage ===\")\n",
    "print(\"Each line shows:\")\n",
    "print(\"- Stage ID and partition count\")\n",
    "print(\"- Transformation applied\")\n",
    "print(\"- Dependencies on parent RDDs\")\n",
    "print(\"\\nIf a partition is lost, Spark can recompute it using this lineage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "* **ParallelCollectionRDD[120]** = the source `data`\n",
    "* **PythonRDD[121] at reduceByKey** = the **entire chain of Python-side narrow ops before the shuffle**, which includes:\n",
    "  * `map(lambda x: x * 2)` â† **step1**\n",
    "  * `filter(lambda x: x > 10)` â† **step2**\n",
    "  * `map(lambda x: (x % 3, x))` â† **step3** (creates the pair RDD)\n",
    "* **PairwiseRDD[122]** = the keyed view feeding `reduceByKey`\n",
    "* **ShuffledRDD[123]** = the shuffle caused by `reduceByKey`\n",
    "* **MapPartitionsRDD[124]** (then a **PythonRDD[126]**) = post-shuffle final reduce and whatever Python action followed\n",
    "\n",
    "Why they look â€œmissingâ€: Spark collapses consecutive **narrow** transformations (map/filter/map) into a single stage and often labels that block by the *last* transformation that created the Python boundary for the upcoming wide op (here it shows â€œat reduceByKeyâ€). So step1 & step2 arenâ€™t in **ParallelCollectionRDD** (thatâ€™s just the source) â€” theyâ€™re executed in the executorsâ€™ Python workers as part of **PythonRDD[121]** right before the shuffle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result: [(0, 120), (1, 140), (2, 130)]\n",
      "\n",
      "Manual verification:\n",
      "After doubling and filtering > 10: [12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]\n",
      "Grouped by (x % 3) and summed: {0: 120, 2: 130, 1: 140}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the result\n",
    "result = step4.collect()\n",
    "print(f\"Final result: {result}\")\n",
    "\n",
    "# Verify our computation manually\n",
    "print(\"\\nManual verification:\")\n",
    "original = list(range(1, 21))\n",
    "doubled = [x * 2 for x in original]\n",
    "filtered = [x for x in doubled if x > 10]\n",
    "print(f\"After doubling and filtering > 10: {filtered}\")\n",
    "\n",
    "# Group by x % 3 and sum\n",
    "\n",
    "groups = defaultdict(int)\n",
    "for x in filtered:\n",
    "    groups[x % 3] += x\n",
    "print(f\"Grouped by (x % 3) and summed: {dict(groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Reading Data from Files\n",
    "\n",
    "Let's create some sample data files and learn how to read them with Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data files created in 'data/' directory\n"
     ]
    }
   ],
   "source": [
    "# Create sample data files\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Create a sample text file\n",
    "sample_text = \"\"\"\n",
    "Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
    "Spark offers over 80 high-level operators that make it easy to build parallel apps.\n",
    "You can use it interactively from the Scala, Python, R, and SQL shells.\n",
    "Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming.\n",
    "You can combine these libraries seamlessly in the same application.\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\"data/sample_data.txt\", \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Create a CSV file\n",
    "csv_data = \"\"\"name,age,city,salary\n",
    "Alice,25,New York,50000\n",
    "Bob,30,San Francisco,75000\n",
    "Charlie,35,Chicago,60000\n",
    "Diana,28,Boston,55000\n",
    "Eve,32,Seattle,70000\n",
    "Eve,34,Chicago,100000\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data/people.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "print(\"Sample data files created in 'data/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading Text Files ===\n",
      "Number of lines: 5\n",
      "\n",
      "Content:\n",
      "1. Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
      "2. Spark offers over 80 high-level operators that make it easy to build parallel apps.\n",
      "3. You can use it interactively from the Scala, Python, R, and SQL shells.\n",
      "4. Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming.\n",
      "5. You can combine these libraries seamlessly in the same application.\n",
      "\n",
      "Top words (length > 3):\n",
      "spark: 4\n",
      "machine: 2\n",
      "learning: 2\n",
      "data: 2\n",
      "libraries: 2\n",
      "apache: 1\n",
      "multi-language: 1\n",
      "engineering: 1\n",
      "science: 1\n",
      "machines: 1\n"
     ]
    }
   ],
   "source": [
    "# Reading text files\n",
    "text_file_rdd = sc.textFile(\"data/sample_data.txt\")\n",
    "\n",
    "print(\"=== Reading Text Files ===\")\n",
    "print(f\"Number of lines: {text_file_rdd.count()}\")\n",
    "print(\"\\nContent:\")\n",
    "for i, line in enumerate(text_file_rdd.collect()):\n",
    "    print(f\"{i+1}. {line}\")\n",
    "\n",
    "# Word count on file data\n",
    "file_word_counts = (\n",
    "    text_file_rdd.flatMap(lambda line: line.split())\n",
    "    .map(lambda word: word.lower().strip(\".,\"))\n",
    "    .filter(lambda word: len(word) > 3)\n",
    "    .map(lambda word: (word, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nTop words (length > 3):\")\n",
    "for word, count in file_word_counts.take(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading CSV Files ===\n",
      "Header: name,age,city,salary\n",
      "Data rows: 6\n",
      "\n",
      "Parsed data:\n",
      "['Alice', '25', 'New York', '50000']\n",
      "['Bob', '30', 'San Francisco', '75000']\n",
      "['Charlie', '35', 'Chicago', '60000']\n",
      "['Diana', '28', 'Boston', '55000']\n",
      "['Eve', '32', 'Seattle', '70000']\n",
      "['Eve', '34', 'Chicago', '100000']\n",
      "\n",
      "Average salary: $68,333.33\n",
      "\n",
      "People by city:\n",
      "New York: ['Alice']\n",
      "Boston: ['Diana']\n",
      "Seattle: ['Eve']\n",
      "San Francisco: ['Bob']\n",
      "Chicago: ['Charlie', 'Eve']\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV files (basic approach with RDDs)\n",
    "csv_rdd = sc.textFile(\"data/people.csv\")\n",
    "\n",
    "print(\"=== Reading CSV Files ===\")\n",
    "# Skip header and parse CSV\n",
    "header = csv_rdd.first()\n",
    "data_rdd = csv_rdd.filter(lambda line: line != header)\n",
    "\n",
    "print(f\"Header: {header}\")\n",
    "print(f\"Data rows: {data_rdd.count()}\")\n",
    "\n",
    "# Parse CSV rows\n",
    "parsed_rdd = data_rdd.map(lambda line: line.split(\",\"))\n",
    "print(\"\\nParsed data:\")\n",
    "for row in parsed_rdd.collect():\n",
    "    print(row)\n",
    "\n",
    "# Calculate average salary\n",
    "salaries = parsed_rdd.map(lambda row: int(row[3]))\n",
    "avg_salary = salaries.reduce(lambda a, b: a + b) / salaries.count()\n",
    "print(f\"\\nAverage salary: ${avg_salary:,.2f}\")\n",
    "\n",
    "# Find people in specific cities\n",
    "cities_rdd = parsed_rdd.map(lambda row: (row[2], row[0]))  # (city, name)\n",
    "people_by_city = cities_rdd.groupByKey().mapValues(list)\n",
    "print(\"\\nPeople by city:\")\n",
    "for city, people in people_by_city.collect():\n",
    "    print(f\"{city}: {people}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸŽ¯ Exercise Checkpoint 4**: Create your own small text file with some content and try reading it with Spark. Implement a character count (instead of word count).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Monitoring and Debugging\n",
    "\n",
    "Let's learn how to monitor and debug Spark applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inefficient Example ===\n",
      "Count: 9999999\n",
      "Sum: 49999995000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 9999999\n",
      "Min: 1\n",
      "Inefficient approach time: 1.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Create a deliberately inefficient computation\n",
    "def inefficient_example():\n",
    "    data = sc.parallelize(range(1, 10000000), 1)  # Only 1 partition - inefficient!\n",
    "\n",
    "    # Multiple unnecessary actions\n",
    "    print(f\"Count: {data.count()}\")\n",
    "    print(f\"Sum: {data.sum()}\")\n",
    "    print(f\"Max: {data.max()}\")\n",
    "    print(f\"Min: {data.min()}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"=== Inefficient Example ===\")\n",
    "start_time = time.time()\n",
    "inefficient_rdd = inefficient_example()\n",
    "inefficient_time = time.time() - start_time\n",
    "print(f\"Inefficient approach time: {inefficient_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Efficient Example ===\n",
      "Count: 9999999\n",
      "Sum: 49999995000000\n",
      "Max: 9999999\n",
      "Min: 1\n",
      "Efficient approach time: 0.89s\n",
      "Speedup: 1.72x\n"
     ]
    }
   ],
   "source": [
    "# More efficient approach\n",
    "def efficient_example():\n",
    "    data = sc.parallelize(range(1, 10000000), 8)  # Multiple partitions\n",
    "\n",
    "    # Cache the data since we'll use it multiple times\n",
    "    data.cache()\n",
    "\n",
    "    # Force materialization\n",
    "    data.count()\n",
    "\n",
    "    # Now use the cached data\n",
    "    print(f\"Count: {data.count()}\")\n",
    "    print(f\"Sum: {data.sum()}\")\n",
    "    print(f\"Max: {data.max()}\")\n",
    "    print(f\"Min: {data.min()}\")\n",
    "\n",
    "    data.unpersist()\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"\\n=== Efficient Example ===\")\n",
    "start_time = time.time()\n",
    "efficient_rdd = efficient_example()\n",
    "efficient_time = time.time() - start_time\n",
    "print(f\"Efficient approach time: {efficient_time:.2f}s\")\n",
    "print(f\"Speedup: {inefficient_time/efficient_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debugging Information ===\n",
      "RDD ID: 190\n",
      "Name: None\n",
      "Partitions: 2\n",
      "Storage level: Serialized 1x Replicated\n",
      "New name: MyDebugRDD\n",
      "Partition contents: [[1, 2], [3, 4, 5]]\n",
      "Final result: [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partition 1 with data: [3, 4, 5]\n",
      "Processing partition 0 with data: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Debugging tips\n",
    "debug_rdd = sc.parallelize([1, 2, 3, 4, 5], 2)\n",
    "\n",
    "print(\"=== Debugging Information ===\")\n",
    "print(f\"RDD ID: {debug_rdd.id()}\")\n",
    "print(f\"Name: {debug_rdd.name()}\")\n",
    "print(f\"Partitions: {debug_rdd.getNumPartitions()}\")\n",
    "print(f\"Storage level: {debug_rdd.getStorageLevel()}\")\n",
    "\n",
    "# Set a name for easier identification in UI\n",
    "debug_rdd.setName(\"MyDebugRDD\")\n",
    "debug_rdd.cache()\n",
    "print(f\"New name: {debug_rdd.name()}\")\n",
    "\n",
    "# Use glom() to see partition contents\n",
    "print(f\"Partition contents: {debug_rdd.glom().collect()}\")\n",
    "\n",
    "\n",
    "# Use mapPartitionsWithIndex to see partition processing\n",
    "def debug_partition(index, iterator):\n",
    "    data = list(iterator)\n",
    "    print(f\"Processing partition {index} with data: {data}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "result = debug_rdd.mapPartitionsWithIndex(debug_partition).collect()\n",
    "print(f\"Final result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸŽ¯ Exercise Checkpoint 5**: Visit the Spark UI and explore:\n",
    "- Jobs tab: See completed jobs\n",
    "- Stages tab: See how jobs were broken into stages\n",
    "- Storage tab: See cached RDDs\n",
    "- Executors tab: See resource utilization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've completed your first Spark lesson. Let's summarize what you learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ LESSON 1 COMPLETE! ðŸŽ‰\n",
      "\n",
      "=== What You've Learned ===\n",
      "âœ… Spark architecture and components\n",
      "âœ… Creating and manipulating RDDs\n",
      "âœ… Transformations vs Actions\n",
      "âœ… Lazy evaluation principles\n",
      "âœ… Common RDD operations\n",
      "âœ… Working with key-value pairs\n",
      "âœ… Text processing and word count\n",
      "âœ… Caching and persistence\n",
      "âœ… RDD lineage and fault tolerance\n",
      "âœ… Reading data from files\n",
      "âœ… Performance monitoring basics\n",
      "\n",
      "=== Key Concepts to Remember ===\n",
      "ðŸ”‘ RDDs are immutable and distributed\n",
      "ðŸ”‘ Transformations are lazy, actions are eager\n",
      "ðŸ”‘ Use caching for RDDs accessed multiple times\n",
      "ðŸ”‘ Spark automatically handles fault tolerance\n",
      "ðŸ”‘ Partitioning affects performance\n",
      "ðŸ”‘ Monitor your jobs using Spark UI\n",
      "\n",
      "=== Next Steps ===\n",
      "ðŸ“š Complete the exercises in the exercises/ folder\n",
      "ðŸ§ª Run the validation tests\n",
      "ðŸš€ Ready for Lesson 2: DataFrames and Spark SQL\n",
      "\n",
      "=== Session Statistics ===\n",
      "Spark Version: 3.5.7\n",
      "Application ID: local-1759052621348\n",
      "Total Cores: 10\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Final summary demonstration\n",
    "print(\"ðŸŽ‰ LESSON 1 COMPLETE! ðŸŽ‰\")\n",
    "print(\"\\n=== What You've Learned ===\")\n",
    "print(\"âœ… Spark architecture and components\")\n",
    "print(\"âœ… Creating and manipulating RDDs\")\n",
    "print(\"âœ… Transformations vs Actions\")\n",
    "print(\"âœ… Lazy evaluation principles\")\n",
    "print(\"âœ… Common RDD operations\")\n",
    "print(\"âœ… Working with key-value pairs\")\n",
    "print(\"âœ… Text processing and word count\")\n",
    "print(\"âœ… Caching and persistence\")\n",
    "print(\"âœ… RDD lineage and fault tolerance\")\n",
    "print(\"âœ… Reading data from files\")\n",
    "print(\"âœ… Performance monitoring basics\")\n",
    "\n",
    "print(\"\\n=== Key Concepts to Remember ===\")\n",
    "print(\"ðŸ”‘ RDDs are immutable and distributed\")\n",
    "print(\"ðŸ”‘ Transformations are lazy, actions are eager\")\n",
    "print(\"ðŸ”‘ Use caching for RDDs accessed multiple times\")\n",
    "print(\"ðŸ”‘ Spark automatically handles fault tolerance\")\n",
    "print(\"ðŸ”‘ Partitioning affects performance\")\n",
    "print(\"ðŸ”‘ Monitor your jobs using Spark UI\")\n",
    "\n",
    "print(\"\\n=== Next Steps ===\")\n",
    "print(\"ðŸ“š Complete the exercises in the exercises/ folder\")\n",
    "print(\"ðŸ§ª Run the validation tests\")\n",
    "print(\"ðŸš€ Ready for Lesson 2: DataFrames and Spark SQL\")\n",
    "\n",
    "# Final application statistics\n",
    "print(\"\\n=== Session Statistics ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Total Cores: {sc.defaultParallelism}\")\n",
    "print(\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Always clean up your Spark session when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. âœ…\")\n",
    "print(\"\\nGreat job completing Lesson 1! ðŸŽŠ\")\n",
    "print(\"Don't forget to check out the exercises and run the validation tests.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
